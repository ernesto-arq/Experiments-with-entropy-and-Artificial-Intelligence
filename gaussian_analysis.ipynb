{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JwZ0hEX_6uAD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions for Code Execution\n",
        "To execute this code and perform analyses, it's necessary to import the datagen_IID and datagen_non-iid files into the project folder.\n",
        "\n",
        "Acknowledgements and References\n",
        "I would like to express my sincere thanks to the author of the repository on GitHub, whose work was fundamental for the development of this project. The algorithm studied for generating non-iid data contributed significantly to the completion of this analysis.\n",
        "\n",
        "Original Algorithm Author: Jeremy (https://github.com/jeremy313)  \n",
        "Reference: [Non-iid Mnist Algorithm](https://github.com/jeremy313/non-iid-dataset-for-personalized-federated-learning/blob/master/dataset/mnist_noniid.py)\n",
        "Modified by  \n",
        "Modification Author: Ernesto Gurgel Valente Neto\n"
      ],
      "metadata": {
        "id": "dPsTDHGenBZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset [MNIST]"
      ],
      "metadata": {
        "id": "MjLxcnw6ExO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions [IDD DATASET]"
      ],
      "metadata": {
        "id": "JwZ0hEX_6uAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import entropy\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import time\n",
        "from mnist_noniid import get_dataset_mnist_extr_noniid\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import kstest\n",
        "from scipy.stats import normaltest\n",
        "from scipy import stats\n",
        "\n",
        "def entropiaBR(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "def pegarDadosIDDMNIST():\n",
        "  num_users_mnist = 1   #Quatidade de usuarios\n",
        "  nclass_mnist = 9      #Quantidade de classes que cada usuario possui\n",
        "  nsamples_mnist = 1    #cada shad divide o conjunto de dados de 60.000 em partes\n",
        "  rate_unbalance_mnist = 0.0 #1 altamente desbalanceado e 0 seria balanceado\n",
        "  train_dataset_mnist, test_dataset_mnist, user_groups_train_mnist, user_groups_test_mnist = get_dataset_mnist_extr_noniid(num_users_mnist, nclass_mnist, nsamples_mnist, rate_unbalance_mnist)\n",
        "  print(train_dataset_mnist)\n",
        "  all_images = [img[0].numpy().squeeze() for img in train_dataset_mnist]\n",
        "  all_images_np = np.stack(all_images)\n",
        "  #Removendo a mudança ao qual deixa os dados\n",
        "  mean, std = 0.1307, 0.3081\n",
        "  all_images_np = all_images_np * std + mean\n",
        "  print(all_images_np.shape)\n",
        "  all_labels = [lbl for _, lbl in train_dataset_mnist]\n",
        "  # Plotting the histogram\n",
        "  plt.hist(all_labels, bins=10, edgecolor='black', alpha=0.7)\n",
        "  plt.title('Class Distribution Histogram')\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "  return train_dataset_mnist, test_dataset_mnist, user_groups_train_mnist, user_groups_test_mnist, all_images_np\n",
        "\n",
        "def iEMM(train_X, base = 0, ftupla = 0):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  print('Valor da Mediana: ', median)\n",
        "  if base == 1:\n",
        "    indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  elif base == 2:\n",
        "    indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] >= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  if base != 0:\n",
        "    train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "  if ftupla == 1:\n",
        "    tuplasEntropia = [tuplasEntropia[i] for i in indices_filtrados]\n",
        "  return train_X, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "def entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado):\n",
        "  print(\"Tamanho do Dataset: \", len(tuplasEntropia))\n",
        "  print(\"Tamanho do divido pela Mediana: \", len(train_X))\n",
        "  print(\"Entropia das imagens (Não-Ordenado | Original): \", tuplasEntropia[:5], \" ...\")\n",
        "  print(\"Entropia das imagens (Ordenado | Valor): \", entropiesLocal_ordenado[:5], \" ...\")\n",
        "\n",
        "  valEnt = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "  soma = sum(valEnt)\n",
        "  print(\"Entropia Total Acumulada: \", soma)\n",
        "  alvos = [0.25 * soma, 0.5 * soma, 0.75 * soma, soma]\n",
        "  grupos = []\n",
        "  grupo_atual = []\n",
        "  soma_acumulada = 0\n",
        "  index_alvo = 0\n",
        "  for num in valEnt:\n",
        "      grupo_atual.append(num)\n",
        "      soma_acumulada += num\n",
        "      if soma_acumulada >= alvos[index_alvo]:\n",
        "          grupos.append(grupo_atual)\n",
        "          grupo_atual = []\n",
        "          index_alvo += 1\n",
        "          if index_alvo == 4:\n",
        "              break\n",
        "  if grupo_atual:\n",
        "      grupos.append(grupo_atual)\n",
        "\n",
        "  print(\"Tamanho do Conjunto Analisado do Grafico a seguir: \", len(valEnt))\n",
        "  print(\"Tamanho do grupo 1 - 25% dos valores (Q1): \", len(grupos[0]))\n",
        "  print(\"Tamanho do grupo 2 - 50% dos valores (Q2): \", len(grupos[1]))\n",
        "  print(\"Tamanho do grupo 3 - 75% dos valores (Q3): \", len(grupos[2]))\n",
        "  print(\"Tamanho do grupo 4 - 100% dos valores (Q4): \", len(grupos[3]))\n",
        "\n",
        "  # Calcula a média e o desvio padrão dos dados\n",
        "  mu, std = np.mean(valEnt), np.std(valEnt)\n",
        "  # Histograma dos dados\n",
        "  plt.hist(valEnt, bins=10, edgecolor='black', alpha=0.7, density=True, label=\"Dados\")\n",
        "  xmin, xmax = plt.xlim()\n",
        "  x = np.linspace(xmin, xmax, 100)\n",
        "  p = norm.pdf(x, mu, std)\n",
        "  # Plota a curva gaussiana\n",
        "  plt.plot(x, p, 'k', linewidth=2, label=\"Gaussiana\")\n",
        "  # Configurações adicionais e exibição do gráfico\n",
        "  plt.title('Data Histogram & Gaussian Curve')\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def hipoteseNula(tuplasEntropia):\n",
        "  print(\"\\n\")\n",
        "  xEnt = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "  mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "  #Shapiro-Wilk:\n",
        "  statSW, pSW = stats.shapiro(xEnt)  # Obtém a estatística de teste e o valor-p\n",
        "  validacaoSW = \"Não Rejeitamos H0\" if pSW > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. Shapiro-Wilk, Statistica: %.13f\" % statSW, \"Valor-p: %.10f\" % pSW, \"Validação de Hipotese H0:\", validacaoSW)\n",
        "  #Kolmogorov-Smirnov\n",
        "  mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "  statKS, pKS = kstest(xEnt, 'norm', args=(mu, std_dev))\n",
        "  validacaoKS = \"Não Rejeitamos H0\" if pKS > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. Kolmogorov-Smirnov, Statistica: %.10f\" % statKS, \"Valor-p: %.10f\" % pKS, \"Validação de Hipotese H0:\", validacaoKS)\n",
        "  #D'Agostino e Pearson's\n",
        "  statAP, pAP = normaltest(xEnt)\n",
        "  validacaoAP = \"Não Rejeitamos H0\" if pAP > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. D'Agostino e Pearson's, Statistica: %.10f\" % statAP, \"Valor-p: %.10f\" % pAP, \"Validação de Hipotese H0:\", validacaoAP)\n",
        "\n",
        "  if (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\") or \\\n",
        "    (validacaoSW == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\") or \\\n",
        "    (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\"):\n",
        "    print(\"Com base em uma validação dupla identificamos de que não há evidência suficiente para rejeitar a hipótese nula. A amostra parece seguir uma distribuição normal.\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "ulDLtw5BDE-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_mnist, test_dataset_mnist, user_groups_train_mnist, user_groups_test_mnist, all_images_np = pegarDadosIDDMNIST()\n",
        "#base = 0, ftupla = 0 : distribuição dos dados completos\n",
        "#base = 1, ftupla = 1 : distribuição dos dados a esquerda da mediana\n",
        "#base = 2, ftupla = 1 : distribuição dos dados a direita da mediana\n",
        "train_X, tuplasEntropia, entropiesLocal_ordenado= iEMM(all_images_np, base = 0, ftupla = 0)\n",
        "hipoteseNula(tuplasEntropia)\n",
        "entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado)"
      ],
      "metadata": {
        "id": "rlmVyF0DEeOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_mnist, test_dataset_mnist, user_groups_train_mnist, user_groups_test_mnist, all_images_np = pegarDadosIDDMNIST()\n",
        "#base = 0, ftupla = 0 : distribuição dos dados completos\n",
        "#base = 1, ftupla = 1 : distribuição dos dados a esquerda da mediana\n",
        "#base = 2, ftupla = 1 : distribuição dos dados a direita da mediana\n",
        "train_X, tuplasEntropia, entropiesLocal_ordenado= iEMM(all_images_np, base = 1, ftupla = 1)\n",
        "hipoteseNula(tuplasEntropia)\n",
        "entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado)"
      ],
      "metadata": {
        "id": "2khJKji_E1ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_mnist, test_dataset_mnist, user_groups_train_mnist, user_groups_test_mnist, all_images_np = pegarDadosIDDMNIST()\n",
        "#base = 0, ftupla = 0 : distribuição dos dados completos\n",
        "#base = 1, ftupla = 1 : distribuição dos dados a esquerda da mediana\n",
        "#base = 2, ftupla = 1 : distribuição dos dados a direita da mediana\n",
        "train_X, tuplasEntropia, entropiesLocal_ordenado= iEMM(all_images_np, base = 2, ftupla = 1)\n",
        "hipoteseNula(tuplasEntropia)\n",
        "entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado)"
      ],
      "metadata": {
        "id": "ScXqGx0FE3A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions [NON-IDD DATASET]"
      ],
      "metadata": {
        "id": "VqowFacn6oXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mnist_noniid import get_dataset_mnist_extr_noniid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import kstest, norm\n",
        "from scipy import stats\n",
        "\n",
        "def entropiaBR(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "def indicesEntropiaMedianMnist(train_X):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #selecionando os indices de entropia entropia baixa abaixo da mediana.\n",
        "  #indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] >= median])  #selecionando os indices de entropia entropia baixa abaixo da mediana.\n",
        "  print('Valor da Mediana: ', median)\n",
        "  train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
        "  #tuplasEntropia = np.array([tuplasEntropia[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
        "  return train_X, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "def mnistNONIDD(num_users_mnist = 400, nclass_mnist = 2, nsamples_mnist = 20, rate_unbalance_mnist = 1.0, p = 100, q = 5, plotar = 5):\n",
        "  #Tratamento de erros\n",
        "  num_users_mnist = 400 if num_users_mnist == 0 else num_users_mnist\n",
        "  nclass_mnist = 2 if nclass_mnist == 0 else nclass_mnist\n",
        "  nsamples_mnist = 20 if nsamples_mnist == 0 else  nsamples_mnist\n",
        "  rate_unbalance_mnist = 1.0 if rate_unbalance_mnist == 0.0 else rate_unbalance_mnist\n",
        "  p = 100 if p == 0 else p\n",
        "  q = 5 if q == 0 else q\n",
        "  plotar = 5 if plotar == 0 else plotar\n",
        "\n",
        "  print(\"Dados da Importação: \")\n",
        "  train_dataset_mnist, test_dataset_mnist, user_groups_train_mnist, user_groups_test_mnist = get_dataset_mnist_extr_noniid(num_users_mnist, nclass_mnist, nsamples_mnist, rate_unbalance_mnist)\n",
        "  print(\"\\n\")\n",
        "  print(\"Informações Gerais do Conjunto de Dados\")\n",
        "  print(f\"Total de usuarios: {len(user_groups_train_mnist)}\")\n",
        "\n",
        "  if p == 1:\n",
        "   for i in (range(int(len(user_groups_train_mnist)))):\n",
        "     print(f\"Grupo {i}: \", user_groups_train_mnist[i][:q], \" ... \", user_groups_train_mnist[i][(nsamples_mnist - q):nsamples_mnist])   #imprimindo os 5 primeiros e os 5 ultimos de cada grupo\n",
        "  elif p >= 2:\n",
        "    for i in (range(int(len(user_groups_train_mnist)/p))):\n",
        "      print(f\"Grupo {i}: \", user_groups_train_mnist[i][:q], \" ... \", user_groups_train_mnist[i][(nsamples_mnist - q):nsamples_mnist])   #imprimindo os 5 primeiros e os 5 ultimos de cada grupo\n",
        "    print(\"\\n\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"\\n\")\n",
        "    g = (len(user_groups_train_mnist) - int(len(user_groups_train_mnist)/p))\n",
        "    for g in range(g, len(user_groups_train_mnist)):\n",
        "      print(f\"Grupo {g}: \", user_groups_train_mnist[g][:q], \" ... \", user_groups_train_mnist[g][(nsamples_mnist - q):nsamples_mnist])\n",
        "\n",
        "  # Desnormalizando as imagens\n",
        "  mean, std = 0.1307, 0.3081\n",
        "  datasetGauss = []\n",
        "  # Iterando para cada conjunto\n",
        "  n = int(num_users_mnist/p) # qtd de imagens apartir da primeira\n",
        "  print(\"\\n\")\n",
        "  print(f\"Calculando os valores para {n} Grupos: \")\n",
        "  for user_idx in range(n):  # os primeiros `n` grupos\n",
        "      all_images_np = [train_dataset_mnist[int(i)][0].numpy().squeeze() for i in user_groups_train_mnist[user_idx]]\n",
        "      all_images_np = np.stack(all_images_np)\n",
        "\n",
        "      #Removendo a normalização\n",
        "      all_images_np = all_images_np * std + mean\n",
        "      all_images_np = np.clip(all_images_np, 0, 1)  # Clipping intervalo [0,1]\n",
        "\n",
        "      print(f\"\\nImagens do Grupo {user_idx+1}:\")\n",
        "      fig, axes = plt.subplots(1, plotar, figsize=(15,3))  # Criando 5 subplots lado a lado\n",
        "      for idx, ax in enumerate(axes):\n",
        "          image = all_images_np[idx]\n",
        "          ax.imshow(image, cmap='gray')\n",
        "          ax.set_title(f\"I:{idx+1}\")\n",
        "          ax.axis('off')  # não mostrar os eixos\n",
        "      plt.tight_layout()  # Ajusta o layout\n",
        "      plt.show()\n",
        "\n",
        "      #Calculando o valor da entropia para o Indice atual\n",
        "      train_x, tuplasEntropia, entropiesLocal_ordenado = indicesEntropiaMedianMnist(all_images_np)\n",
        "      e = 2  # imprimir os primeiros e últimos 5 elementos\n",
        "      arr = train_x[0].flatten()\n",
        "      output_str = \"Valores do dataset Original (Chave|Valor): [\" + ' '.join(map(str, arr[:e])) + \" ... \" + ' '.join(map(str, arr[-e:])) + \"] ...\"\n",
        "      print(output_str)\n",
        "      print(\"Valores do Conjunto (Chave|Valor) Não-Ordenado: \", tuplasEntropia[:q], \" ...\")\n",
        "      print(\"Valores do Conjunto (Chave|Valor) Ordenado: \", entropiesLocal_ordenado[:q] , \" ...\")\n",
        "\n",
        "      print(\"\\n\")\n",
        "      valEntr = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "      xEnt = np.array(valEntr)\n",
        "      #Shapiro-Wilk:\n",
        "      statSW, pSW = stats.shapiro(xEnt)  # estatística de teste e o valor-p\n",
        "      validacaoSW = \"Não Rejeitamos H0\" if pSW > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. Shapiro-Wilk, Statistica: %.13f\" % statSW, \"Valor-p: %.10f\" % pSW, \"Validação de Hipotese H0:\", validacaoSW)\n",
        "      #Kolmogorov-Smirnov\n",
        "      mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "      statKS, pKS = kstest(xEnt, 'norm', args=(mu, std_dev))\n",
        "      validacaoKS = \"Não Rejeitamos H0\" if pKS > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. Kolmogorov-Smirnov, Statistica: %.10f\" % statKS, \"Valor-p: %.10f\" % pKS, \"Validação de Hipotese H0:\", validacaoKS)\n",
        "      #D'Agostino e Pearson's\n",
        "      statAP, pAP = normaltest(xEnt)\n",
        "      validacaoAP = \"Não Rejeitamos H0\" if pAP > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. D'Agostino e Pearson's, Statistica: %.10f\" % statAP, \"Valor-p: %.10f\" % pAP, \"Validação de Hipotese H0:\", validacaoAP)\n",
        "\n",
        "      if (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\") or \\\n",
        "        (validacaoSW == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\") or \\\n",
        "        (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\"):\n",
        "        print(\"Com base em uma validação dupla identificamos de que não há evidência suficiente para rejeitar a hipótese nula. A amostra parece seguir uma distribuição normal.\")\n",
        "        datasetGauss.append(train_x)\n",
        "      print(\"\\n\")\n",
        "\n",
        "      plt.hist(valEntr, bins=10, edgecolor='black', alpha=0.7, density=True, label=\"Dados\")\n",
        "      xmin, xmax = plt.xlim()\n",
        "      mu, std_dev = np.mean(valEntr), np.std(valEntr)\n",
        "      x = np.linspace(xmin, xmax, 100)\n",
        "      y = norm.pdf(x, mu, std_dev)\n",
        "      plt.plot(x, y, 'k-', lw=2, label='Curva Gaussiana')\n",
        "      plt.title(f'Histograma dos Dados & Curva Gaussiana para o Usuário {user_idx}')\n",
        "      plt.xlabel('Valor')\n",
        "      plt.ylabel('Frequência')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "  print(\"Total de Distribuições Gaussianas Encontradas: \", len(datasetGauss), \"| Total de Usuarios de até: \", len(user_groups_train_mnist))\n",
        "  return datasetGauss"
      ],
      "metadata": {
        "id": "ptc9kj3zROjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetGauss = mnistNONIDD(num_users_mnist = 400, nclass_mnist = 2, nsamples_mnist = 20, rate_unbalance_mnist = 1.0, p = 1, q = 5, plotar = 5)"
      ],
      "metadata": {
        "id": "AwbGgLjOeilk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset [Cifar10]"
      ],
      "metadata": {
        "id": "9w1pBjBd0N4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions [DADOS IDD]"
      ],
      "metadata": {
        "id": "ng8J3x-c6jVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import entropy\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import time\n",
        "from cifar10_noniid import get_dataset_cifar10_extr_noniid\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import kstest\n",
        "from scipy.stats import normaltest\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def entropiaBR(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "def pegarDadosIDDCifar():\n",
        "  num_users_cifar = 1\n",
        "  nclass_cifar = 9\n",
        "  nsamples_cifar = 1\n",
        "  rate_unbalance_cifar = 0.0\n",
        "  train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar = get_dataset_cifar10_extr_noniid(num_users_cifar, nclass_cifar, nsamples_cifar, rate_unbalance_cifar)\n",
        "  print(train_dataset_cifar)\n",
        "  all_images = [img[0].numpy().squeeze() for img in train_dataset_cifar]\n",
        "  all_images_np = np.stack(all_images)\n",
        "  #Removendo as mudanças da normaliação\n",
        "  mean_rgb = np.array([0.4914, 0.4822, 0.4465])\n",
        "  std_rgb = np.array([0.2023, 0.1994, 0.201])\n",
        "  all_images_np = all_images_np * std_rgb[:, None, None] + mean_rgb[:, None, None]\n",
        "  #all_images_np = np.transpose(all_images_np[0], (1, 2, 0))\n",
        "  print(all_images_np.shape)\n",
        "  # selecionando a primeira imagem\n",
        "  image = all_images_np[0]\n",
        "  # exibir a imagem\n",
        "  image = image\n",
        "  plt.show()\n",
        "  all_labels = [lbl for _, lbl in train_dataset_cifar]\n",
        "  plt.hist(all_labels, bins=10, edgecolor='black', alpha=0.7)\n",
        "  plt.title('Class Distribution Histogram')\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "  return train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar, all_images_np\n",
        "\n",
        "def iEMC(train_X, base = 0, ftupla = 0):\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  print('Valor da Mediana: ', median)\n",
        "  if base == 1:\n",
        "    indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  elif base == 2:\n",
        "    indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] >= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  if base != 0:\n",
        "    train_X = np.array([train_X[i] for i in indices_filtrados])\n",
        "  if ftupla == 1:\n",
        "    # passa os indices selecionados\n",
        "    tuplasEntropia = ([tuplasEntropia[i] for i in indices_filtrados])\n",
        "  return train_X, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "def entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado):\n",
        "  print(\"Tamanho do Dataset: \", len(tuplasEntropia))\n",
        "  print(\"Tamanho do divido pela Mediana: \", len(train_X))\n",
        "  print(\"Entropia das imagens (Não-Ordenado | Original): \", tuplasEntropia[:5], \" ...\")\n",
        "  print(\"Entropia das imagens (Ordenado | Valor): \", entropiesLocal_ordenado[:5], \" ...\")\n",
        "\n",
        "  valEnt = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "  soma = sum(valEnt)\n",
        "  print(\"Entropia Total Acumulada: \", soma)\n",
        "  alvos = [0.25 * soma, 0.5 * soma, 0.75 * soma, soma]\n",
        "  grupos = []\n",
        "  grupo_atual = []\n",
        "  soma_acumulada = 0\n",
        "  index_alvo = 0\n",
        "  for num in valEnt:\n",
        "      grupo_atual.append(num)\n",
        "      soma_acumulada += num\n",
        "      if soma_acumulada >= alvos[index_alvo]:\n",
        "          grupos.append(grupo_atual)\n",
        "          grupo_atual = []\n",
        "          index_alvo += 1\n",
        "          if index_alvo == 4:\n",
        "              break\n",
        "  if grupo_atual:\n",
        "      grupos.append(grupo_atual)\n",
        "\n",
        "  print(\"Tamanho do Conjunto Analisado do Grafico a seguir: \", len(valEnt))\n",
        "  print(\"Tamanho do grupo 1 - 25% dos valores (Q1): \", len(grupos[0]))\n",
        "  print(\"Tamanho do grupo 2 - 50% dos valores (Q2): \", len(grupos[1]))\n",
        "  print(\"Tamanho do grupo 3 - 75% dos valores (Q3): \", len(grupos[2]))\n",
        "  print(\"Tamanho do grupo 4 - 100% dos valores (Q4): \", len(grupos[3]))\n",
        "\n",
        "  # Calcula a média e o desvio padrão dos dados\n",
        "  mu, std = np.mean(valEnt), np.std(valEnt)\n",
        "  # Histograma dos dados\n",
        "  plt.hist(valEnt, bins=10, edgecolor='black', alpha=0.7, density=True, label=\"Dados\")\n",
        "  xmin, xmax = plt.xlim()\n",
        "  x = np.linspace(xmin, xmax, 100)\n",
        "  # Calcula a PDF da distribuição normal para cada valor de x\n",
        "  p = norm.pdf(x, mu, std)\n",
        "  # Plota a curva gaussiana\n",
        "  plt.plot(x, p, 'k', linewidth=2, label=\"Gaussiana\")\n",
        "  plt.title('Data Histogram & Gaussian Curve')\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def hipoteseNula(tuplasEntropia):\n",
        "  print(\"\\n\")\n",
        "  xEnt = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "  mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "  #Shapiro-Wilk:\n",
        "  statSW, pSW = stats.shapiro(xEnt)  # Obtém a estatística de teste e o valor-p\n",
        "  validacaoSW = \"Não Rejeitamos H0\" if pSW > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. Shapiro-Wilk, Statistica: %.13f\" % statSW, \"Valor-p: %.10f\" % pSW, \"Validação de Hipotese H0:\", validacaoSW)\n",
        "  #Kolmogorov-Smirnov\n",
        "  mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "  statKS, pKS = kstest(xEnt, 'norm', args=(mu, std_dev))\n",
        "  validacaoKS = \"Não Rejeitamos H0\" if pKS > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. Kolmogorov-Smirnov, Statistica: %.10f\" % statKS, \"Valor-p: %.10f\" % pKS, \"Validação de Hipotese H0:\", validacaoKS)\n",
        "  #D'Agostino e Pearson's\n",
        "  statAP, pAP = normaltest(xEnt)\n",
        "  validacaoAP = \"Não Rejeitamos H0\" if pAP > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. D'Agostino e Pearson's, Statistica: %.10f\" % statAP, \"Valor-p: %.10f\" % pAP, \"Validação de Hipotese H0:\", validacaoAP)\n",
        "\n",
        "  if (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\") or \\\n",
        "    (validacaoSW == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\") or \\\n",
        "    (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\"):\n",
        "    print(\"Com base em uma validação dupla identificamos de que não há evidência suficiente para rejeitar a hipótese nula. A amostra parece seguir uma distribuição normal.\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "AuqAdNOl0ShH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar, all_images_np = pegarDadosIDDCifar()\n",
        "#base = 0, ftupla = 0 : distribuição dos dados completos\n",
        "#base = 1, ftupla = 1 : distribuição dos dados a esquerda da mediana\n",
        "#base = 2, ftupla = 1 : distribuição dos dados a direita da mediana\n",
        "train_X, tuplasEntropia, entropiesLocal_ordenado= iEMC(all_images_np, base = 0, ftupla = 0)\n",
        "hipoteseNula(tuplasEntropia)\n",
        "entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado)"
      ],
      "metadata": {
        "id": "pzbdSCCX-Jsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar, all_images_np = pegarDadosIDDCifar()\n",
        "#base = 0, ftupla = 0 : distribuição dos dados completos\n",
        "#base = 1, ftupla = 1 : distribuição dos dados a esquerda da mediana\n",
        "#base = 2, ftupla = 1 : distribuição dos dados a direita da mediana\n",
        "train_X, tuplasEntropia, entropiesLocal_ordenado= iEMC(all_images_np, base = 1, ftupla = 1)\n",
        "hipoteseNula(tuplasEntropia)\n",
        "entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado)"
      ],
      "metadata": {
        "id": "n7YliSvQ-w02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar, all_images_np = pegarDadosIDDCifar()\n",
        "#base = 0, ftupla = 0 : distribuição dos dados completos\n",
        "#base = 1, ftupla = 1 : distribuição dos dados a esquerda da mediana\n",
        "#base = 2, ftupla = 1 : distribuição dos dados a direita da mediana\n",
        "train_X, tuplasEntropia, entropiesLocal_ordenado= iEMC(all_images_np, base = 2, ftupla = 1)\n",
        "hipoteseNula(tuplasEntropia)\n",
        "entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado)"
      ],
      "metadata": {
        "id": "de9CAEnB-3jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cifar10 [Dados NON-IDD]"
      ],
      "metadata": {
        "id": "NX6cSBvfgayg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cifar10_noniid import get_dataset_cifar10_extr_noniid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "\n",
        "def entropiaBR(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "def indicesEntropiaMedianCifar(train_X):\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  #indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] >= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  print('Valor da Mediana: ', median)\n",
        "  train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "  return train_X, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "def cifar10NONIDD(num_users_cifar = 400, nclass_cifar = 2, nsamples_cifar = 20, rate_unbalance_cifar = 1.0, p = 100, q = 5, plotar = 5):\n",
        "  #Tratamento de erros\n",
        "  num_users_cifar = 400 if num_users_cifar == 0 else num_users_cifar\n",
        "  nclass_cifar = 2 if nclass_cifar == 0 else nclass_cifar\n",
        "  nsamples_cifar = 20 if nsamples_cifar == 0 else  nsamples_cifar\n",
        "  rate_unbalance_cifar = 1.0 if rate_unbalance_cifar == 0.0 else rate_unbalance_cifar\n",
        "  p = 100 if p == 0 else p\n",
        "  q = 5 if q == 0 else q\n",
        "  plotar = 5 if plotar == 0 else plotar\n",
        "\n",
        "  print(\"Dados da Importação: \")\n",
        "  train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar = get_dataset_cifar10_extr_noniid(num_users_cifar, nclass_cifar, nsamples_cifar, rate_unbalance_cifar)\n",
        "  print(\"\\n\")\n",
        "  print(\"Informações Gerais do Conjunto de Dados\")\n",
        "  print(f\"Total de usuarios: {len(user_groups_train_cifar)}\")\n",
        "\n",
        "  if p == 1:\n",
        "   for i in (range(int(len(user_groups_train_cifar)))):\n",
        "     print(f\"Grupo {i}: \", user_groups_train_cifar[i][:q], \" ... \", user_groups_train_cifar[i][(nsamples_cifar - q):nsamples_cifar])   #imprimindo os 5 primeiros e os 5 ultimos de cada grupo\n",
        "  elif p >= 2:\n",
        "    for i in (range(int(len(user_groups_train_cifar)/p))):\n",
        "      print(f\"Grupo {i}: \", user_groups_train_cifar[i][:q], \" ... \", user_groups_train_cifar[i][(nsamples_cifar - q):nsamples_cifar])   #imprimindo os 5 primeiros e os 5 ultimos de cada grupo\n",
        "    print(\"\\n\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"\\n\")\n",
        "    g = (len(user_groups_train_cifar) - int(len(user_groups_train_cifar)/p))\n",
        "    for g in range(g, len(user_groups_train_cifar)):\n",
        "      print(f\"Grupo {g}: \", user_groups_train_cifar[g][:q], \" ... \", user_groups_train_cifar[g][(nsamples_cifar - q):nsamples_cifar])\n",
        "\n",
        "  # Desnormalizando as imagens\n",
        "  mean_rgb = np.array([0.4914, 0.4822, 0.4465])\n",
        "  std_rgb = np.array([0.2023, 0.1994, 0.201])\n",
        "\n",
        "  # Iterando sobre cada conjunto de índices de usuário\n",
        "  n = int(num_users_cifar/p) #qtd de imagens apartir da primeira\n",
        "  print(\"\\n\")\n",
        "  print(f\"Calculando os valores para {n} Grupos: \")\n",
        "  datasetGauss = []\n",
        "  for user_idx in range(n):\n",
        "      # Convertendo índices flutuantes para inteiros para obter as imagens usando esses índices\n",
        "      all_images = [train_dataset_cifar[int(i)][0].numpy() for i in user_groups_train_cifar[user_idx]]\n",
        "      all_images_np = np.stack(all_images)\n",
        "      all_images_np = all_images_np * std_rgb[:, None, None] + mean_rgb[:, None, None]\n",
        "      all_images_np = np.clip(all_images_np, 0, 1) # Clipping para garantir que a imagem esteja no intervalo [0,1]\n",
        "      ##################################################################################\n",
        "      print('\\n')\n",
        "      print('...... ......  ...... ......  ......  ......  ......  ...... ...... ...... ...... ...... ......  ...... ......  ......  ......  ......  ...... ...... ...... ......')\n",
        "      print(f\"Imagens do Grupo {user_idx}: \")\n",
        "      fig, axes = plt.subplots(1, plotar, figsize=(15,3))  # Criando 5 subplots lado a lado\n",
        "      for idx, ax in enumerate(axes):\n",
        "          image = all_images_np[idx].transpose(1, 2, 0)\n",
        "          ax.imshow(image)\n",
        "          ax.set_title(f\"Img {idx+1}\")\n",
        "          ax.axis('off')  # Para não mostrar os eixos\n",
        "      plt.tight_layout()  # Ajusta o layout para evitar sobreposições\n",
        "      plt.show()\n",
        "\n",
        "      ##################################################################################\n",
        "      #Calculando o valor da entropia para o Indice atual\n",
        "      train_x, tuplasEntropia, entropiesLocal_ordenado = indicesEntropiaMedianCifar(all_images_np)\n",
        "      e = 2  # para imprimir os primeiros e últimos 5 elementos\n",
        "      arr = train_x[0].flatten()\n",
        "      output_str = \"Valores do dataset Original (Chave|Valor): [\" + ' '.join(map(str, arr[:e])) + \" ... \" + ' '.join(map(str, arr[-e:])) + \"] ...\"\n",
        "      print(output_str)\n",
        "      print(\"Valores do Conjunto (Chave|Valor) Não-Ordenado: \", tuplasEntropia[:q], \" ...\")\n",
        "      print(\"Valores do Conjunto (Chave|Valor) Ordenado: \", entropiesLocal_ordenado[:q] , \" ...\")\n",
        "\n",
        "\n",
        "      print(\"\\n\")\n",
        "      valEntr = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "      xEnt = np.array(valEntr)\n",
        "      mu, std = np.mean(valEntr), np.std(valEntr)\n",
        "      mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "      #Shapiro-Wilk:\n",
        "      statSW, pSW = stats.shapiro(xEnt)  # Obtém a estatística de teste e o valor-p\n",
        "      validacaoSW = \"Não Rejeitamos H0\" if pSW > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. Shapiro-Wilk, Statistica: %.13f\" % statSW, \"Valor-p: %.10f\" % pSW, \"Validação de Hipotese H0:\", validacaoSW)\n",
        "      #Kolmogorov-Smirnov\n",
        "      mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "      statKS, pKS = kstest(xEnt, 'norm', args=(mu, std_dev))\n",
        "      validacaoKS = \"Não Rejeitamos H0\" if pKS > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. Kolmogorov-Smirnov, Statistica: %.10f\" % statKS, \"Valor-p: %.10f\" % pKS, \"Validação de Hipotese H0:\", validacaoKS)\n",
        "      #D'Agostino e Pearson's\n",
        "      statAP, pAP = normaltest(xEnt)\n",
        "      validacaoAP = \"Não Rejeitamos H0\" if pAP > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. D'Agostino e Pearson's, Statistica: %.10f\" % statAP, \"Valor-p: %.10f\" % pAP, \"Validação de Hipotese H0:\", validacaoAP)\n",
        "\n",
        "      if (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\") or \\\n",
        "        (validacaoSW == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\") or \\\n",
        "        (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\"):\n",
        "        print(\"Com base em uma validação dupla identificamos de que não há evidência suficiente para rejeitar a hipótese nula. A amostra parece seguir uma distribuição normal.\")\n",
        "        datasetGauss.append(train_x)\n",
        "      print(\"\\n\")\n",
        "\n",
        "\n",
        "      plt.hist(valEntr, bins=10, edgecolor='black', alpha=0.7, density=True, label=\"Dados\")\n",
        "      xmin, xmax = plt.xlim()\n",
        "      x = np.linspace(xmin, xmax, 100)\n",
        "      p = norm.pdf(x, mu, std)\n",
        "      plt.plot(x, p, 'k', linewidth=2, label=\"Gaussiana\")\n",
        "      plt.title(f'Histograma dos Dados & Curva Gaussiana para o Usuário {user_idx}')\n",
        "      plt.xlabel('Valor')\n",
        "      plt.ylabel('Frequência')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "  print(\"Total de Distribuições Gaussianas Encontradas: \", len(datasetGauss), \"| Total de Usuarios de até: \", len(user_groups_train_cifar))\n",
        "  return datasetGauss"
      ],
      "metadata": {
        "id": "3F4nkOVWUsf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetGauss = cifar10NONIDD(num_users_cifar = 400, nclass_cifar = 2, nsamples_cifar = 20, rate_unbalance_cifar = 1.0, p = 1, q = 5, plotar = 5)"
      ],
      "metadata": {
        "id": "2d-ucgNjn7-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset [Cifar100]"
      ],
      "metadata": {
        "id": "3VssG2-mb4Fp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cifar100 Functions [data IDD]"
      ],
      "metadata": {
        "id": "OsJfsPFWcKJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import entropy\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "import time\n",
        "from cifar100_noniid import get_dataset_cifar100_extr_noniid\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import kstest\n",
        "from scipy.stats import normaltest\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def entropiaBR(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "def pegarDadosIDDCifar100():\n",
        "  num_users_cifar = 1\n",
        "  nclass_cifar = 9\n",
        "  nsamples_cifar = 1\n",
        "  rate_unbalance_cifar = 0.0\n",
        "  train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar = get_dataset_cifar100_extr_noniid(num_users_cifar, nclass_cifar, nsamples_cifar, rate_unbalance_cifar)\n",
        "  print(train_dataset_cifar)\n",
        "  all_images = [img[0].numpy().squeeze() for img in train_dataset_cifar]\n",
        "  all_images_np = np.stack(all_images)\n",
        "  #Removendo a mudança ao qual deixa os dados\n",
        "  mean_rgb = np.array([0.5071, 0.4867, 0.4408])\n",
        "  std_rgb = np.array([0.2675, 0.2565, 0.2761])\n",
        "\n",
        "  all_images_np = all_images_np * std_rgb[:, None, None] + mean_rgb[:, None, None]\n",
        "  print(all_images_np.shape)\n",
        "  image = all_images_np[0]\n",
        "  image = image\n",
        "  plt.show()\n",
        "  all_labels = [lbl for _, lbl in train_dataset_cifar]\n",
        "  plt.hist(all_labels, bins=10, edgecolor='black', alpha=0.7)\n",
        "  plt.title('Class Distribution Histogram')\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "  return train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar, all_images_np\n",
        "\n",
        "def iEMC(train_X, base = 0, ftupla = 0):\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  print('Valor da Mediana: ', median)\n",
        "  if base == 1:\n",
        "    indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  elif base == 2:\n",
        "    indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] >= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  if base != 0:\n",
        "    train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "  if ftupla == 1:\n",
        "    # passa os indices selecionados\n",
        "    tuplasEntropia = ([tuplasEntropia[i] for i in indices_filtrados]) #\n",
        "  return train_X, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "def entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado):\n",
        "  print(\"Tamanho do Dataset: \", len(tuplasEntropia))\n",
        "  print(\"Tamanho do divido pela Mediana: \", len(train_X))\n",
        "  print(\"Entropia das imagens (Não-Ordenado | Original): \", tuplasEntropia[:5], \" ...\")\n",
        "  print(\"Entropia das imagens (Ordenado | Valor): \", entropiesLocal_ordenado[:5], \" ...\")\n",
        "\n",
        "  valEnt = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "  soma = sum(valEnt)\n",
        "  print(\"Entropia Total Acumulada: \", soma)\n",
        "  alvos = [0.25 * soma, 0.5 * soma, 0.75 * soma, soma]\n",
        "  grupos = []\n",
        "  grupo_atual = []\n",
        "  soma_acumulada = 0\n",
        "  index_alvo = 0\n",
        "  for num in valEnt:\n",
        "      grupo_atual.append(num)\n",
        "      soma_acumulada += num\n",
        "      if soma_acumulada >= alvos[index_alvo]:\n",
        "          grupos.append(grupo_atual)\n",
        "          grupo_atual = []\n",
        "          index_alvo += 1\n",
        "          if index_alvo == 4:\n",
        "              break\n",
        "  if grupo_atual:\n",
        "      grupos.append(grupo_atual)\n",
        "\n",
        "  print(\"Tamanho do Conjunto Analisado do Grafico a seguir: \", len(valEnt))\n",
        "  print(\"Tamanho do grupo 1 - 25% dos valores (Q1): \", len(grupos[0]))\n",
        "  print(\"Tamanho do grupo 2 - 50% dos valores (Q2): \", len(grupos[1]))\n",
        "  print(\"Tamanho do grupo 3 - 75% dos valores (Q3): \", len(grupos[2]))\n",
        "  print(\"Tamanho do grupo 4 - 100% dos valores (Q4): \", len(grupos[3]))\n",
        "\n",
        "  # Calcula a média e o desvio padrão dos dados\n",
        "  mu, std = np.mean(valEnt), np.std(valEnt)\n",
        "  # Histograma dos dados\n",
        "  plt.hist(valEnt, bins=10, edgecolor='black', alpha=0.7, density=True, label=\"Dados\")\n",
        "  xmin, xmax = plt.xlim()\n",
        "  x = np.linspace(xmin, xmax, 100)\n",
        "  p = norm.pdf(x, mu, std)\n",
        "  # Plota a curva gaussiana\n",
        "  plt.plot(x, p, 'k', linewidth=2, label=\"Gaussiana\")\n",
        "  # Configurações adicionais e exibição do gráfico\n",
        "  plt.title('Data Histogram & Gaussian Curve')\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def hipoteseNula(tuplasEntropia):\n",
        "  print(\"\\n\")\n",
        "  xEnt = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "  mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "  #Shapiro-Wilk:\n",
        "  statSW, pSW = stats.shapiro(xEnt)  # Obtém a estatística de teste e o valor-p\n",
        "  validacaoSW = \"Não Rejeitamos H0\" if pSW > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. Shapiro-Wilk, Statistica: %.13f\" % statSW, \"Valor-p: %.10f\" % pSW, \"Validação de Hipotese H0:\", validacaoSW)\n",
        "  #Kolmogorov-Smirnov\n",
        "  mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "  statKS, pKS = kstest(xEnt, 'norm', args=(mu, std_dev))\n",
        "  validacaoKS = \"Não Rejeitamos H0\" if pKS > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. Kolmogorov-Smirnov, Statistica: %.10f\" % statKS, \"Valor-p: %.10f\" % pKS, \"Validação de Hipotese H0:\", validacaoKS)\n",
        "  #D'Agostino e Pearson's\n",
        "  statAP, pAP = normaltest(xEnt)\n",
        "  validacaoAP = \"Não Rejeitamos H0\" if pAP > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. D'Agostino e Pearson's, Statistica: %.10f\" % statAP, \"Valor-p: %.10f\" % pAP, \"Validação de Hipotese H0:\", validacaoAP)\n",
        "\n",
        "  if (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\") or \\\n",
        "    (validacaoSW == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\") or \\\n",
        "    (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\"):\n",
        "    print(\"Com base em uma validação dupla identificamos de que não há evidência suficiente para rejeitar a hipótese nula. A amostra parece seguir uma distribuição normal.\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "YNIOObOFhHLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar, all_images_np = pegarDadosIDDCifar100()\n",
        "#base = 0, ftupla = 0 : distribuição dos dados completos\n",
        "#base = 1, ftupla = 1 : distribuição dos dados a esquerda da mediana\n",
        "#base = 2, ftupla = 1 : distribuição dos dados a direita da mediana\n",
        "train_X, tuplasEntropia, entropiesLocal_ordenado= iEMC(all_images_np, base = 0, ftupla = 0)\n",
        "hipoteseNula(tuplasEntropia)\n",
        "entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado)"
      ],
      "metadata": {
        "id": "CBXaW-svifDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cifar100 [data NON-IDD]"
      ],
      "metadata": {
        "id": "9Irsyk0bcMDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cifar100_noniid import get_dataset_cifar100_extr_noniid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "\n",
        "def entropiaBR(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "def indicesEntropiaMedianCifar(train_X):\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  print('Valor da Mediana: ', median)\n",
        "  train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "  return train_X, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "def cifar100NONIDD(num_users_cifar = 400, nclass_cifar = 2, nsamples_cifar = 20, rate_unbalance_cifar = 1.0, p = 100, q = 5, plotar = 5):\n",
        "  #Tratamento de erros\n",
        "  num_users_cifar = 400 if num_users_cifar == 0 else num_users_cifar\n",
        "  nclass_cifar = 2 if nclass_cifar == 0 else nclass_cifar\n",
        "  nsamples_cifar = 20 if nsamples_cifar == 0 else  nsamples_cifar\n",
        "  rate_unbalance_cifar = 1.0 if rate_unbalance_cifar == 0.0 else rate_unbalance_cifar\n",
        "  p = 100 if p == 0 else p\n",
        "  q = 5 if q == 0 else q\n",
        "  plotar = 5 if plotar == 0 else plotar\n",
        "\n",
        "  print(\"Dados da Importação: \")\n",
        "  train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar = get_dataset_cifar100_extr_noniid(num_users_cifar, nclass_cifar, nsamples_cifar, rate_unbalance_cifar)\n",
        "  print(\"\\n\")\n",
        "  print(\"Informações Gerais do Conjunto de Dados\")\n",
        "  print(f\"Total de usuarios: {len(user_groups_train_cifar)}\")\n",
        "\n",
        "  if p == 1:\n",
        "   for i in (range(int(len(user_groups_train_cifar)))):\n",
        "     print(f\"Grupo {i}: \", user_groups_train_cifar[i][:q], \" ... \", user_groups_train_cifar[i][(nsamples_cifar - q):nsamples_cifar])   #imprimindo os 5 primeiros e os 5 ultimos de cada grupo\n",
        "  elif p >= 2:\n",
        "    for i in (range(int(len(user_groups_train_cifar)/p))):\n",
        "      print(f\"Grupo {i}: \", user_groups_train_cifar[i][:q], \" ... \", user_groups_train_cifar[i][(nsamples_cifar - q):nsamples_cifar])   #imprimindo os 5 primeiros e os 5 ultimos de cada grupo\n",
        "    print(\"\\n\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"\\n\")\n",
        "    g = (len(user_groups_train_cifar) - int(len(user_groups_train_cifar)/p))\n",
        "    for g in range(g, len(user_groups_train_cifar)):\n",
        "      print(f\"Grupo {g}: \", user_groups_train_cifar[g][:q], \" ... \", user_groups_train_cifar[g][(nsamples_cifar - q):nsamples_cifar])\n",
        "\n",
        "  # Desnormalizando as imagens\n",
        "  mean_rgb = np.array([0.5071, 0.4867, 0.4408])\n",
        "  std_rgb = np.array([0.2675, 0.2565, 0.2761])\n",
        "\n",
        "  # Iterando sobre cada conjunto de índices de usuário\n",
        "  n = int(num_users_cifar/p) #qtd de imagens apartir da primeira\n",
        "  print(\"\\n\")\n",
        "  print(f\"Calculando os valores para {n} Grupos: \")\n",
        "  datasetGauss = []\n",
        "  for user_idx in range(n):\n",
        "      # Convertendo índices flutuantes para inteiros e obtemos as imagens usando esses índices\n",
        "      all_images = [train_dataset_cifar[int(i)][0].numpy() for i in user_groups_train_cifar[user_idx]]\n",
        "      all_images_np = np.stack(all_images)\n",
        "      all_images_np = all_images_np * std_rgb[:, None, None] + mean_rgb[:, None, None]\n",
        "      all_images_np = np.clip(all_images_np, 0, 1) # Clipping para garantir que a imagem esteja no intervalo [0,1]\n",
        "      ##################################################################################\n",
        "      print('\\n')\n",
        "      print('...... ......  ...... ......  ......  ......  ......  ...... ...... ...... ...... ...... ......  ...... ......  ......  ......  ......  ...... ...... ...... ......')\n",
        "      print(f\"Imagens do Grupo {user_idx}: \")\n",
        "      fig, axes = plt.subplots(1, plotar, figsize=(15,3))  # Criando 5 subplots lado a lado\n",
        "      for idx, ax in enumerate(axes):\n",
        "          image = all_images_np[idx].transpose(1, 2, 0)\n",
        "          ax.imshow(image)\n",
        "          ax.set_title(f\"Img {idx+1}\")\n",
        "          ax.axis('off')  # Para não mostrar os eixos\n",
        "      plt.tight_layout()  # Ajusta o layout para evitar sobreposições\n",
        "      plt.show()\n",
        "\n",
        "      ##################################################################################\n",
        "      #Calculando o valor da entropia para o Indice atual\n",
        "      train_x, tuplasEntropia, entropiesLocal_ordenado = indicesEntropiaMedianCifar(all_images_np)\n",
        "      e = 2  # para imprimir os primeiros e últimos 5 elementos\n",
        "      arr = train_x[0].flatten()\n",
        "      output_str = \"Valores do dataset Original (Chave|Valor): [\" + ' '.join(map(str, arr[:e])) + \" ... \" + ' '.join(map(str, arr[-e:])) + \"] ...\"\n",
        "      print(output_str)\n",
        "      print(\"Valores do Conjunto (Chave|Valor) Não-Ordenado: \", tuplasEntropia[:q], \" ...\")\n",
        "      print(\"Valores do Conjunto (Chave|Valor) Ordenado: \", entropiesLocal_ordenado[:q] , \" ...\")\n",
        "\n",
        "\n",
        "      print(\"\\n\")\n",
        "      valEntr = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "      xEnt = np.array(valEntr)\n",
        "      mu, std = np.mean(valEntr), np.std(valEntr)\n",
        "      mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "      #Shapiro-Wilk:\n",
        "      statSW, pSW = stats.shapiro(xEnt)  # Obtém a estatística de teste e o valor-p\n",
        "      validacaoSW = \"Não Rejeitamos H0\" if pSW > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. Shapiro-Wilk, Statistica: %.13f\" % statSW, \"Valor-p: %.10f\" % pSW, \"Validação de Hipotese H0:\", validacaoSW)\n",
        "      #Kolmogorov-Smirnov\n",
        "      mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "      statKS, pKS = kstest(xEnt, 'norm', args=(mu, std_dev))\n",
        "      validacaoKS = \"Não Rejeitamos H0\" if pKS > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. Kolmogorov-Smirnov, Statistica: %.10f\" % statKS, \"Valor-p: %.10f\" % pKS, \"Validação de Hipotese H0:\", validacaoKS)\n",
        "      #D'Agostino e Pearson's\n",
        "      statAP, pAP = normaltest(xEnt)\n",
        "      validacaoAP = \"Não Rejeitamos H0\" if pAP > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. D'Agostino e Pearson's, Statistica: %.10f\" % statAP, \"Valor-p: %.10f\" % pAP, \"Validação de Hipotese H0:\", validacaoAP)\n",
        "\n",
        "      if (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\") or \\\n",
        "        (validacaoSW == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\") or \\\n",
        "        (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\"):\n",
        "        print(\"Com base em uma validação dupla identificamos de que não há evidência suficiente para rejeitar a hipótese nula. A amostra parece seguir uma distribuição normal.\")\n",
        "        datasetGauss.append(train_x)\n",
        "      print(\"\\n\")\n",
        "\n",
        "\n",
        "      plt.hist(valEntr, bins=10, edgecolor='black', alpha=0.7, density=True, label=\"Dados\")\n",
        "      xmin, xmax = plt.xlim()\n",
        "      x = np.linspace(xmin, xmax, 100)\n",
        "      p = norm.pdf(x, mu, std)\n",
        "      plt.plot(x, p, 'k', linewidth=2, label=\"Gaussiana\")\n",
        "      plt.title(f'Histograma dos Dados & Curva Gaussiana para o Usuário {user_idx}')\n",
        "      plt.xlabel('Valor')\n",
        "      plt.ylabel('Frequência')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "  print(\"Total de Distribuições Gaussianas Encontradas: \", len(datasetGauss), \"| Total de Usuarios de até: \", len(user_groups_train_cifar))\n",
        "  return datasetGauss"
      ],
      "metadata": {
        "id": "_ISmVAU_otH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetGauss = cifar100NONIDD(num_users_cifar = 400, nclass_cifar = 3, nsamples_cifar = 20, rate_unbalance_cifar = 1.0, p = 1, q = 5, plotar = 5)"
      ],
      "metadata": {
        "id": "Sdolkj9ipDd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Fashion Mnist"
      ],
      "metadata": {
        "id": "bc7sBWuSxaLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funções [IDD DATASET]"
      ],
      "metadata": {
        "id": "yB92R2Er3z_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import entropy\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import time\n",
        "from fashionmnist_noniid import get_dataset_fashion_mnist_extr_noniid\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import kstest\n",
        "from scipy.stats import normaltest\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def entropiaBR(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "def pegarDadosIDDFashion():\n",
        "  num_users_fashion = 1\n",
        "  nclass_fashion = 9\n",
        "  nsamples_fashion = 1\n",
        "  rate_unbalance_fashion = 0.0\n",
        "  train_dataset_fashion, test_dataset_fashion, user_groups_train_fashion, user_groups_test_fashion = get_dataset_fashion_mnist_extr_noniid(num_users_fashion, nclass_fashion, nsamples_fashion, rate_unbalance_fashion)\n",
        "  print(train_dataset_fashion)\n",
        "  all_images = [img[0].numpy().squeeze() for img in train_dataset_fashion]\n",
        "  all_images_np = np.stack(all_images)\n",
        "  #Removendo a normalização\n",
        "  mean, std = 0.2860, 0.3530\n",
        "  all_images_np = all_images_np * std + mean\n",
        "  print(all_images_np.shape)\n",
        "  plt.show()\n",
        "  all_labels = [lbl for _, lbl in train_dataset_fashion]\n",
        "  plt.hist(all_labels, bins=10, edgecolor='black', alpha=0.7)\n",
        "  plt.title('Class Distribution Histogram')\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "  return train_dataset_fashion, test_dataset_fashion, user_groups_train_fashion, user_groups_test_fashion, all_images_np\n",
        "\n",
        "def iEMC(train_X, base = 0, ftupla = 0):\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  print('Valor da Mediana: ', median)\n",
        "  if base == 1:\n",
        "    indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  elif base == 2:\n",
        "    indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] >= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  if base != 0:\n",
        "    train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
        "  if ftupla == 1:\n",
        "    tuplasEntropia = ([tuplasEntropia[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "  return train_X, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "def entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado):\n",
        "  print(\"Tamanho do Dataset: \", len(tuplasEntropia))\n",
        "  print(\"Tamanho do divido pela Mediana: \", len(train_X))\n",
        "  print(\"Entropia das imagens (Não-Ordenado | Original): \", tuplasEntropia[:5], \" ...\")\n",
        "  print(\"Entropia das imagens (Ordenado | Valor): \", entropiesLocal_ordenado[:5], \" ...\")\n",
        "\n",
        "  valEnt = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "  soma = sum(valEnt)\n",
        "  print(\"Entropia Total Acumulada: \", soma)\n",
        "  alvos = [0.25 * soma, 0.5 * soma, 0.75 * soma, soma]\n",
        "  grupos = []\n",
        "  grupo_atual = []\n",
        "  soma_acumulada = 0\n",
        "  index_alvo = 0\n",
        "  for num in valEnt:\n",
        "      grupo_atual.append(num)\n",
        "      soma_acumulada += num\n",
        "      if soma_acumulada >= alvos[index_alvo]:\n",
        "          grupos.append(grupo_atual)\n",
        "          grupo_atual = []\n",
        "          index_alvo += 1\n",
        "          if index_alvo == 4:\n",
        "              break\n",
        "  if grupo_atual:\n",
        "      grupos.append(grupo_atual)\n",
        "\n",
        "  print(\"Tamanho do Conjunto Analisado do Grafico a seguir: \", len(valEnt))\n",
        "  print(\"Tamanho do grupo 1 - 25% dos valores (Q1): \", len(grupos[0]))\n",
        "  print(\"Tamanho do grupo 2 - 50% dos valores (Q2): \", len(grupos[1]))\n",
        "  print(\"Tamanho do grupo 3 - 75% dos valores (Q3): \", len(grupos[2]))\n",
        "  print(\"Tamanho do grupo 4 - 100% dos valores (Q4): \", len(grupos[3]))\n",
        "\n",
        "  # Calcula a média e o desvio padrão dos dados\n",
        "  mu, std = np.mean(valEnt), np.std(valEnt)\n",
        "  # Histograma dos dados\n",
        "  plt.hist(valEnt, bins=10, edgecolor='black', alpha=0.7, density=True, label=\"Dados\")\n",
        "  xmin, xmax = plt.xlim()\n",
        "  x = np.linspace(xmin, xmax, 100)\n",
        "  p = norm.pdf(x, mu, std)\n",
        "  # Plota a curva gaussiana\n",
        "  plt.plot(x, p, 'k', linewidth=2, label=\"Gaussiana\")\n",
        "  # Configurações adicionais e exibição do gráfico\n",
        "  plt.title('Data Histogram & Gaussian Curve')\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def hipoteseNula(tuplasEntropia):\n",
        "  print(\"\\n\")\n",
        "  xEnt = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "  mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "  #Shapiro-Wilk:\n",
        "  statSW, pSW = stats.shapiro(xEnt)  # Obtém a estatística de teste e o valor-p\n",
        "  validacaoSW = \"Não Rejeitamos H0\" if pSW > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. Shapiro-Wilk, Statistica: %.13f\" % statSW, \"Valor-p: %.10f\" % pSW, \"Validação de Hipotese H0:\", validacaoSW)\n",
        "  #Kolmogorov-Smirnov\n",
        "  mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "  statKS, pKS = kstest(xEnt, 'norm', args=(mu, std_dev))\n",
        "  validacaoKS = \"Não Rejeitamos H0\" if pKS > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. Kolmogorov-Smirnov, Statistica: %.10f\" % statKS, \"Valor-p: %.10f\" % pKS, \"Validação de Hipotese H0:\", validacaoKS)\n",
        "  #D'Agostino e Pearson's\n",
        "  statAP, pAP = normaltest(xEnt)\n",
        "  validacaoAP = \"Não Rejeitamos H0\" if pAP > 0.05 else \"Rejeitamos H0\"\n",
        "  print(\"Cal. D'Agostino e Pearson's, Statistica: %.10f\" % statAP, \"Valor-p: %.10f\" % pAP, \"Validação de Hipotese H0:\", validacaoAP)\n",
        "\n",
        "  if (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\") or \\\n",
        "    (validacaoSW == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\") or \\\n",
        "    (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\"):\n",
        "    print(\"Com base em uma validação dupla identificamos de que não há evidência suficiente para rejeitar a hipótese nula. A amostra parece seguir uma distribuição normal.\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "Xl7SX2H9xhFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_cifar, test_dataset_cifar, user_groups_train_cifar, user_groups_test_cifar, all_images_np = pegarDadosIDDFashion()\n",
        "#base = 0, ftupla = 0 : distribuição dos dados completos\n",
        "#base = 1, ftupla = 1 : distribuição dos dados a esquerda da mediana\n",
        "#base = 2, ftupla = 1 : distribuição dos dados a direita da mediana\n",
        "train_X, tuplasEntropia, entropiesLocal_ordenado= iEMC(all_images_np, base = 0, ftupla = 0)\n",
        "hipoteseNula(tuplasEntropia)\n",
        "entropiaEstatisticas(train_X, tuplasEntropia, entropiesLocal_ordenado)"
      ],
      "metadata": {
        "id": "stT_VTWK0t7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funções [NON-IDD DATASET]"
      ],
      "metadata": {
        "id": "u_Z_Z0TV3_Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fashionmnist_noniid import get_dataset_fashion_mnist_extr_noniid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import kstest, norm\n",
        "from scipy import stats\n",
        "from scipy.stats import normaltest\n",
        "\n",
        "def entropiaBR(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "def indicesEntropiaMedianFashionMnist(train_X):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  print('Valor da Mediana: ', median)\n",
        "  train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
        "  return train_X, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "def fashionMnistNONIDD(num_users_fashion_mnist = 400, nclass_fashion_mnist = 2, nsamples_fashion_mnist = 20, rate_unbalance_fashion_mnist = 1.0, p = 100, q = 5, plotar = 5):\n",
        "  #Tratamento de erros\n",
        "  num_users_fashion_mnist = 400 if num_users_fashion_mnist == 0 else num_users_fashion_mnist\n",
        "  nclass_fashion_mnist = 2 if nclass_fashion_mnist == 0 else nclass_fashion_mnist\n",
        "  nsamples_fashion_mnist = 20 if nsamples_fashion_mnist == 0 else  nsamples_fashion_mnist\n",
        "  rate_unbalance_fashion_mnist = 1.0 if rate_unbalance_fashion_mnist == 0.0 else rate_unbalance_fashion_mnist\n",
        "  p = 100 if p == 0 else p\n",
        "  q = 5 if q == 0 else q\n",
        "  plotar = 5 if plotar == 0 else plotar\n",
        "\n",
        "  print(\"Dados da Importação: \")\n",
        "  train_dataset_fashion_mnist, test_dataset_fashion_mnist, user_groups_train_fashion_mnist, user_groups_test_fashion_mnist = get_dataset_fashion_mnist_extr_noniid(num_users_fashion_mnist, nclass_fashion_mnist, nsamples_fashion_mnist, rate_unbalance_fashion_mnist)\n",
        "  print(\"\\n\")\n",
        "  print(\"Informações Gerais do Conjunto de Dados\")\n",
        "  print(f\"Total de usuarios: {len(user_groups_train_fashion_mnist)}\")\n",
        "\n",
        "  if p == 1:\n",
        "   for i in (range(int(len(user_groups_train_fashion_mnist)))):\n",
        "     print(f\"Grupo {i}: \", user_groups_train_fashion_mnist[i][:q], \" ... \", user_groups_train_fashion_mnist[i][(nsamples_fashion_mnist - q):nsamples_fashion_mnist])   #imprimindo os 5 primeiros e os 5 ultimos de cada grupo\n",
        "  elif p >= 2:\n",
        "    for i in (range(int(len(user_groups_train_fashion_mnist)/p))):\n",
        "      print(f\"Grupo {i}: \", user_groups_train_fashion_mnist[i][:q], \" ... \", user_groups_train_fashion_mnist[i][(nsamples_fashion_mnist - q):nsamples_fashion_mnist])   #imprimindo os 5 primeiros e os 5 ultimos de cada grupo\n",
        "    print(\"\\n\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"Grupo n:  [...... ......  ...... ......  ......]  ...  [......  ...... ...... ...... ......]\")\n",
        "    print(\"\\n\")\n",
        "    g = (len(user_groups_train_fashion_mnist) - int(len(user_groups_train_fashion_mnist)/p))\n",
        "    for g in range(g, len(user_groups_train_fashion_mnist)):\n",
        "      print(f\"Grupo {g}: \", user_groups_train_fashion_mnist[g][:q], \" ... \", user_groups_train_fashion_mnist[g][(nsamples_fashion_mnist - q):nsamples_fashion_mnist])\n",
        "\n",
        "  # Desnormalizando as imagens\n",
        "  mean, std = 0.2860, 0.3530\n",
        "  datasetGauss = []\n",
        "  # Iterando sobre cada conjunto\n",
        "  n = int(num_users_fashion_mnist/p) # qtd de imagens apartir da primeira\n",
        "  print(\"\\n\")\n",
        "  print(f\"Calculando os valores para {n} Grupos: \")\n",
        "  for user_idx in range(n):  # itera sobre os primeiros `n` grupos\n",
        "      all_images_np = [train_dataset_fashion_mnist[int(i)][0].numpy().squeeze() for i in user_groups_train_fashion_mnist[user_idx]]\n",
        "      all_images_np = np.stack(all_images_np)\n",
        "\n",
        "      #Removendo a mudança ao qual deixa os dados\n",
        "      all_images_np = all_images_np * std + mean\n",
        "      all_images_np = np.clip(all_images_np, 0, 1)  # Clipping no intervalo [0,1]\n",
        "\n",
        "      print(f\"\\nImagens do Grupo {user_idx+1}:\")\n",
        "      fig, axes = plt.subplots(1, plotar, figsize=(15,3))  # Criando 5 subplots lado a lado\n",
        "      for idx, ax in enumerate(axes):\n",
        "          image = all_images_np[idx]\n",
        "          ax.imshow(image, cmap='gray')\n",
        "          ax.set_title(f\"I:{idx+1}\")\n",
        "          ax.axis('off')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      #Calculando o valor da entropia para o Indice atual\n",
        "      train_x, tuplasEntropia, entropiesLocal_ordenado = indicesEntropiaMedianFashionMnist(all_images_np)\n",
        "      e = 2  # para imprimir os primeiros e últimos 5 elementos\n",
        "      arr = train_x[0].flatten()\n",
        "      output_str = \"Valores do dataset Original (Chave|Valor): [\" + ' '.join(map(str, arr[:e])) + \" ... \" + ' '.join(map(str, arr[-e:])) + \"] ...\"\n",
        "      print(output_str)\n",
        "      print(\"Valores do Conjunto (Chave|Valor) Não-Ordenado: \", tuplasEntropia[:q], \" ...\")\n",
        "      print(\"Valores do Conjunto (Chave|Valor) Ordenado: \", entropiesLocal_ordenado[:q] , \" ...\")\n",
        "\n",
        "      print(\"\\n\")\n",
        "      valEntr = [(ento[1]) for ento in (tuplasEntropia)]\n",
        "      xEnt = np.array(valEntr)\n",
        "      #Shapiro-Wilk:\n",
        "      statSW, pSW = stats.shapiro(xEnt)  # Obtém a estatística de teste e o valor-p\n",
        "      validacaoSW = \"Não Rejeitamos H0\" if pSW > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. Shapiro-Wilk, Statistica: %.13f\" % statSW, \"Valor-p: %.10f\" % pSW, \"Validação de Hipotese H0:\", validacaoSW)\n",
        "      #Kolmogorov-Smirnov\n",
        "      mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "      statKS, pKS = kstest(xEnt, 'norm', args=(mu, std_dev))\n",
        "      validacaoKS = \"Não Rejeitamos H0\" if pKS > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. Kolmogorov-Smirnov, Statistica: %.10f\" % statKS, \"Valor-p: %.10f\" % pKS, \"Validação de Hipotese H0:\", validacaoKS)\n",
        "      #D'Agostino e Pearson's\n",
        "      statAP, pAP = normaltest(xEnt)\n",
        "      validacaoAP = \"Não Rejeitamos H0\" if pAP > 0.05 else \"Rejeitamos H0\"\n",
        "      print(\"Cal. D'Agostino e Pearson's, Statistica: %.10f\" % statAP, \"Valor-p: %.10f\" % pAP, \"Validação de Hipotese H0:\", validacaoAP)\n",
        "\n",
        "      if (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\") or \\\n",
        "        (validacaoSW == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\") or \\\n",
        "        (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\"):\n",
        "        print(\"Com base em uma validação dupla identificamos de que não há evidência suficiente para rejeitar a hipótese nula. A amostra parece seguir uma distribuição normal.\")\n",
        "        datasetGauss.append(train_x)\n",
        "      print(\"\\n\")\n",
        "\n",
        "      plt.hist(valEntr, bins=10, edgecolor='black', alpha=0.7, density=True, label=\"Dados\")\n",
        "      xmin, xmax = plt.xlim()\n",
        "      mu, std_dev = np.mean(valEntr), np.std(valEntr)\n",
        "      x = np.linspace(xmin, xmax, 100)\n",
        "      y = norm.pdf(x, mu, std_dev)\n",
        "      plt.plot(x, y, 'k-', lw=2, label='Curva Gaussiana')\n",
        "      plt.title(f'Histograma dos Dados & Curva Gaussiana para o Usuário {user_idx}')\n",
        "      plt.xlabel('Valor')\n",
        "      plt.ylabel('Frequência')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "  print(\"Total de Distribuições Gaussianas Encontradas: \", len(datasetGauss), \"| Total de Usuarios de até: \", len(user_groups_train_fashion_mnist))\n",
        "  return datasetGauss"
      ],
      "metadata": {
        "id": "bzxZMoFX3968"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetGauss = fashionMnistNONIDD(num_users_fashion_mnist = 400, nclass_fashion_mnist = 2, nsamples_fashion_mnist = 20, rate_unbalance_fashion_mnist = 1.0, p = 1, q = 5, plotar = 5)"
      ],
      "metadata": {
        "id": "As_qgaoN8n3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}