{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Description\n",
        "\n",
        "Experiments conducted for the centralized data scenario. In this way, we seek to observe the behavior of entropy, its performance, and relationships.\n",
        "\n",
        "Author: Ernesto Gurgel Valente Neto\n"
      ],
      "metadata": {
        "id": "w23vdkHswOK8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ9EZXXfiN6j"
      },
      "source": [
        "# MNIST\n",
        "* A dataset of handwritten digits, containing 70,000 grayscale images of 28x28 pixels, often used for training in image recognition techniques and machine learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPhyGa9t-s5Y"
      },
      "source": [
        "## Creation and Definition of All Main Functions\n",
        "\n",
        "1. Entropy Filter\n",
        "2. Loading, Preprocessing, and Handling\n",
        "3. Model Creation\n",
        "4. Statistical Predictions\n",
        "5. Accuracy and Loss Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLGOTl6X1wcb"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def entropia(pk, base=2, max=1):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    return result\n",
        "\n",
        "def filtrar_entropia_classe(train_X, train_y, totalClasses):  # filtro de entropia\n",
        "    train_Xextend = []\n",
        "    train_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(train_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = train_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        train_Xextend.extend([train_X[i] for i in indices_filtrados_da_classe])\n",
        "        train_yextend.extend([train_y[i] for i in indices_filtrados_da_classe])\n",
        "    return np.array(train_Xextend), np.array(train_yextend)\n",
        "\n",
        "def filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y, totalClasses):  # filtro de entropia\n",
        "    train_Xextend = []\n",
        "    train_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(train_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = train_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        train_Xextend.extend([train_X[i] for i in indices_filtrados_da_classe])\n",
        "        train_yextend.extend([train_y[i] for i in indices_filtrados_da_classe])\n",
        "    test_Xextend = []\n",
        "    test_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(test_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = test_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        test_Xextend.extend([test_X[i] for i in indices_filtrados_da_classe])\n",
        "        test_yextend.extend([test_y[i] for i in indices_filtrados_da_classe])\n",
        "    return np.array(train_Xextend), np.array(train_yextend), np.array(test_Xextend), np.array(test_yextend)\n",
        "\n",
        "def carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, d, c): # Carregamento e Tratamento\n",
        "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  Val_X = Val_X.reshape((Val_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], d, d, c))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  Val_X = Val_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_y = to_categorical(train_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  Val_y = to_categorical(Val_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_y = to_categorical(test_y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "  return train_X, train_y, Val_X, Val_y, test_X, test_y\n",
        "\n",
        "def carregarPepararValidacaoCruzada(train_X, train_y, test_X, test_y, d, c): # Carregamento e Tratamento\n",
        "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], d, d, c))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_y = to_categorical(train_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_y = to_categorical(test_y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "  return train_X, train_y, test_X, test_y\n",
        "\n",
        "def avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y):\n",
        "    # Para o conjunto de TREINAMENTO\n",
        "    print(\"Dados de Treino\")\n",
        "    predictions_train = model.predict(train_X) # Previsões\n",
        "    classePredita_train = np.argmax(predictions_train, axis=1)\n",
        "    classeVerdadeira_train = np.argmax(train_y, axis=1)\n",
        "    cm_train = confusion_matrix(classeVerdadeira_train, classePredita_train)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(cm_train)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Training')\n",
        "    plt.show()\n",
        "\n",
        "    acc_train = accuracy_score(classeVerdadeira_train, classePredita_train) * 100  # Acurácia\n",
        "    print(\"Accuracy (Treinamento): \", '%.3f' % (acc_train))\n",
        "    sens_train = recall_score(classeVerdadeira_train, classePredita_train, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Treinamento): \", '%.3f' % (sens_train))\n",
        "    loss_train = history.history['loss'][-1]\n",
        "    print(\"Loss (Treinamento): \", '%.3f' % loss_train)\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Dados de Validação\")\n",
        "    # Para o conjunto de VALIDAÇÃO\n",
        "    predictions_val = model.predict(Val_X) # Previsões\n",
        "    classePredita_val = np.argmax(predictions_val, axis=1)\n",
        "    classeVerdadeira_val = np.argmax(Val_y, axis=1)\n",
        "    cm_val = confusion_matrix(classeVerdadeira_val, classePredita_val)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(cm_val)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Validation')\n",
        "    plt.show()\n",
        "\n",
        "    acc_val = accuracy_score(classeVerdadeira_val, classePredita_val) * 100  # Acurácia\n",
        "    print(\"Accuracy (Validação): \", '%.3f' % (acc_val))\n",
        "    sens_val = recall_score(classeVerdadeira_val, classePredita_val, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Validação): \", '%.3f' % (sens_val))\n",
        "    loss_val = history.history['val_loss'][-1]\n",
        "    print(\"Loss (Validação): \", '%.3f' % loss_val)\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Dados de Teste\")\n",
        "    # Para o conjunto de TESTE\n",
        "    predictions_test = model.predict(test_X) # Previsões\n",
        "    classePredita_test = np.argmax(predictions_test, axis=1)\n",
        "    classeVerdadeira_test = np.argmax(test_y, axis=1)\n",
        "    cm_test = confusion_matrix(classeVerdadeira_test, classePredita_test)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(cm_test)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Test')\n",
        "    plt.show()\n",
        "\n",
        "    acc_test = accuracy_score(classeVerdadeira_test, classePredita_test) * 100  # Acurácia\n",
        "    print(\"Accuracy (Teste): \", '%.3f' % (acc_test))\n",
        "    sens_test = recall_score(classeVerdadeira_test, classePredita_test, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Teste): \", '%.3f' % (sens_test))\n",
        "    loss_test = history.history['val_loss'][-1]\n",
        "    print(\"Loss (Teste): \", '%.3f' % loss_test)\n",
        "\n",
        "def plotAcuraciaLoss(history): #plots\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  plt.tight_layout()\n",
        "\n",
        "def plotarDadosTrain(index_inicio, qtd, train_X): # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
        "  base = 330\n",
        "  import matplotlib.pyplot as plt\n",
        "  for i in range(index_inicio, index_inicio + qtd):\n",
        "      plt.subplot(base + 1 + (i - index_inicio))\n",
        "      plt.imshow(train_X[i], cmap=plt.get_cmap('gray'))\n",
        "  plt.show()\n",
        "  print(train_y[index_inicio:index_inicio + qtd])\n",
        "\n",
        "def entHistogram(testar, num):\n",
        "  import matplotlib.pyplot as plt\n",
        "  if num == 1:\n",
        "    title = 'Class histogram train_y'\n",
        "  elif num == 2:\n",
        "    title = 'Class histogram Val_y'\n",
        "  elif num == 3:\n",
        "    title = 'Class histogram test_y'\n",
        "  plt.hist(testar, bins=10, edgecolor='black', alpha=0.7)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "\n",
        "def tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF = None): # x = time.time() || inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
        "  elapsed_time = [[\"Carregar DataSet\", 0], [\"Filtro Entropia\", 0], [\"Pre-processamento\", 0], [\"Criação do Modelo\", 0], [\"Treinamento\", 0], [\"Inicio à Fim Execução\" , 0]]\n",
        "  elapsed_time[0][1] = importacaoF - inicio  # tempo de inicio de execução até o final da importação do dataset\n",
        "  if (entropiaF != None): elapsed_time[1][1] = entropiaF - importacaoF  # Tempo gasto Inicio é importaçãoF - Execução da Entropia\n",
        "  elapsed_time[2][1] = padrozinacaoF - importacaoF  # pega o tempo da entropia e subtrai do tempo apos a padronização para verificar quanto tempo padronizacao demorou\n",
        "  elapsed_time[3][1] = criacaoModeloF - padrozinacaoF  # tempo da padronizacao - tempo apos a criacao do modelo para verificar tempo decorido\n",
        "  elapsed_time[4][1] = treinamentoF - criacaoModeloF  # tempo inicial é marcado pela criacaoModeloF - tempoTreinamento que marca o momento que treinamento terminou\n",
        "  elapsed_time[5][1] = treinamentoF - inicio #tempo total de execução inicio| importações até final da Execução | treinamento\n",
        "  for index, tempo in enumerate(elapsed_time):\n",
        "    if entropiaF is None and index == 1:\n",
        "      continue\n",
        "    print(\"{}: {:.4f}\".format(tempo[0], round(tempo[1], 2)) + str(\" em milissegundos\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkzM7M9RugW0"
      },
      "outputs": [],
      "source": [
        "def criacaoModeloF1(): # Criação do Modelo\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3,3), activation = 'relu', kernel_initializer = 'he_uniform', input_shape = (28, 28, 1)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(10, activation = 'softmax'))  # classificação 10 categorias\n",
        "\n",
        "  opt = SGD(learning_rate=0.01, momentum =0.9) #copilaçãoModelo\n",
        "  model.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtJvfWG7D7N-"
      },
      "source": [
        "## [MNIST] Full Dataset Training:\n",
        "  * Without entropy selection\n",
        "  * Full Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veguqbGILUYC"
      },
      "outputs": [],
      "source": [
        "inicio = time.time() #tempo de inicio\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyzRTrg4dHof"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inicio = time.time() #tempo de inicio\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "importacaoF = time.time() #tempo de importacao\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 28, 1) #padronização e categorização\n",
        "padrozinacaoF = time.time() #tempo de padronizacao\n",
        "model = criacaoModeloF1() # instanciando o modelo\n",
        "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
        "history = model.fit(train_X, train_y, epochs = 10, batch_size = 32, validation_data=(Val_X, Val_y), verbose = 1) #treinamento\n",
        "treinamentoF = time.time() #tempo final\n",
        "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGDUPLMzNFb5"
      },
      "source": [
        "## [MNIST] Random and Stratified Training:\n",
        "  * Without entropy selection\n",
        "  * Stratified dataset divided proportionally in half\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUmKNCWuLUYH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "partial_train_X, _, partial_train_y, _ = train_test_split(train_X, train_y, test_size=0.5, random_state=42, stratify=train_y)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(partial_train_X, partial_train_y, test_size=(0.2), random_state=42, stratify=partial_train_y)\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17ZH_Wz_LUYI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inicio = time.time() #tempo de inicio\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "importacaoF = time.time() #tempo de importacao\n",
        "\n",
        "partial_train_X, _, partial_train_y, _ = train_test_split(train_X, train_y, test_size=0.5, random_state=42, stratify=train_y)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(partial_train_X, partial_train_y, test_size=(0.2), random_state=42, stratify=partial_train_y)\n",
        "\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 28, 1) #padronização e categorização\n",
        "padrozinacaoF = time.time() #tempo de padronizacao\n",
        "model = criacaoModeloF1() # instanciando o modelo\n",
        "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
        "history = model.fit(train_X, train_y, epochs = 10, batch_size = 32, validation_data=(Val_X, Val_y), verbose = 1) #treinamento\n",
        "treinamentoF = time.time() #tempo final\n",
        "\n",
        "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgYBuah9-0Kt"
      },
      "source": [
        "## [MNIST] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in training data causing \"noise\" in validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e66k2yN9PkmG"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orD6RONz-xh9"
      },
      "outputs": [],
      "source": [
        "inicio = time.time() #tempo de inicio\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "importacaoF = time.time() #tempo de importacao\n",
        "\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "\n",
        "entropiaF = time.time() #tempo de filtragem entropia\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 28, 1) #padronização e categorização\n",
        "padrozinacaoF = time.time() #tempo de padronizacao\n",
        "model = criacaoModeloF1() # instanciando o modelo\n",
        "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
        "history = model.fit(train_X, train_y, epochs = 10, batch_size = 32, validation_data=(Val_X, Val_y), verbose = 1) #treinamento\n",
        "treinamentoF = time.time() #tempo final\n",
        "\n",
        "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSelnrdf-qu6"
      },
      "source": [
        "## [MNIST] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in both training and validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLXPrIQwWXNI"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJx2l3sJLUYJ"
      },
      "outputs": [],
      "source": [
        "inicio = time.time() #tempo de inicio\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "importacaoF = time.time() #tempo de importacao\n",
        "\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "\n",
        "entropiaF = time.time() #tempo de filtragem entropia\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 28, 1) #padronização e categorização\n",
        "padrozinacaoF = time.time() #tempo de padronizacao\n",
        "model = criacaoModeloF1() # instanciando o modelo\n",
        "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
        "history = model.fit(train_X, train_y, epochs = 10, batch_size = 32, validation_split=0.2, verbose = 1) #treinamento\n",
        "treinamentoF = time.time() #tempo final\n",
        "\n",
        "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeO9EcbqLUYK"
      },
      "source": [
        "## [MNIST] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in both training and validation data.\n",
        "  * Cross-validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X3I80sQuLUYL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def crossValidation(train_X,train_y,n_folds = 5):\n",
        "    #divide seus dados em k folds, e treina no k−1 e valida em 1 fold, então a proporção de dados usados para validação é 1/k.\n",
        "    #2 folds (50% de validação): Treina em 50% dos dados e valida nos outros 50%.\n",
        "    #5 folds (20% de validação): Treina em 80% dos dados e valida em 20%.\n",
        "    #10 folds (10% de validação): Treina em 90% dos dados e valida em 10%.\n",
        "    n_fold = 5 if n_folds == 0 else n_folds\n",
        "\n",
        "    kf = KFold(n_splits = n_folds, shuffle = True, random_state = 42)  #divisão do conjunto de dados\n",
        "    fold_no = 1\n",
        "    for train_index, val_index in kf.split(train_X):\n",
        "        train_data, val_data = train_X[train_index], train_X[val_index]\n",
        "        train_label, val_label = train_y[train_index], train_y[val_index]\n",
        "\n",
        "        print(f\"Treinando a rede no folder numero: {fold_no}\")\n",
        "        model = criacaoModeloF1() # instanciando o modelo\n",
        "        inicio = time.time() #tempo de inicio\n",
        "        history = model.fit(train_data, train_label, epochs = 10, batch_size = 32, verbose = 1, validation_data = (val_data, val_label))\n",
        "        treinamentoF = time.time() #tempo final\n",
        "        tempoTreino = round(treinamentoF - inicio,2)\n",
        "        scores = model.evaluate(val_data, val_label, verbose = 1)\n",
        "        print(f\"Num do Folder: {fold_no}: Taxa de Loss: {scores[0]}, Taxa de Acuracia: {scores[1]*100:.2f}%, Tamanho do Conjunto de Dados: {len(train_data)}, Tempo de Treino: {tempoTreino}\")\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.title(f'Model Accuracy: {fold_no}')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title(f'Model Loss: {fold_no}')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Para o conjunto de TREINAMENTO\n",
        "        print(\"Dados de Treino\")\n",
        "        predictions_train = model.predict(train_data) # Previsões\n",
        "        classePredita_train = np.argmax(predictions_train, axis=1)\n",
        "        classeVerdadeira_train = np.argmax(train_label, axis=1)\n",
        "        cm_train = confusion_matrix(classeVerdadeira_train, classePredita_train)\n",
        "        disp = ConfusionMatrixDisplay(cm_train)\n",
        "        disp.plot()\n",
        "        plt.title('Confusion Matrix - Training')\n",
        "        plt.show()\n",
        "\n",
        "        acc_train = accuracy_score(classeVerdadeira_train, classePredita_train) * 100  # Acurácia\n",
        "        print(\"Accuracy (Treinamento): \", '%.3f' % (acc_train))\n",
        "        sens_train = recall_score(classeVerdadeira_train, classePredita_train, average='macro')  # Sensibilidade\n",
        "        print(\"Recall (Treinamento): \", '%.3f' % (sens_train))\n",
        "        loss_train = history.history['loss'][-1]\n",
        "        print(\"Loss (Treinamento): \", '%.3f' % loss_train)\n",
        "\n",
        "        print(\"--------------------------------------------------\")\n",
        "        print(\"Dados de Validação\")\n",
        "        # Para o conjunto de VALIDAÇÃO\n",
        "        predictions_val = model.predict(val_data) # Previsões\n",
        "        classePredita_val = np.argmax(predictions_val, axis=1)\n",
        "        classeVerdadeira_val = np.argmax(val_label, axis=1)\n",
        "        cm_val = confusion_matrix(classeVerdadeira_val, classePredita_val)\n",
        "        disp = ConfusionMatrixDisplay(cm_val)\n",
        "        disp.plot()\n",
        "        plt.title('Confusion Matrix - Validation')\n",
        "        plt.show()\n",
        "\n",
        "        acc_val = accuracy_score(classeVerdadeira_val, classePredita_val) * 100  # Acurácia\n",
        "        print(\"Accuracy (Validação): \", '%.3f' % (acc_val))\n",
        "        sens_val = recall_score(classeVerdadeira_val, classePredita_val, average='macro')  # Sensibilidade\n",
        "        print(\"Recall (Validação): \", '%.3f' % (sens_val))\n",
        "        loss_val = history.history['val_loss'][-1]\n",
        "        print(\"Loss (Validação): \", '%.3f' % loss_val)\n",
        "\n",
        "        fold_no += 1\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "#carregar os dados\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, train_y, test_X, test_y = filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y, 10) #novo filtro da reunião 03-10-2023\n",
        "train_X, train_y, test_X, test_y = carregarPepararValidacaoCruzada(train_X, train_y, test_X, test_y, 28, 1) #padronização e categorização\n",
        "train_X = np.concatenate([train_X, test_X], axis=0)\n",
        "train_y = np.concatenate([train_y, test_y], axis=0)\n",
        "\n",
        "#criar/instanciar o modelo para validação cruzada\n",
        "crossValidation(train_X,train_y, 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V97k9nXXLUYM"
      },
      "source": [
        "# FASHION MNIST\n",
        "* An alternative to MNIST, containing 70,000 grayscale images of 28x28 pixels of 10 categories of clothing items, such as t-shirts, pants, and shoes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toLS6IrmhYeL"
      },
      "source": [
        "## Creation and definition of all functions\n",
        "\n",
        "1. Entropy Filter\n",
        "2. Loading, Preprocessing, and Handling\n",
        "3. Model Creation\n",
        "4. Statistical Predictions\n",
        "5. Accuracy and Loss Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88UovG5Cu3H9"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def entropia(pk, base=10, max=1):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    return result\n",
        "\n",
        "def filtrar_entropia_classe(train_X, train_y, totalClasses):  # filtro de entropia\n",
        "    train_Xextend = []\n",
        "    train_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(train_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = train_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        train_Xextend.extend([train_X[i] for i in indices_filtrados_da_classe])\n",
        "        train_yextend.extend([train_y[i] for i in indices_filtrados_da_classe])\n",
        "    return np.array(train_Xextend), np.array(train_yextend)\n",
        "\n",
        "def filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y, totalClasses):  # filtro de entropia\n",
        "    train_Xextend = []\n",
        "    train_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(train_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = train_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        train_Xextend.extend([train_X[i] for i in indices_filtrados_da_classe])\n",
        "        train_yextend.extend([train_y[i] for i in indices_filtrados_da_classe])\n",
        "    test_Xextend = []\n",
        "    test_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(test_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = test_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        test_Xextend.extend([test_X[i] for i in indices_filtrados_da_classe])\n",
        "        test_yextend.extend([test_y[i] for i in indices_filtrados_da_classe])\n",
        "    return np.array(train_Xextend), np.array(train_yextend), np.array(test_Xextend), np.array(test_yextend)\n",
        "\n",
        "def carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, d, c): # Carregamento e Tratamento\n",
        "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  Val_X = Val_X.reshape((Val_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], d, d, c))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  Val_X = Val_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_y = to_categorical(train_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  Val_y = to_categorical(Val_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_y = to_categorical(test_y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "  return train_X, train_y, Val_X, Val_y, test_X, test_y\n",
        "\n",
        "def carregarPepararValidacaoCruzada(train_X, train_y, test_X, test_y, d, c): # Carregamento e Tratamento\n",
        "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], d, d, c))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_y = to_categorical(train_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_y = to_categorical(test_y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "  return train_X, train_y, test_X, test_y\n",
        "\n",
        "def avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y):\n",
        "    # Para o conjunto de TREINAMENTO\n",
        "    print(\"Dados de Treino\")\n",
        "    predictions_train = model.predict(train_X) # Previsões\n",
        "    classePredita_train = np.argmax(predictions_train, axis=1)\n",
        "    classeVerdadeira_train = np.argmax(train_y, axis=1)\n",
        "    cm_train = confusion_matrix(classeVerdadeira_train, classePredita_train)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(cm_train)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Training')\n",
        "    plt.show()\n",
        "\n",
        "    acc_train = accuracy_score(classeVerdadeira_train, classePredita_train) * 100  # Acurácia\n",
        "    print(\"Accuracy (Treinamento): \", '%.3f' % (acc_train))\n",
        "    sens_train = recall_score(classeVerdadeira_train, classePredita_train, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Treinamento): \", '%.3f' % (sens_train))\n",
        "    loss_train = history.history['loss'][-1]\n",
        "    print(\"Loss (Treinamento): \", '%.3f' % loss_train)\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Dados de Validação\")\n",
        "    # Para o conjunto de VALIDAÇÃO\n",
        "    predictions_val = model.predict(Val_X) # Previsões\n",
        "    classePredita_val = np.argmax(predictions_val, axis=1)\n",
        "    classeVerdadeira_val = np.argmax(Val_y, axis=1)\n",
        "    cm_val = confusion_matrix(classeVerdadeira_val, classePredita_val)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(cm_val)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Validation')\n",
        "    plt.show()\n",
        "\n",
        "    acc_val = accuracy_score(classeVerdadeira_val, classePredita_val) * 100  # Acurácia\n",
        "    print(\"Accuracy (Validação): \", '%.3f' % (acc_val))\n",
        "    sens_val = recall_score(classeVerdadeira_val, classePredita_val, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Validação): \", '%.3f' % (sens_val))\n",
        "    loss_val = history.history['val_loss'][-1]\n",
        "    print(\"Loss (Validação): \", '%.3f' % loss_val)\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Dados de Teste\")\n",
        "    # Para o conjunto de TESTE\n",
        "    predictions_test = model.predict(test_X) # Previsões\n",
        "    classePredita_test = np.argmax(predictions_test, axis=1)\n",
        "    classeVerdadeira_test = np.argmax(test_y, axis=1)\n",
        "    cm_test = confusion_matrix(classeVerdadeira_test, classePredita_test)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(cm_test)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Test')\n",
        "    plt.show()\n",
        "\n",
        "    acc_test = accuracy_score(classeVerdadeira_test, classePredita_test) * 100  # Acurácia\n",
        "    print(\"Accuracy (Teste): \", '%.3f' % (acc_test))\n",
        "    sens_test = recall_score(classeVerdadeira_test, classePredita_test, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Teste): \", '%.3f' % (sens_test))\n",
        "    loss_test = history.history['val_loss'][-1]\n",
        "    print(\"Loss (Teste): \", '%.3f' % loss_test)\n",
        "\n",
        "def plotAcuraciaLoss(history): #plots\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  plt.tight_layout()\n",
        "\n",
        "def plotarDadosTrain(index_inicio, qtd, train_X): # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
        "  base = 330\n",
        "  import matplotlib.pyplot as plt\n",
        "  for i in range(index_inicio, index_inicio + qtd):\n",
        "      plt.subplot(base + 1 + (i - index_inicio))\n",
        "      plt.imshow(train_X[i], cmap=plt.get_cmap('gray'))\n",
        "  plt.show()\n",
        "  print(train_y[index_inicio:index_inicio + qtd])\n",
        "\n",
        "def entHistogram(testar, num):\n",
        "  import matplotlib.pyplot as plt\n",
        "  if num == 1:\n",
        "    title = 'Class histogram train_y'\n",
        "  elif num == 2:\n",
        "    title = 'Class histogram Val_y'\n",
        "  elif num == 3:\n",
        "    title = 'Class histogram test_y'\n",
        "  plt.hist(testar, bins=10, edgecolor='black', alpha=0.7)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "\n",
        "def tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF = None): # x = time.time() || inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
        "  elapsed_time = [[\"Carregar DataSet\", 0], [\"Filtro Entropia\", 0], [\"Pre-processamento\", 0], [\"Criação do Modelo\", 0], [\"Treinamento\", 0], [\"Inicio à Fim Execução\" , 0]]\n",
        "  elapsed_time[0][1] = importacaoF - inicio  # tempo de inicio de execução até o final da importação do dataset\n",
        "  if (entropiaF != None): elapsed_time[1][1] = entropiaF - importacaoF  # Tempo gasto Inicio é importaçãoF - Execução da Entropia\n",
        "  elapsed_time[2][1] = padrozinacaoF - importacaoF  # pega o tempo da entropia e subtrai do tempo apos a padronização para verificar quanto tempo padronizacao demorou\n",
        "  elapsed_time[3][1] = criacaoModeloF - padrozinacaoF  # tempo da padronizacao - tempo apos a criacao do modelo para verificar tempo decorido\n",
        "  elapsed_time[4][1] = treinamentoF - criacaoModeloF  # tempo inicial é marcado pela criacaoModeloF - tempoTreinamento que marca o momento que treinamento terminou\n",
        "  elapsed_time[5][1] = treinamentoF - inicio #tempo total de execução inicio| importações até final da Execução | treinamento\n",
        "  for index, tempo in enumerate(elapsed_time):\n",
        "    if entropiaF is None and index == 1:\n",
        "      continue\n",
        "    print(\"{}: {:.4f}\".format(tempo[0], round(tempo[1], 2)) + str(\" em milissegundos\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_KqRk6Qhcx4"
      },
      "outputs": [],
      "source": [
        "from numpy import mean, std\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def criacaoModeloFashion(): # Criação do Modelo\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(28, 28, 1)))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "  model.add(Dropout(rate=0.45))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dropout(rate=0.25))\n",
        "  model.add(Dense(units=10, activation='softmax'))\n",
        "  opt = SGD(learning_rate=0.01, momentum =0.9)\n",
        "  model.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIimbHTqLUYM"
      },
      "source": [
        "## [FASHION MNIST] Full Dataset Training:\n",
        "  * Without entropy selection\n",
        "  * Full Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9rmXd4nLUYM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "(train_X, train_y), (test_X, test_y) = tf.keras.datasets.fashion_mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76anR-KPLUYN"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 28, 1) #padronização e categorização\n",
        "\n",
        "# Criaçaõ do Modelo\n",
        "model = criacaoModeloFashion() # instanciando o modelo\n",
        "\n",
        "# Treinamento\n",
        "tempoIni = time.time() #tempo de filtragem entropia\n",
        "history = model.fit(train_X, train_y, epochs = 10, batch_size = 128, validation_data=(Val_X, Val_y), verbose = 1)\n",
        "tempoFim = time.time() #tempo de filtragem entropia\n",
        "tempoTotal = round(tempoFim - tempoIni,2)\n",
        "\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "print(\"tempoTotal de treinamento: \", tempoTotal)\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCOPUjLqLUYN"
      },
      "source": [
        "## [Fashion MNIST] Random and Stratified Training:\n",
        "  * Without entropy selection\n",
        "  * Stratified dataset divided proportionally in half\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfPzlXAVLUYO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "(train_X, train_y), (test_X, test_y) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "partial_train_X, _, partial_train_y, _ = train_test_split(train_X, train_y, test_size=0.5, random_state=42, stratify=train_y)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(partial_train_X, partial_train_y, test_size=(0.2), random_state=42, stratify=partial_train_y)\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCYwIjKTLUYO"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "partial_train_X, _, partial_train_y, _ = train_test_split(train_X, train_y, test_size=0.5, random_state=42, stratify=train_y)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(partial_train_X, partial_train_y, test_size=(0.2), random_state=42, stratify=partial_train_y)\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 28, 1) #padronização e categorização\n",
        "\n",
        "# Criaçaõ do Modelo\n",
        "model = criacaoModeloFashion() # instanciando o modelo\n",
        "\n",
        "# Treinamento\n",
        "tempoIni = time.time() #tempo de filtragem entropia\n",
        "history = model.fit(train_X, train_y, epochs = 10, batch_size = 128, validation_data=(Val_X, Val_y), verbose = 1)\n",
        "tempoFim = time.time() #tempo de filtragem entropia\n",
        "tempoTotal = round(tempoFim - tempoIni,2)\n",
        "\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "print(\"tempoTotal de treinamento: \", tempoTotal)\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVNGRs9sLUYO"
      },
      "source": [
        "## [Fashion MNIST] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in training data causing \"noise\" in validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6D02yrFLUYP"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) =  tf.keras.datasets.fashion_mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE4IB4SzLUYP"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "tempoIniCalEntropia = time.time() #tempo de filtragem entropia\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "tempoFimCalEntropia = time.time() #tempo de filtragem entropia\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 28, 1) #padronização e categorização\n",
        "\n",
        "# Criaçaõ do Modelo\n",
        "model = criacaoModeloFashion() # instanciando o modelo\n",
        "\n",
        "# Treinamento\n",
        "tempoIni = time.time() #tempo de filtragem entropia\n",
        "history = model.fit(train_X, train_y, epochs = 10, batch_size = 128, validation_data=(Val_X, Val_y), verbose = 1)\n",
        "tempoFim = time.time() #tempo de filtragem entropia\n",
        "tempoTotal = round(tempoFim - tempoIni,2)\n",
        "tempoTotalCalEntropia = round(tempoFimCalEntropia - tempoIniCalEntropia,2)\n",
        "\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "print(\"tempoTotal de treinamento: \", tempoTotal)\n",
        "print(\"tempo Total CalEntropia: \", tempoTotalCalEntropia)\n",
        "\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dHAD9uqLUYP"
      },
      "source": [
        "## [FASHION MNIST] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in both training and validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff8vEK9SLUYQ"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) =  tf.keras.datasets.fashion_mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzM1c9H6LUYa"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "tempoIniCalEntropia = time.time() #tempo de filtragem entropia\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "tempoFimCalEntropia = time.time() #tempo de filtragem entropia\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 28, 1) #padronização e categorização\n",
        "\n",
        "# Criaçaõ do Modelo\n",
        "model = criacaoModeloFashion() # instanciando o modelo\n",
        "\n",
        "# Treinamento\n",
        "tempoIni = time.time() #tempo de filtragem entropia\n",
        "history = model.fit(train_X, train_y, epochs = 10, batch_size = 128, validation_data=(Val_X, Val_y), verbose = 1)\n",
        "tempoFim = time.time() #tempo de filtragem entropia\n",
        "tempoTotal = round(tempoFim - tempoIni,2)\n",
        "tempoTotalCalEntropia = round(tempoFimCalEntropia - tempoIniCalEntropia,2)\n",
        "\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "print(\"tempoTotal de treinamento: \", tempoTotal)\n",
        "print(\"tempo Total CalEntropia: \", tempoTotalCalEntropia)\n",
        "\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwUF-0JgLUYb"
      },
      "source": [
        "## [Fashion MNIST] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in both training and validation data.\n",
        "  * Cross-validation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from numpy import mean, std\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def criacaoModeloFM1():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(28, 28, 1)))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "  model.add(Dropout(rate=0.45))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dropout(rate=0.25))\n",
        "  model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "  opt = SGD(learning_rate=0.01, momentum =0.9)\n",
        "  model.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "  return model\n",
        "\n",
        "def crossValidation(combinado_treino_X, combinado_validacao_y,n_folds = 5):\n",
        "    #divide seus dados em k folds, e treina no k−1 e valida em 1 fold, então a proporção de dados usados para validação é 1/k.\n",
        "    #2 folds (50% de validação): Treina em 50% dos dados e valida nos outros 50%.\n",
        "    #5 folds (20% de validação): Treina em 80% dos dados e valida em 20%.\n",
        "    #10 folds (10% de validação): Treina em 90% dos dados e valida em 10%.\n",
        "    n_fold = 5 if n_folds == 0 else n_folds\n",
        "    datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "    datagen.fit(combinado_treino_X)\n",
        "\n",
        "    kf = KFold(n_splits = n_folds, shuffle = True, random_state = 42)  #divisão do conjunto de dados\n",
        "    fold_no = 1\n",
        "    for train_index, val_index in kf.split(train_X):\n",
        "        train_data, val_data = train_X[train_index], train_X[val_index]\n",
        "        train_label, val_label = train_y[train_index], train_y[val_index]\n",
        "\n",
        "        print(f\"Treinando a rede no folder numero: {fold_no}\")\n",
        "        model = criacaoModeloFM1() # instanciando o modelo\n",
        "        inicio = time.time() #tempo de inicio\n",
        "        history = model.fit(train_data, train_label, epochs = 10, batch_size = 128, verbose = 1, validation_data = (val_data, val_label))\n",
        "        treinamentoF = time.time() #tempo final\n",
        "        tempoTreino = round(treinamentoF - inicio,2)\n",
        "        scores = model.evaluate(val_data, val_label, verbose = 1)\n",
        "        print(f\"Num do Folder: {fold_no}: Taxa de Loss: {scores[0]}, Taxa de Acuracia: {scores[1]*100:.2f}%, Tamanho do Conjunto de Dados: {len(train_data)}, Tempo de Treino: {tempoTreino}\")\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy']) # Métrica de validação\n",
        "        plt.title(f'Model Accuracy: {fold_no}')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss']) # Métrica de validação\n",
        "        plt.title(f'Model Loss: {fold_no}')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Para o conjunto de TREINAMENTO\n",
        "        print(\"Dados de Treino\")\n",
        "        predictions_train = model.predict(train_data) # Previsões\n",
        "        classePredita_train = np.argmax(predictions_train, axis=1)\n",
        "        classeVerdadeira_train = np.argmax(train_label, axis=1)\n",
        "        cm_train = confusion_matrix(classeVerdadeira_train, classePredita_train)\n",
        "        disp = ConfusionMatrixDisplay(cm_train)\n",
        "        disp.plot()\n",
        "        plt.title('Confusion Matrix - Training')\n",
        "        plt.show()\n",
        "\n",
        "        acc_train = accuracy_score(classeVerdadeira_train, classePredita_train) * 100  # Acurácia\n",
        "        print(\"Accuracy (Treinamento): \", '%.3f' % (acc_train))\n",
        "        sens_train = recall_score(classeVerdadeira_train, classePredita_train, average='macro')  # Sensibilidade\n",
        "        print(\"Recall (Treinamento): \", '%.3f' % (sens_train))\n",
        "        loss_train = history.history['loss'][-1]\n",
        "        print(\"Loss (Treinamento): \", '%.3f' % loss_train)\n",
        "\n",
        "        print(\"--------------------------------------------------\")\n",
        "        print(\"Dados de Validação\")\n",
        "        # Para o conjunto de VALIDAÇÃO\n",
        "        predictions_val = model.predict(val_data) # Previsões\n",
        "        classePredita_val = np.argmax(predictions_val, axis=1)\n",
        "        classeVerdadeira_val = np.argmax(val_label, axis=1)\n",
        "        cm_val = confusion_matrix(classeVerdadeira_val, classePredita_val)\n",
        "        disp = ConfusionMatrixDisplay(cm_val)\n",
        "        disp.plot()\n",
        "        plt.title('Confusion Matrix - Validation')\n",
        "        plt.show()\n",
        "\n",
        "        acc_val = accuracy_score(classeVerdadeira_val, classePredita_val) * 100  # Acurácia\n",
        "        print(\"Accuracy (Validação): \", '%.3f' % (acc_val))\n",
        "        sens_val = recall_score(classeVerdadeira_val, classePredita_val, average='macro')  # Sensibilidade\n",
        "        print(\"Recall (Validação): \", '%.3f' % (sens_val))\n",
        "        loss_val = history.history['val_loss'][-1]\n",
        "        print(\"Loss (Validação): \", '%.3f' % loss_val)\n",
        "\n",
        "        fold_no += 1\n",
        "        print(\"\\n\")\n",
        "\n",
        "#carregar os dados\n",
        "(train_X, train_y), (test_X, test_y) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "tempoEntropiaIni = time.time()\n",
        "train_X, train_y, test_X, test_y = filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y,10) #novo filtro da reunião 03-10-2023\n",
        "tempoEntropiaFim = time.time()\n",
        "train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))\n",
        "test_X = test_X.reshape((test_X.shape[0], 28, 28, 1))\n",
        "train_X = train_X.astype(\"float\")/255.0\n",
        "test_X = test_X.astype(\"float\")/255.0\n",
        "train_y = to_categorical(train_y, 10)\n",
        "test_y = to_categorical(test_y, 10)\n",
        "\n",
        "train_X = np.concatenate([train_X, test_X], axis=0)\n",
        "train_y = np.concatenate([train_y, test_y], axis=0)\n",
        "\n",
        "#criar/instanciar o modelo para validação cruzada\n",
        "crossValidation(train_X,train_y, 5)\n"
      ],
      "metadata": {
        "id": "nKrll3v34trQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGBSsNe4CxBg"
      },
      "source": [
        "# CIFAR-10\n",
        "* Dataset of 32x32 pixel color images, containing 60,000 images distributed into 10 classes, such as animals and vehicles, with 6,000 images per class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96LR7YwNCxBp"
      },
      "source": [
        "## Creation and definition of all functions\n",
        "\n",
        "1. Entropy Filter\n",
        "2. Loading, Preprocessing, and Handling\n",
        "3. Model Creation\n",
        "4. Statistical Predictions\n",
        "5. Accuracy and Loss Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WoxISNsLUYd"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def entropia(pk, base=10, max=1):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    return result\n",
        "\n",
        "def filtrar_entropia_classe(train_X, train_y, totalClasses):  # filtro de entropia\n",
        "    train_Xextend = []\n",
        "    train_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(train_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = train_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        train_Xextend.extend([train_X[i] for i in indices_filtrados_da_classe])\n",
        "        train_yextend.extend([train_y[i] for i in indices_filtrados_da_classe])\n",
        "    return np.array(train_Xextend), np.array(train_yextend)\n",
        "\n",
        "def filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y, totalClasses):  # filtro de entropia\n",
        "    train_Xextend = []\n",
        "    train_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(train_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = train_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        train_Xextend.extend([train_X[i] for i in indices_filtrados_da_classe])\n",
        "        train_yextend.extend([train_y[i] for i in indices_filtrados_da_classe])\n",
        "    test_Xextend = []\n",
        "    test_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(test_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = test_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        test_Xextend.extend([test_X[i] for i in indices_filtrados_da_classe])\n",
        "        test_yextend.extend([test_y[i] for i in indices_filtrados_da_classe])\n",
        "    return np.array(train_Xextend), np.array(train_yextend), np.array(test_Xextend), np.array(test_yextend)\n",
        "\n",
        "def carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, d, c): # Carregamento e Tratamento\n",
        "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  Val_X = Val_X.reshape((Val_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], d, d, c))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  Val_X = Val_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_y = to_categorical(train_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  Val_y = to_categorical(Val_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_y = to_categorical(test_y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "  return train_X, train_y, Val_X, Val_y, test_X, test_y\n",
        "\n",
        "def carregarPepararValidacaoCruzada(train_X, train_y, test_X, test_y, d, c): # Carregamento e Tratamento\n",
        "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], d, d, c))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_y = to_categorical(train_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_y = to_categorical(test_y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "  return train_X, train_y, test_X, test_y\n",
        "\n",
        "def avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y):\n",
        "    # Para o conjunto de TREINAMENTO\n",
        "    print(\"Dados de Treino\")\n",
        "    predictions_train = model.predict(train_X) # Previsões\n",
        "    classePredita_train = np.argmax(predictions_train, axis=1)\n",
        "    classeVerdadeira_train = np.argmax(train_y, axis=1)\n",
        "    cm_train = confusion_matrix(classeVerdadeira_train, classePredita_train)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(cm_train)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Training')\n",
        "    plt.show()\n",
        "\n",
        "    acc_train = accuracy_score(classeVerdadeira_train, classePredita_train) * 100  # Acurácia\n",
        "    print(\"Accuracy (Treinamento): \", '%.3f' % (acc_train))\n",
        "    sens_train = recall_score(classeVerdadeira_train, classePredita_train, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Treinamento): \", '%.3f' % (sens_train))\n",
        "    loss_train = history.history['loss'][-1]\n",
        "    print(\"Loss (Treinamento): \", '%.3f' % loss_train)\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Dados de Validação\")\n",
        "    # Para o conjunto de VALIDAÇÃO\n",
        "    predictions_val = model.predict(Val_X) # Previsões\n",
        "    classePredita_val = np.argmax(predictions_val, axis=1)\n",
        "    classeVerdadeira_val = np.argmax(Val_y, axis=1)\n",
        "    cm_val = confusion_matrix(classeVerdadeira_val, classePredita_val)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(cm_val)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Validation')\n",
        "    plt.show()\n",
        "\n",
        "    acc_val = accuracy_score(classeVerdadeira_val, classePredita_val) * 100  # Acurácia\n",
        "    print(\"Accuracy (Validação): \", '%.3f' % (acc_val))\n",
        "    sens_val = recall_score(classeVerdadeira_val, classePredita_val, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Validação): \", '%.3f' % (sens_val))\n",
        "    loss_val = history.history['val_loss'][-1]\n",
        "    print(\"Loss (Validação): \", '%.3f' % loss_val)\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Dados de Teste\")\n",
        "    # Para o conjunto de TESTE\n",
        "    predictions_test = model.predict(test_X) # Previsões\n",
        "    classePredita_test = np.argmax(predictions_test, axis=1)\n",
        "    classeVerdadeira_test = np.argmax(test_y, axis=1)\n",
        "    cm_test = confusion_matrix(classeVerdadeira_test, classePredita_test)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(cm_test)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Test')\n",
        "    plt.show()\n",
        "\n",
        "    acc_test = accuracy_score(classeVerdadeira_test, classePredita_test) * 100  # Acurácia\n",
        "    print(\"Accuracy (Teste): \", '%.3f' % (acc_test))\n",
        "    sens_test = recall_score(classeVerdadeira_test, classePredita_test, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Teste): \", '%.3f' % (sens_test))\n",
        "    loss_test = history.history['val_loss'][-1]\n",
        "    print(\"Loss (Teste): \", '%.3f' % loss_test)\n",
        "\n",
        "def plotAcuraciaLoss(history): #plots\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  plt.tight_layout()\n",
        "\n",
        "def plotarDadosTrain(index_inicio, qtd, train_X): # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
        "  base = 330\n",
        "  import matplotlib.pyplot as plt\n",
        "  for i in range(index_inicio, index_inicio + qtd):\n",
        "      plt.subplot(base + 1 + (i - index_inicio))\n",
        "      plt.imshow(train_X[i], cmap=plt.get_cmap('gray'))\n",
        "  plt.show()\n",
        "  print(train_y[index_inicio:index_inicio + qtd])\n",
        "\n",
        "def entHistogram(testar, num):\n",
        "  import matplotlib.pyplot as plt\n",
        "  if num == 1:\n",
        "    title = 'Class histogram train_y'\n",
        "  elif num == 2:\n",
        "    title = 'Class histogram Val_y'\n",
        "  elif num == 3:\n",
        "    title = 'Class histogram test_y'\n",
        "  plt.hist(testar, bins=10, edgecolor='black', alpha=0.7)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "\n",
        "def tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF = None): # x = time.time() || inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
        "  elapsed_time = [[\"Carregar DataSet\", 0], [\"Filtro Entropia\", 0], [\"Pre-processamento\", 0], [\"Criação do Modelo\", 0], [\"Treinamento\", 0], [\"Inicio à Fim Execução\" , 0]]\n",
        "  elapsed_time[0][1] = importacaoF - inicio  # tempo de inicio de execução até o final da importação do dataset\n",
        "  if (entropiaF != None): elapsed_time[1][1] = entropiaF - importacaoF  # Tempo gasto Inicio é importaçãoF - Execução da Entropia\n",
        "  elapsed_time[2][1] = padrozinacaoF - importacaoF  # pega o tempo da entropia e subtrai do tempo apos a padronização para verificar quanto tempo padronizacao demorou\n",
        "  elapsed_time[3][1] = criacaoModeloF - padrozinacaoF  # tempo da padronizacao - tempo apos a criacao do modelo para verificar tempo decorido\n",
        "  elapsed_time[4][1] = treinamentoF - criacaoModeloF  # tempo inicial é marcado pela criacaoModeloF - tempoTreinamento que marca o momento que treinamento terminou\n",
        "  elapsed_time[5][1] = treinamentoF - inicio #tempo total de execução inicio| importações até final da Execução | treinamento\n",
        "  for index, tempo in enumerate(elapsed_time):\n",
        "    if entropiaF is None and index == 1:\n",
        "      continue\n",
        "    print(\"{}: {:.4f}\".format(tempo[0], round(tempo[1], 2)) + str(\" em milissegundos\"))\n",
        "\n",
        "def criacaoModeloG9():\n",
        "    opt=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='elu', input_shape=(32, 32, 3), padding='same'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Conv2D(32, (3, 3), activation='elu', padding='same'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='elu', padding='same'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Conv2D(128, (3, 3), activation='elu', padding='same'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='elu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-px8VA8DFIx"
      },
      "source": [
        "## [CIFAR-10] Full Dataset Training:\n",
        "  * Without entropy selection\n",
        "  * Full Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYPzYc2-LUYe"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxHtQXu2LUYe"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 32, 3) #padronização e categorização\n",
        "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "datagen.fit(train_X)\n",
        "\n",
        "model = criacaoModeloG9()\n",
        "inicio = time.time() #tempo de inicio\n",
        "history=model.fit(datagen.flow(train_X, train_y, batch_size=128),\n",
        "                    steps_per_epoch = len(train_X) / 128, epochs=50, validation_data=(Val_X, Val_y))\n",
        "treinamentoF = time.time() #tempo final\n",
        "tempoTreino = round(treinamentoF - inicio,2)\n",
        "print(\"Tempo de treinamento: \", tempoTreino)\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bvdvgxeDFsC"
      },
      "source": [
        "## [CIFAR-10] Random and Stratified Training:\n",
        "  * Without entropy selection\n",
        "  * Stratified dataset divided proportionally in half\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPar3BKKLUYf"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "partial_train_X, _, partial_train_y, _ = train_test_split(train_X, train_y, test_size=0.5, random_state=42, stratify=train_y)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(partial_train_X, partial_train_y, test_size=(0.2), random_state=42, stratify=partial_train_y)\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fmh-pbnLUYf"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "partial_train_X, _, partial_train_y, _ = train_test_split(train_X, train_y, test_size=0.5, random_state=42, stratify=train_y)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(partial_train_X, partial_train_y, test_size=(0.2), random_state=42, stratify=partial_train_y)\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 32, 3) #padronização e categorização\n",
        "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "datagen.fit(train_X)\n",
        "\n",
        "model = criacaoModeloG9()\n",
        "inicio = time.time() #tempo de inicio\n",
        "history=model.fit(datagen.flow(train_X, train_y, batch_size=128),\n",
        "                    steps_per_epoch = len(train_X) / 128, epochs=50, validation_data=(Val_X, Val_y))\n",
        "treinamentoF = time.time() #tempo final\n",
        "tempoTreino = round(treinamentoF - inicio,2)\n",
        "print(\"Tempo de treinamento: \", tempoTreino)\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIOmmoBiDSL8"
      },
      "source": [
        "## [CIFAR-10] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in training data causing \"noise\" in validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsP1pCk1LUYg"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.5, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruUim6kwC9XQ"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "\n",
        "tempoCalEntropiaIni = time.time()\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "tempoCalEntropiaFim = time.time()\n",
        "\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 32, 3) #padronização e categorização\n",
        "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "datagen.fit(train_X)\n",
        "model = criacaoModeloG9()\n",
        "inicio = time.time() #tempo de inicio\n",
        "history=model.fit(datagen.flow(train_X, train_y, batch_size=128),\n",
        "                    steps_per_epoch = len(train_X) / 128, epochs=50, validation_data=(Val_X, Val_y))\n",
        "treinamentoF = time.time() #tempo final\n",
        "tempoTreino = round(treinamentoF - inicio,2)\n",
        "print(\"Tempo de treinamento: \", tempoTreino)\n",
        "tempoCalEntropia = round(tempoCalEntropiaFim - tempoCalEntropiaIni,2)\n",
        "print(\"Tempo de calculo da entropia: \", tempoCalEntropia)\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfb0j0jZqbY-"
      },
      "source": [
        "## [CIFAR-10] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in both training and validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_lhM7J8aMzy"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvVFRHesLUYg"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "tempoCalEntropiaIni = time.time()\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,10)\n",
        "tempoCalEntropiaFim = time.time()\n",
        "\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPeparar(train_X, train_y, Val_X, Val_y, test_X, test_y, 32, 3) #padronização e categorização\n",
        "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "datagen.fit(train_X)\n",
        "model = criacaoModeloG9()\n",
        "inicio = time.time() #tempo de inicio\n",
        "history=model.fit(datagen.flow(train_X, train_y, batch_size=128),\n",
        "                    steps_per_epoch = len(train_X) / 128, epochs=50, validation_data=(Val_X, Val_y))\n",
        "treinamentoF = time.time() #tempo final\n",
        "tempoTreino = round(treinamentoF - inicio,2)\n",
        "print(\"Tempo de treinamento: \", tempoTreino)\n",
        "tempoCalEntropia = round(tempoCalEntropiaFim - tempoCalEntropiaIni,2)\n",
        "print(\"Tempo de calculo da entropia: \", tempoCalEntropia)\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU-LbuX6LUYh"
      },
      "source": [
        "## [CIFAR-10] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in both training and validation data.\n",
        "  * Cross-validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5QMMpIwHy-V"
      },
      "outputs": [],
      "source": [
        "#Corrigindo o metodo que estava errado:\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def crossValidation(train_X,train_y,n_folds = 5):\n",
        "    #divide seus dados em k folds, e treina no k−1 e valida em 1 fold, então a proporção de dados usados para validação é 1/k.\n",
        "    #2 folds (50% de validação): Treina em 50% dos dados e valida nos outros 50%.\n",
        "    #5 folds (20% de validação): Treina em 80% dos dados e valida em 20%.\n",
        "    #10 folds (10% de validação): Treina em 90% dos dados e valida em 10%.\n",
        "    n_fold = 5 if n_folds == 0 else n_folds\n",
        "    datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "    datagen.fit(train_X)\n",
        "\n",
        "    kf = KFold(n_splits = n_folds, shuffle = True, random_state = 42)  #divisão do conjunto de dados\n",
        "    fold_no = 1\n",
        "    for train_index, val_index in kf.split(train_X):\n",
        "        train_data, val_data = train_X[train_index], train_X[val_index]\n",
        "        train_label, val_label = train_y[train_index], train_y[val_index]\n",
        "\n",
        "        print(f\"Treinando a rede no folder numero: {fold_no}\")\n",
        "        model = criacaoModeloG9() # instanciando o modelo\n",
        "        inicio = time.time() #tempo de inicio\n",
        "        history=model.fit(datagen.flow(train_data, train_label, batch_size=128),\n",
        "                          steps_per_epoch = len(train_data) / 128, epochs=50, validation_data=(val_data, val_label))\n",
        "\n",
        "        treinamentoF = time.time() #tempo final\n",
        "        tempoTreino = round(treinamentoF - inicio,2)\n",
        "        scores = model.evaluate(val_data, val_label, verbose = 1)\n",
        "\n",
        "\n",
        "        print(f\"Num do Folder: {fold_no}: Taxa de Loss: {scores[0]}, Taxa de Acuracia: {scores[1]*100:.2f}%, Tamanho do Conjunto de Dados: {len(train_data)}, Tempo de Treino: {tempoTreino}\")\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.title(f'Model Accuracy: {fold_no}')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title(f'Model Loss: {fold_no}')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Para o conjunto de TREINAMENTO\n",
        "        print(\"Dados de Treino\")\n",
        "        predictions_train = model.predict(train_data) # Previsões\n",
        "        classePredita_train = np.argmax(predictions_train, axis=1)\n",
        "        classeVerdadeira_train = np.argmax(train_label, axis=1)\n",
        "        cm_train = confusion_matrix(classeVerdadeira_train, classePredita_train)\n",
        "        disp = ConfusionMatrixDisplay(cm_train)\n",
        "        disp.plot()\n",
        "        plt.title('Confusion Matrix - Training')\n",
        "        plt.show()\n",
        "\n",
        "        acc_train = accuracy_score(classeVerdadeira_train, classePredita_train) * 100  # Acurácia\n",
        "        print(\"Accuracy (Treinamento): \", '%.3f' % (acc_train))\n",
        "        sens_train = recall_score(classeVerdadeira_train, classePredita_train, average='macro')  # Sensibilidade\n",
        "        print(\"Recall (Treinamento): \", '%.3f' % (sens_train))\n",
        "        loss_train = history.history['loss'][-1]\n",
        "        print(\"Loss (Treinamento): \", '%.3f' % loss_train)\n",
        "\n",
        "        print(\"--------------------------------------------------\")\n",
        "        print(\"Dados de Validação\")\n",
        "        # Para o conjunto de VALIDAÇÃO\n",
        "        predictions_val = model.predict(val_data) # Previsões\n",
        "        classePredita_val = np.argmax(predictions_val, axis=1)\n",
        "        classeVerdadeira_val = np.argmax(val_label, axis=1)\n",
        "        cm_val = confusion_matrix(classeVerdadeira_val, classePredita_val)\n",
        "        disp = ConfusionMatrixDisplay(cm_val)\n",
        "        disp.plot()\n",
        "        plt.title('Confusion Matrix - Validation')\n",
        "        plt.show()\n",
        "\n",
        "        acc_val = accuracy_score(classeVerdadeira_val, classePredita_val) * 100  # Acurácia\n",
        "        print(\"Accuracy (Validação): \", '%.3f' % (acc_val))\n",
        "        sens_val = recall_score(classeVerdadeira_val, classePredita_val, average='macro')  # Sensibilidade\n",
        "        print(\"Recall (Validação): \", '%.3f' % (sens_val))\n",
        "        loss_val = history.history['val_loss'][-1]\n",
        "        print(\"Loss (Validação): \", '%.3f' % loss_val)\n",
        "\n",
        "        fold_no += 1\n",
        "        print(\"\\n\")\n",
        "\n",
        "#carregar os dados\n",
        "from keras.datasets import cifar10\n",
        "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, train_y, test_X, test_y = filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y,10) #novo filtro da reunião 03-10-2023\n",
        "train_X, train_y, test_X, test_y = carregarPepararValidacaoCruzada(train_X, train_y, test_X, test_y, 32, 3) #padronização e categorização\n",
        "\n",
        "train_X = np.concatenate([train_X, test_X], axis=0)\n",
        "train_y = np.concatenate([train_y, test_y], axis=0)\n",
        "\n",
        "#criar/instanciar o modelo para validação cruzada\n",
        "crossValidation(train_X,train_y, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGzIIRleLUYi"
      },
      "source": [
        "# CIFAR-100\n",
        "* Similar to CIFAR-10, but with 100 classes containing 600 images each. The classes are grouped into 20 superclasses, providing a greater diversity of objects and beings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJT8bmGOgNYI"
      },
      "source": [
        "### [CIFAR100] Creation and definition of all main functions\n",
        "\n",
        "1. Entropy Filter\n",
        "2. Loading, Preprocessing, and Handling\n",
        "3. Model Creation\n",
        "4. Statistical Predictions\n",
        "5. Accuracy and Loss Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g_jFq0ub01B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, UpSampling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from keras.datasets import mnist\n",
        "from scipy.stats import entropy\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.optimizers import SGD\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import recall_score\n",
        "from PIL import Image\n",
        "import time\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "\n",
        "def entropia(pk, base=10, max=1):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    return result\n",
        "\n",
        "def filtrar_entropia_classe(train_X, train_y, totalClasses):  # filtro de entropia\n",
        "    train_Xextend = []\n",
        "    train_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(train_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = train_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        train_Xextend.extend([train_X[i] for i in indices_filtrados_da_classe])\n",
        "        train_yextend.extend([train_y[i] for i in indices_filtrados_da_classe])\n",
        "    return np.array(train_Xextend), np.array(train_yextend)\n",
        "\n",
        "def filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y, totalClasses):  # filtro de entropia\n",
        "    train_Xextend = []\n",
        "    train_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(train_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = train_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        train_Xextend.extend([train_X[i] for i in indices_filtrados_da_classe])\n",
        "        train_yextend.extend([train_y[i] for i in indices_filtrados_da_classe])\n",
        "    test_Xextend = []\n",
        "    test_yextend = []\n",
        "    for label in range(totalClasses):\n",
        "        indices_originais = np.where(test_y == label)[0] #pega os indices originais da classe em label range\n",
        "        indicesDasImagensDaClasse = test_X[indices_originais] #Os indices das imagens da classe são armazenados\n",
        "        #Calculo a entropia para cada imagem da classe e em seguida enumera os indices da classe.\n",
        "        tuplas = [(indices_originais[index], entropia(img)) for index, img in enumerate(indicesDasImagensDaClasse)]\n",
        "        #Ordena as tuplas com base no segundo elemento, ou seja com base na entropia\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        #pega o tamanho da classe ordenada e separa a mediana da classe.\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        #Todas as imagens ordenadas pela entropia abaixo da medina são armezenadas nos indices_filtrados_da_Classe\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        #as imagens abaixo da mediana são anexadas a cada nova interação em uma nova variavel.\n",
        "        test_Xextend.extend([test_X[i] for i in indices_filtrados_da_classe])\n",
        "        test_yextend.extend([test_y[i] for i in indices_filtrados_da_classe])\n",
        "    return np.array(train_Xextend), np.array(train_yextend), np.array(test_Xextend), np.array(test_yextend)\n",
        "\n",
        "def avaliacao_statistica(train_X, train_y, Val_X, Val_y, test_X, test_y):\n",
        "    # Para o conjunto de TREINAMENTO\n",
        "    print(\"Dados de Treino\")\n",
        "    predictions_train = model.predict(train_X) # Previsões\n",
        "    classePredita_train = np.argmax(predictions_train, axis=1)\n",
        "    classeVerdadeira_train = np.argmax(train_y, axis=1)\n",
        "    cm_train = confusion_matrix(classeVerdadeira_train, classePredita_train)\n",
        "\n",
        "    plt.figure(figsize=(32, 32))  # Define o tamanho da figura\n",
        "    disp = ConfusionMatrixDisplay(cm_train)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Training')\n",
        "    plt.show()\n",
        "\n",
        "    acc_train = accuracy_score(classeVerdadeira_train, classePredita_train) * 100  # Acurácia\n",
        "    print(\"Accuracy (Treinamento): \", '%.3f' % (acc_train))\n",
        "    sens_train = recall_score(classeVerdadeira_train, classePredita_train, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Treinamento): \", '%.3f' % (sens_train))\n",
        "    loss_train = history.history['loss'][-1]\n",
        "    print(\"Loss (Treinamento): \", '%.3f' % loss_train)\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Dados de Validação\")\n",
        "    # Para o conjunto de VALIDAÇÃO\n",
        "    predictions_val = model.predict(Val_X) # Previsões\n",
        "    classePredita_val = np.argmax(predictions_val, axis=1)\n",
        "    classeVerdadeira_val = np.argmax(Val_y, axis=1)\n",
        "    cm_val = confusion_matrix(classeVerdadeira_val, classePredita_val)\n",
        "\n",
        "    plt.figure(figsize=(32, 32))  # Define o tamanho da figura\n",
        "    disp = ConfusionMatrixDisplay(cm_val)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Validation')\n",
        "    plt.show()\n",
        "\n",
        "    acc_val = accuracy_score(classeVerdadeira_val, classePredita_val) * 100  # Acurácia\n",
        "    print(\"Accuracy (Validação): \", '%.3f' % (acc_val))\n",
        "    sens_val = recall_score(classeVerdadeira_val, classePredita_val, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Validação): \", '%.3f' % (sens_val))\n",
        "    loss_val = history.history['val_loss'][-1]\n",
        "    print(\"Loss (Validação): \", '%.3f' % loss_val)\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Dados de Teste\")\n",
        "    # Para o conjunto de TESTE\n",
        "    predictions_test = model.predict(test_X) # Previsões\n",
        "    classePredita_test = np.argmax(predictions_test, axis=1)\n",
        "    classeVerdadeira_test = np.argmax(test_y, axis=1)\n",
        "    cm_test = confusion_matrix(classeVerdadeira_test, classePredita_test)\n",
        "\n",
        "    plt.figure(figsize=(32, 32))  # Define o tamanho da figura\n",
        "    disp = ConfusionMatrixDisplay(cm_test)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix - Test')\n",
        "    plt.show()\n",
        "\n",
        "    acc_test = accuracy_score(classeVerdadeira_test, classePredita_test) * 100  # Acurácia\n",
        "    print(\"Accuracy (Teste): \", '%.3f' % (acc_test))\n",
        "    sens_test = recall_score(classeVerdadeira_test, classePredita_test, average='macro')  # Sensibilidade\n",
        "    print(\"Recall (Teste): \", '%.3f' % (sens_test))\n",
        "    loss_test = history.history['val_loss'][-1]\n",
        "    print(\"Loss (Teste): \", '%.3f' % loss_test)\n",
        "\n",
        "\n",
        "def plotAcuraciaLoss(history): #plots\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  plt.tight_layout()\n",
        "\n",
        "def entHistogram(testar, num):\n",
        "  import matplotlib.pyplot as plt\n",
        "  if num == 1:\n",
        "    title = 'Class histogram train_y'\n",
        "  elif num == 2:\n",
        "    title = 'Class histogram Val_y'\n",
        "  elif num == 3:\n",
        "    title = 'Class histogram test_y'\n",
        "  plt.hist(testar, bins=10, edgecolor='black', alpha=0.7)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "\n",
        "def carregarPepararCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y): # Carregamento e Tratamento\n",
        "  train_X = train_X.astype(\"float\")/255\n",
        "  Val_X = Val_X.astype(\"float\")/255\n",
        "  test_X = test_X.astype(\"float\")/255\n",
        "  train_y = to_categorical(train_y, num_classes=100)\n",
        "  Val_y = to_categorical(Val_y, num_classes=100)\n",
        "  test_y = to_categorical(test_y, num_classes=100)\n",
        "  return train_X, train_y, Val_X, Val_y, test_X, test_y\n",
        "\n",
        "def carregarPepararValidacaoCruzadaCifar100(train_X, train_y, test_X, test_y): # Carregamento e Tratamento\n",
        "  train_X = train_X.astype(\"float\")/255\n",
        "  test_X = test_X.astype(\"float\")/255\n",
        "  train_y = to_categorical(train_y, num_classes=100)\n",
        "  test_y = to_categorical(test_y, num_classes=100)\n",
        "  return train_X, train_y, test_X, test_y\n",
        "\n",
        "def criarModeloCifar100():\n",
        "  #Importing the Resnet Model\n",
        "  from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "  resnet_model = ResNet50(\n",
        "      include_top = False,\n",
        "      weights = 'imagenet',\n",
        "      input_shape = (224,224,3)\n",
        "  )\n",
        "  for layer in resnet_model.layers:\n",
        "      if isinstance(layer, BatchNormalization):\n",
        "          layer.trainable = True\n",
        "      else:\n",
        "          layer.trainable = False\n",
        "\n",
        "\n",
        "  #reducao da taxa de aprendizagem\n",
        "  from keras.callbacks import ReduceLROnPlateau\n",
        "  learning_rate_reduction = ReduceLROnPlateau(\n",
        "      monitor='val_accuracy',\n",
        "      patience=2,\n",
        "      verbose=1,\n",
        "      factor=0.96,\n",
        "      min_lr=1e-8)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(UpSampling2D(size=(7, 7),interpolation='bilinear'))\n",
        "  model.add(resnet_model)\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=None))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(100, activation='softmax'))\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n",
        "  model.compile(\n",
        "      optimizer = optimizer,\n",
        "      loss='categorical_crossentropy',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  return model, learning_rate_reduction\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "def return_label(preds):\n",
        "    return np.argmax(preds, axis=1)\n",
        "\n",
        "def avaliacao_statisticaCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y, model, history):\n",
        "\n",
        "    datasets = {\n",
        "        \"Training\": (train_X, train_y),\n",
        "        \"Validation\": (Val_X, Val_y),\n",
        "        \"Test\": (test_X, test_y)\n",
        "    }\n",
        "\n",
        "    for name, data in datasets.items():\n",
        "        X, y = data\n",
        "        print(f\"--------------------------------------------------\")\n",
        "        print(f\"Dados de {name}\")\n",
        "\n",
        "        # Previsões\n",
        "        predictions = model.predict(X)\n",
        "        pred_labels = return_label(predictions)\n",
        "        true_labels = return_label(y)\n",
        "\n",
        "        # Preparando a matriz de confusão\n",
        "        conf_mat = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "        # Exibindo a matriz de confusão usando heatmap\n",
        "        df_cm = pd.DataFrame(conf_mat, index=[i for i in range(100)], columns=[i for i in range(100)])\n",
        "        plt.figure(figsize=(32, 32))\n",
        "        #sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "        sn.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"viridis\")\n",
        "        plt.title(f'Confusion Matrix - {name}')\n",
        "        plt.show()\n",
        "\n",
        "        # Métricas\n",
        "        acc = accuracy_score(true_labels, pred_labels) * 100\n",
        "        sens = recall_score(true_labels, pred_labels, average='macro')\n",
        "\n",
        "        if name == \"Treinamento\":\n",
        "            loss = history.history['loss'][-1]\n",
        "        else:\n",
        "            loss = history.history['val_loss'][-1]\n",
        "\n",
        "        print(f\"Accuracy ({name}): \", '%.3f' % (acc))\n",
        "        print(f\"Recall ({name}): \", '%.3f' % (sens))\n",
        "        print(f\"Loss ({name}): \", '%.3f' % loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpd32mnoLUYi"
      },
      "source": [
        "### [CIFAR100] Full Dataset Training:\n",
        "  * Without entropy selection\n",
        "  * Full Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHLhrSBOLUYi"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPepararCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "datagen.fit(train_X)\n",
        "\n",
        "model, learning_rate_reduction = criarModeloCifar100()\n",
        "\n",
        "tempoTreinamento = time.time() #tempo para treinamento\n",
        "history = model.fit(\n",
        "    datagen.flow(train_X, train_y, batch_size=128),\n",
        "    steps_per_epoch=len(train_X) / 128,\n",
        "    epochs=50,\n",
        "    validation_data=(Val_X, Val_y),\n",
        "    callbacks=[learning_rate_reduction]\n",
        ")\n",
        "tempoFimTreino = time.time() #tempo para treinamento\n",
        "tempoFinalTreino = round(tempoFimTreino - tempoTreinamento,2)\n",
        "print(\"Tempo de treinamento(s): \", tempoFinalTreino)\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statisticaCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y, model, history)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZf-VHmNLUYj"
      },
      "source": [
        "### [CIFAR100] Random and Stratified Training:\n",
        "  * Without entropy selection\n",
        "  * Stratified dataset divided proportionally in half\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxYtLIU7LUYj"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "partial_train_X, _, partial_train_y, _ = train_test_split(train_X, train_y, test_size=0.5, random_state=42, stratify=train_y)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(partial_train_X, partial_train_y, test_size=(0.2), random_state=42, stratify=partial_train_y)\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPepararCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "datagen.fit(train_X)\n",
        "\n",
        "model, learning_rate_reduction = criarModeloCifar100()\n",
        "\n",
        "tempoTreinamento = time.time() #tempo para treinamento\n",
        "history = model.fit(\n",
        "    datagen.flow(train_X, train_y, batch_size=128),\n",
        "    steps_per_epoch=len(train_X) / 128,\n",
        "    epochs=50,\n",
        "    validation_data=(Val_X, Val_y),\n",
        "    callbacks=[learning_rate_reduction]\n",
        ")\n",
        "tempoFimTreino = time.time() #tempo para treinamento\n",
        "tempoFinalTreino = round(tempoFimTreino - tempoTreinamento,2)\n",
        "print(\"Tempo de treinamento(s): \", tempoFinalTreino)\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statisticaCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y, model, history)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOlbkbAELUYk"
      },
      "source": [
        "### [CIFAR100] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in training data causing \"noise\" in validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cGBOn5XyfQsi"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,100)\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hgfD5qilLUYk"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "tempoFiltroEntropiaInicio = time.time() #tempo para calculo do filtro\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,100)\n",
        "tempoFiltroEntropiaFim = time.time() #Tempo final do calculo do filtro\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPepararCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "datagen.fit(train_X)\n",
        "\n",
        "model, learning_rate_reduction = criarModeloCifar100()\n",
        "\n",
        "tempoTreinamento = time.time() #tempo para treinamento\n",
        "history = model.fit(\n",
        "    datagen.flow(train_X, train_y, batch_size=128),\n",
        "    steps_per_epoch=len(train_X) / 128,\n",
        "    epochs=50,\n",
        "    validation_data=(Val_X, Val_y),\n",
        "    callbacks=[learning_rate_reduction]\n",
        ")\n",
        "tempoFimTreino = time.time() #tempo para treinamento\n",
        "tempoFinalTreino = round(tempoFimTreino - tempoTreinamento,2)\n",
        "print(\"Tempo de treinamento(s): \", tempoFinalTreino)\n",
        "TempoCalEntropia = round(tempoFiltroEntropiaFim - tempoFiltroEntropiaInicio,2)\n",
        "print(\"Tempo de Calculo Entropia(s): \", TempoCalEntropia)\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statisticaCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y, model, history)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRYt5-LcLUYl"
      },
      "source": [
        "### [CIFAR100] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in both training and validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmC1AK4efbgb"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,100)\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "entHistogram(train_y, 1)\n",
        "entHistogram(Val_y, 2)\n",
        "entHistogram(test_y, 3)\n",
        "print(len(train_X))\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "print(len(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlgZZ9y4LUYm"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "tempoFiltroEntropiaInicio = time.time() #tempo para calculo do filtro\n",
        "train_X, train_y = filtrar_entropia_classe(train_X, train_y,100)\n",
        "tempoFiltroEntropiaFim = time.time() #Tempo final do calculo do filtro\n",
        "train_X, Val_X, train_y, Val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y) # divisao do conjunto de dados\n",
        "train_X, train_y, Val_X, Val_y, test_X, test_y = carregarPepararCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y)\n",
        "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "datagen.fit(train_X)\n",
        "\n",
        "model, learning_rate_reduction = criarModeloCifar100()\n",
        "\n",
        "tempoTreinamento = time.time() #tempo para treinamento\n",
        "history = model.fit(\n",
        "    datagen.flow(train_X, train_y, batch_size=128),\n",
        "    steps_per_epoch=len(train_X) / 128,\n",
        "    epochs=50,\n",
        "    validation_data=(Val_X, Val_y),\n",
        "    callbacks=[learning_rate_reduction]\n",
        ")\n",
        "tempoFimTreino = time.time() #tempo para treinamento\n",
        "tempoFinalTreino = round(tempoFimTreino - tempoTreinamento,2)\n",
        "print(\"Tempo de treinamento(s): \", tempoFinalTreino)\n",
        "TempoCalEntropia = round(tempoFiltroEntropiaFim - tempoFiltroEntropiaInicio,2)\n",
        "print(\"Tempo de Calculo Entropia(s): \", TempoCalEntropia)\n",
        "print(\"train_X: \",len(train_X))\n",
        "print(\"train_y: \",len(train_y))\n",
        "print(\"Val_X: \",len(Val_X))\n",
        "print(\"Val_y: \",len(Val_y))\n",
        "print(\"test_X: \",len(test_X))\n",
        "print(\"test_y: \",len(test_y))\n",
        "avaliacao_statisticaCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y, model, history)\n",
        "plotAcuraciaLoss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CakuQ8v1LUYm"
      },
      "source": [
        "### [CIFAR100] Entropy Filter:\n",
        "  * Filtered Dataset\n",
        "  * Low Entropy in both training and validation data.\n",
        "  * Cross-validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CiD7vzweLUYn"
      },
      "outputs": [],
      "source": [
        "def crossValidation(train_X,train_y,n_folds = 5):\n",
        "    from sklearn.model_selection import KFold\n",
        "    #divide seus dados em k folds, e treina no k−1 e valida em 1 fold, então a proporção de dados usados para validação é 1/k.\n",
        "    #2 folds (50% de validação): Treina em 50% dos dados e valida nos outros 50%.\n",
        "    #5 folds (20% de validação): Treina em 80% dos dados e valida em 20%.\n",
        "    #10 folds (10% de validação): Treina em 90% dos dados e valida em 10%.\n",
        "    n_fold = 5 if n_folds == 0 else n_folds\n",
        "    datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "    datagen.fit(train_X)\n",
        "\n",
        "    kf = KFold(n_splits = n_folds, shuffle = True, random_state = 42)  #divisão do conjunto de dados\n",
        "    fold_no = 1\n",
        "    for train_index, val_index in kf.split(train_X):\n",
        "        train_data, val_data = train_X[train_index], train_X[val_index]\n",
        "        train_label, val_label = train_y[train_index], train_y[val_index]\n",
        "\n",
        "        print(f\"Treinando a rede no folder numero: {fold_no}\")\n",
        "        model, learning_rate_reduction = criarModeloCifar100()\n",
        "        datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
        "        datagen.fit(train_X)\n",
        "\n",
        "        model, learning_rate_reduction = criarModeloCifar100()\n",
        "\n",
        "        tempoTreinamento = time.time() #tempo para treinamento\n",
        "        history = model.fit(\n",
        "            datagen.flow(train_X, train_y, batch_size=128),\n",
        "            steps_per_epoch=len(train_X) / 128,\n",
        "            epochs=50,\n",
        "            validation_data=(Val_X, Val_y),\n",
        "            callbacks=[learning_rate_reduction]\n",
        "        )\n",
        "        tempoFimTreino = time.time() #tempo para treinamento\n",
        "        tempoFinalTreino = round(tempoFimTreino - tempoTreinamento,2)\n",
        "        print(\"Tempo de treinamento(s): \", tempoFinalTreino)\n",
        "        TempoCalEntropia = round(tempoFiltroEntropiaFim - tempoFiltroEntropiaInicio,2)\n",
        "        print(\"Tempo de Calculo Entropia(s): \", TempoCalEntropia)\n",
        "        print(\"train_X: \",len(train_X))\n",
        "        print(\"train_y: \",len(train_y))\n",
        "        print(\"Val_X: \",len(Val_X))\n",
        "        print(\"Val_y: \",len(Val_y))\n",
        "        print(\"test_X: \",len(test_X))\n",
        "        print(\"test_y: \",len(test_y))\n",
        "        avaliacao_statisticaCifar100(train_X, train_y, Val_X, Val_y, test_X, test_y, model, history)\n",
        "        plotAcuraciaLoss(history)\n",
        "\n",
        "        fold_no += 1\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
        "tempoFiltro1 = time.time() #tempo de filtro\n",
        "train_X, train_y, test_X, test_y = filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y,100) #novo filtro da reunião 03-10-2023\n",
        "tempoFiltro2 = time.time() #tempo de filtro\n",
        "train_X, train_y, test_X, test_y = carregarPepararValidacaoCruzadaCifar100(train_X, train_y, test_X, test_y) #padronização e categorização\n",
        "\n",
        "train_X = np.concatenate([train_X, test_X], axis=0)\n",
        "train_y = np.concatenate([train_y, test_y], axis=0)\n",
        "\n",
        "#criar/instanciar o modelo para validação cruzada\n",
        "crossValidation(train_X,train_y, 5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}