{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Description\n",
        "\n",
        "A sophisticated approach to training Convolutional Neural Networks (CNNs) in a distributed environment is federated learning with PyTorch. Clients receive data and train their version of the model locally. Afterwards, the models are combined using federated averaging, creating a stronger global model. A unified test set is used to evaluate performance, and metrics such as loss, accuracy, and confusion matrix are employed for analysis.\n",
        "\n",
        "Referência: [PYTORCH DOCUMENTATION](https://pytorch.org/docs/stable/index.html)\n",
        "Referência: [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
        "Referência: [PySyft’s documentation](https://openmined.github.io/PySyft/index.html)\n",
        "Referência: [Hello PyTorch: NVIDIA](https://nvflare.readthedocs.io/en/main/examples/hello_pt.html)\n",
        "\n",
        "Autor: Ernesto Gurgel Valente Neto\n",
        "\n"
      ],
      "metadata": {
        "id": "fjGSVGu4BV2O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVAQpcvl0E13"
      },
      "source": [
        "# Fed-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed implementation of a federated learning system using PyTorch, aimed at training a Convolutional Neural Network (CNN).\n",
        "**Structure and Functionality:**\n",
        "\n",
        "*   **Federated Data Distribution:** The training dataset is divided among multiple clients to simulate a federated environment, where each client possesses a portion of the data.\n",
        "\n",
        "*   **Data Preparation:** Utilizes transformations and normalizations to prepare the training and testing datasets for efficient network training.\n",
        "\n",
        "*   **Local Training and Evaluation:** Each client trains its model locally using its own data. After training, the models are evaluated to check the performance before aggregation.\n",
        "\n",
        "*   **Model Aggregation (aggregate_models):** The locally trained models are aggregated into a single global model using the federated averaging technique, which combines the weights of the models to form an updated and potentially more robust model.\n",
        "\n",
        "*   **Evaluation of Global Model:** The resulting global model is tested using a unified test dataset to evaluate its overall performance and ensure it generalizes well on unseen data.\n",
        "\n",
        "*   **Metrics Visualization:** Features for visualizing training progress, including loss and accuracy over time, as well as the confusion matrix for detailed analysis of model performance.\n",
        "\n",
        "*   **CNN Architecture:** CNN architecture for image processing. The networks utilize convolutional layers to extract features from images, followed by dense layers that classify the images.\n"
      ],
      "metadata": {
        "id": "eomvxPFoF5xJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro9wSM_m1kYV"
      },
      "source": [
        "### FUNCTIONS TO FED-ENVIRONMENT.\n",
        "\n",
        "The main objective functions generate subsets of data that are representative of NON-IID federated learning environments, simulating a situation where different users have data from different classes and in varying quantities.\n",
        "\n",
        "*   NON-IID FUNCTIONS\n",
        "*   FED-LEARNING FUNCTIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NON-IID FUNCTIONS\n",
        "\n",
        "Each function starts by calculating the number of \"shards\" or data segments available based on the size of the original dataset and the number of samples per class.\n",
        "Users receive data from a specific number of randomly chosen classes, ensuring that each user has a unique and diversified subset of data in terms of classes.\n",
        "Sample selection for each class is done randomly without replacement, ensuring no duplicates in the samples distributed to each user."
      ],
      "metadata": {
        "id": "h7lPRfZBcC-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [NO FILTER] IDD&NON-IDD FUNCTIONS\n",
        "*   Creates a federated environment with balanced and unbalanced data distributions.\n"
      ],
      "metadata": {
        "id": "jIKlxBKMUj07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MNISTNonIID(train_dataset, num_users, n_class=2, num_samples=3000, rate_unbalance=0):\n",
        "    # Parâmetros:\n",
        "    # - num_users: Número de usuários/clientes.\n",
        "    # - n_class: Número de classes por usuário.\n",
        "    # - num_samples: Número base de amostras por classe para cada usuário.\n",
        "    # - rate_unbalance: Fator de desequilíbrio na distribuição das amostras.\n",
        "    num_shards_train = int(60000 / num_samples)\n",
        "    num_classes = 10  # Total de classes\n",
        "    assert(n_class * num_users <= num_shards_train), \"There are not enough shards to allocate\"\n",
        "    assert(n_class <= num_classes), \"Number of classes per user greater than the total available classes\"\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = [Subset(train_dataset, indices.astype(int)) for indices in users_dict_train.values()]\n",
        "    return client_datasets\n",
        "\n",
        "def FashionMNISTNonIID(train_dataset, num_users, n_class=2, num_samples=3000, rate_unbalance=0):\n",
        "    # Parâmetros:\n",
        "    # - num_users: Número de usuários/clientes.\n",
        "    # - n_class: Número de classes por usuário.\n",
        "    # - num_samples: Número base de amostras por classe para cada usuário.\n",
        "    # - rate_unbalance: Fator de desequilíbrio na distribuição das amostras.\n",
        "    num_shards_train = int(60000 / num_samples)\n",
        "    num_classes = 10  # Total de classes\n",
        "    assert(n_class * num_users <= num_shards_train), \"There are not enough shards to allocate\"\n",
        "    assert(n_class <= num_classes), \"Number of classes per user greater than the total available classes\"\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = [Subset(train_dataset, indices.astype(int)) for indices in users_dict_train.values()]\n",
        "    return client_datasets\n",
        "\n",
        "def Cifar10NonIID(train_dataset, num_users, n_class=2, num_samples=2500, rate_unbalance=0):\n",
        "    # Parâmetros:\n",
        "    # - num_users: Número de usuários/clientes.\n",
        "    # - n_class: Número de classes por usuário.\n",
        "    # - num_samples: Número base de amostras por classe para cada usuário.\n",
        "    # - rate_unbalance: Fator de desequilíbrio na distribuição das amostras.\n",
        "    num_shards_train = int(50000 / num_samples)\n",
        "    num_classes = len(np.unique(train_dataset.targets))\n",
        "    assert(n_class * num_users <= num_shards_train), \"There are not enough shards to allocate\"\n",
        "    assert(n_class <= num_classes), \"Number of classes per user greater than the total available classes\"\n",
        "    # Preparação dos índices de dados\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    # Dicionário para armazenar os índices dos dados para cada usuário\n",
        "    users_dict = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        user_labels = np.array([])\n",
        "        # Seleciona classes de forma aleatória e sem reposição para garantir diversidade\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict[user] = np.concatenate((users_dict[user], idxs), axis=0)\n",
        "    # Criar datasets para cada cliente baseado nos índices selecionados\n",
        "    client_datasets = [Subset(train_dataset, indices.astype(int)) for indices in users_dict.values()]\n",
        "    return client_datasets\n",
        "\n",
        "def Cifar100NonIID(train_dataset, num_users, n_class=20, num_samples=500, rate_unbalance=0):\n",
        "    # Parâmetros:\n",
        "    # - num_users: Número de usuários/clientes.\n",
        "    # - n_class: Número de classes por usuário.\n",
        "    # - num_samples: Número base de amostras por classe para cada usuário.\n",
        "    # - rate_unbalance: Fator de desequilíbrio na distribuição das amostras.\n",
        "    num_shards_train = int(50000 / num_samples)  # Baseado no tamanho do CIFAR-100\n",
        "    num_classes = 100  # Total de classes no CIFAR-100\n",
        "    assert(n_class * num_users <= num_shards_train), \"There are not enough shards to allocate\"\n",
        "    assert(n_class <= num_classes), \"Number of classes per user greater than the total available classes\"\n",
        "    # Preparação dos índices de dados\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    # Dicionário para armazenar os índices dos dados para cada usuário\n",
        "    users_dict = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        # Escolha aleatória de classes para garantir diversidade\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict[user] = np.concatenate((users_dict[user], idxs), axis=0)\n",
        "    # Criar datasets para cada cliente baseado nos índices selecionados\n",
        "    client_datasets = [Subset(train_dataset, indices.astype(int)) for indices in users_dict.values()]\n",
        "    return client_datasets\n",
        "\n",
        "# Parâmetros:\n",
        "# - num_users: Número de usuários/clientes.\n",
        "# - n_class: Número de classes por usuário.\n",
        "# - num_samples: Número base de amostras por classe para cada usuário.\n",
        "# - rate_unbalance: Fator de desequilíbrio na distribuição das amostras.\n",
        "# - Alto fator de desbalanceamento de quanitade de classes e dados (DADOS NON-IDD).\n",
        "\n",
        "def MNISTNonIIDunbalance(train_dataset, num_users, n_class=2, num_samples=3000, rate_unbalance=1.2):\n",
        "    num_classes = 10  # Total de classes para MNIST\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    # Calcula o número de amostras por usuário considerando o desequilíbrio\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        # Aplica o rate_unbalance de forma cumulativa\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = [Subset(train_dataset, indices.astype(int)) for indices in users_dict_train.values()]\n",
        "    return client_datasets\n",
        "\n",
        "def FashionMNISTNonIIDunbalance(train_dataset, num_users, n_class=2, num_samples=3000, rate_unbalance=1.2):\n",
        "    num_classes = 10  # Total de classes para FashionMNIST\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = [Subset(train_dataset, indices.astype(int)) for indices in users_dict_train.values()]\n",
        "    return client_datasets\n",
        "\n",
        "def Cifar10NonIIDunbalance(train_dataset, num_users, n_class=2, num_samples=2500, rate_unbalance=1.2):\n",
        "    num_classes = len(np.unique(train_dataset.targets))  # Total de classes para CIFAR10\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict[user] = np.concatenate((users_dict[user], idxs), axis=0)\n",
        "    client_datasets = [Subset(train_dataset, indices.astype(int)) for indices in users_dict.values()]\n",
        "    return client_datasets\n",
        "\n",
        "def Cifar100NonIIDunbalance(train_dataset, num_users, n_class=20, num_samples=500, rate_unbalance=1.2):\n",
        "    num_classes = 100  # Total de classes para CIFAR100\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict[user] = np.concatenate((users_dict[user], idxs), axis=0)\n",
        "    client_datasets = [Subset(train_dataset, indices.astype(int)) for indices in users_dict.values()]\n",
        "    return client_datasets"
      ],
      "metadata": {
        "id": "k1rX_kL8cHSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [ENTROPY FILTER] IDD&NON-IDD FUNCTIONS\n",
        "*   Creates a federated environment with balanced and unbalanced data distributions where entropy is applied before training.\n"
      ],
      "metadata": {
        "id": "sRhf1AU6Vb2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funções de entropia, calcula a entropia de cada classe individualmente\n",
        "# Seleciona as imagens de acordo com a distribuição de entropia\n",
        "# Recebe o numero de classes e os conjuntos de dados de train_X e train_y.\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def entropia(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    return result\n",
        "\n",
        "def filtrar_entropia_classe(train_X, train_y, classes_relevantes):\n",
        "    start_time = time.time()  # Inicia a contagem de tempo\n",
        "    train_Xextend = []\n",
        "    train_yextend = []\n",
        "    indices_filtrados_extend = []\n",
        "    for label in classes_relevantes: #Se existe amostras para a classe atual\n",
        "        indices_originais = np.where(train_y == label)[0]\n",
        "        if len(indices_originais) == 0:\n",
        "            continue  # Se não há amostras para a classe atual\n",
        "        indicesDasImagensDaClasse = train_X[indices_originais]\n",
        "        tuplas = [(indice, entropia(img)) for indice, img in zip(indices_originais, indicesDasImagensDaClasse)]\n",
        "        local_ordenado = sorted(tuplas, key=lambda x: x[1])\n",
        "        n = len(local_ordenado)\n",
        "        if n % 2 == 1:\n",
        "            median = local_ordenado[n // 2][1]\n",
        "        else:\n",
        "            median = (local_ordenado[n // 2 - 1][1] + local_ordenado[n // 2][1]) / 2.0\n",
        "        indices_filtrados_da_classe = [item[0] for item in local_ordenado if item[1] <= median]\n",
        "        train_Xextend.extend(train_X[indice] for indice in indices_filtrados_da_classe)\n",
        "        train_yextend.extend(train_y[indice] for indice in indices_filtrados_da_classe)\n",
        "        indices_filtrados_extend.extend(indices_filtrados_da_classe)\n",
        "    end_time = time.time()  # Finaliza a contagem de tempo\n",
        "    print(f\"Total entropy execution time: {end_time - start_time} seconds.\")\n",
        "    return np.array(train_Xextend), np.array(train_yextend), np.array(indices_filtrados_extend)\n",
        "\n",
        "    # Parâmetros:\n",
        "    # - num_users: Número de usuários/clientes.\n",
        "    # - n_class: Número de classes por usuário.\n",
        "    # - num_samples: Número base de amostras por classe para cada usuário.\n",
        "    # - rate_unbalance: Fator de desequilíbrio na distribuição das amostras.\n",
        "def MNISTNonIID_filtrado(train_dataset, num_users, n_class=2, num_samples=3000, rate_unbalance=0):\n",
        "    num_shards_train = int(60000 / num_samples)\n",
        "    num_classes = 10  # Total de classes para MNIST\n",
        "    assert(n_class * num_users <= num_shards_train), \"There are not enough shards to allocate\"\n",
        "    assert(n_class <= num_classes), \"Number of classes per user greater than the total available classes\"\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    chosen_classes_per_user = {}  # Novo dicionário para armazenar as classes escolhidas para cada usuário\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        chosen_classes_per_user[user] = chosen_classes  # Armazena as classes escolhidas para o usuário\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets_filtrados = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)].numpy() for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        # Aplicar filtragem baseada em entropia\n",
        "        train_X_filtrado, train_y_filtrado, indices_filtrados = filtrar_entropia_classe(train_X, train_y, chosen_classes_per_user[user])  # Passa as classes escolhidas\n",
        "        # Corrige para usar os índices filtrados corretamente\n",
        "        indices_filtrados_absolutos = [int(indices[int(i)]) for i in indices_filtrados]\n",
        "        client_datasets_filtrados.append(Subset(train_dataset, indices_filtrados_absolutos))\n",
        "    return client_datasets_filtrados\n",
        "\n",
        "def FashionMNISTNonIID_filtrado(train_dataset, num_users, n_class=2, num_samples=3000, rate_unbalance=0):\n",
        "    num_shards_train = int(60000 / num_samples)\n",
        "    num_classes = 10  # Total de classes\n",
        "    assert(n_class * num_users <= num_shards_train), \"There are not enough shards to allocate\"\n",
        "    assert(n_class <= num_classes), \"Number of classes per user greater than the total available classes\"\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    chosen_classes_per_user = {}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        chosen_classes_per_user[user] = chosen_classes\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets_filtrados = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)].numpy() for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X_filtrado, train_y_filtrado, indices_filtrados = filtrar_entropia_classe(train_X, train_y, chosen_classes_per_user[user])\n",
        "        # Corrige para usar os índices filtrados corretamente\n",
        "        indices_filtrados_absolutos = [int(indices[int(i)]) for i in indices_filtrados]\n",
        "        client_datasets_filtrados.append(Subset(train_dataset, indices_filtrados_absolutos))\n",
        "    return client_datasets_filtrados\n",
        "\n",
        "def Cifar10NonIID_filtrado(train_dataset, num_users, n_class=2, num_samples=2500, rate_unbalance=0):\n",
        "    num_shards_train = int(50000 / num_samples)  # Baseado no tamanho do CIFAR-10\n",
        "    num_classes = len(np.unique(train_dataset.targets))  # Total de classes no CIFAR-10\n",
        "    assert(n_class * num_users <= num_shards_train), \"There are not enough shards to allocate\"\n",
        "    assert(n_class <= num_classes), \"Number of classes per user greater than the total available classes\"\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict = {i: np.array([]) for i in range(num_users)}  # Corrigido para users_dict\n",
        "    chosen_classes_per_user = {}  # Dicionário para armazenar as classes escolhidas para cada usuário\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)  # Escolhe n_class classes aleatoriamente\n",
        "        chosen_classes_per_user[user] = chosen_classes  # Armazena as classes escolhidas para este usuário\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)  # Escolhe num_samples índices para cada classe\n",
        "            users_dict[user] = np.concatenate((users_dict[user], idxs), axis=0)  # Concatena os índices no dicionário do usuário\n",
        "    client_datasets_filtrados = []\n",
        "    for user, indices in users_dict.items():\n",
        "        #train_X = np.array([train_dataset.data[int(i)].numpy() for i in indices])\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        # Aplicar filtragem baseada em entropia\n",
        "        train_X_filtrado, train_y_filtrado, indices_filtrados = filtrar_entropia_classe(train_X, train_y, chosen_classes_per_user[user])  # Passa as classes escolhidas para a função\n",
        "        # Corrige para usar os índices filtrados corretamente\n",
        "        indices_filtrados_absolutos = [int(indices[int(i)]) for i in indices_filtrados]\n",
        "        client_datasets_filtrados.append(Subset(train_dataset, indices_filtrados_absolutos))\n",
        "    return client_datasets_filtrados\n",
        "\n",
        "def Cifar100NonIID_filtrado(train_dataset, num_users, n_class=20, num_samples=250, rate_unbalance=0):\n",
        "    num_shards_train = int(50000 / num_samples)\n",
        "    num_classes = 100  # Total de classes no CIFAR-100\n",
        "    assert(n_class * num_users <= num_shards_train), \"There are not enough shards to allocate\"\n",
        "    assert(n_class <= num_classes), \"Number of classes per user greater than the total available classes\"\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict = {i: np.array([]) for i in range(num_users)}\n",
        "    chosen_classes_per_user = {}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        chosen_classes_per_user[user] = chosen_classes\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict[user] = np.concatenate((users_dict[user], idxs), axis=0)\n",
        "    client_datasets_filtrados = []\n",
        "    for user, indices in users_dict.items():\n",
        "        #train_X = np.array([train_dataset.data[int(i)].numpy() for i in indices])\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X_filtrado, train_y_filtrado, indices_filtrados = filtrar_entropia_classe(train_X, train_y, chosen_classes_per_user[user])\n",
        "        # Corrige para usar os índices filtrados corretamente\n",
        "        indices_filtrados_absolutos = [int(indices[int(i)]) for i in indices_filtrados]\n",
        "        client_datasets_filtrados.append(Subset(train_dataset, indices_filtrados_absolutos))\n",
        "    return client_datasets_filtrados\n",
        "\n",
        "# Parâmetros:\n",
        "# - num_users: Número de usuários/clientes.\n",
        "# - n_class: Número de classes por usuário.\n",
        "# - num_samples: Número base de amostras por classe para cada usuário.\n",
        "# - rate_unbalance: Fator de desequilíbrio na distribuição das amostras.\n",
        "# - Alto fator de desbalanceamento de quanitade de classes e dados (DADOS NON-IDD).\n",
        "def MNISTNonIIDunbalance_filtrado(train_dataset, num_users, n_class=2, num_samples=3000, rate_unbalance=1.2):\n",
        "    num_classes = 10  # Total de classes para MNIST\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    chosen_classes_per_user = {}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        chosen_classes_per_user[user] = chosen_classes\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets_filtrados = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)].numpy() for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X_filtrado, train_y_filtrado, indices_filtrados = filtrar_entropia_classe(train_X, train_y, chosen_classes_per_user[user])\n",
        "        # Corrige para usar os índices filtrados corretamente\n",
        "        indices_filtrados_absolutos = [int(indices[int(i)]) for i in indices_filtrados]\n",
        "        client_datasets_filtrados.append(Subset(train_dataset, indices_filtrados_absolutos))\n",
        "    return client_datasets_filtrados\n",
        "\n",
        "def FashionMNISTNonIIDunbalance_filtrado(train_dataset, num_users, n_class=2, num_samples=3000, rate_unbalance=1.2):\n",
        "    num_classes = 10  # Total de classes para FashionMNIST\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    chosen_classes_per_user = {}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        chosen_classes_per_user[user] = chosen_classes\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))  # Ajuste para desbalanceamento\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets_filtrados = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)].numpy() for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X_filtrado, train_y_filtrado, indices_filtrados = filtrar_entropia_classe(train_X, train_y, chosen_classes_per_user[user])\n",
        "        # Corrige para usar os índices filtrados corretamente\n",
        "        indices_filtrados_absolutos = [int(indices[int(i)]) for i in indices_filtrados]\n",
        "        client_datasets_filtrados.append(Subset(train_dataset, indices_filtrados_absolutos))\n",
        "    return client_datasets_filtrados\n",
        "\n",
        "def Cifar10NonIIDunbalance_filtrado(train_dataset, num_users, n_class=2, num_samples=2500, rate_unbalance=1.2):\n",
        "    num_classes = len(np.unique(train_dataset.targets))\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict = {i: np.array([]) for i in range(num_users)}\n",
        "    chosen_classes_per_user = {}  # Adiciona armazenamento para as classes escolhidas por usuário\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        chosen_classes_per_user[user] = chosen_classes  # Armazena as classes escolhidas\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))  # Aplica o desbalanceamento\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict[user] = np.concatenate((users_dict[user], idxs), axis=0)\n",
        "    client_datasets_filtrados = []\n",
        "    for user, indices in users_dict.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        # Aplica a filtragem baseada em entropia passando as classes escolhidas\n",
        "        train_X_filtrado, train_y_filtrado, indices_filtrados = filtrar_entropia_classe(train_X, train_y, chosen_classes_per_user[user])\n",
        "        # Corrige para usar os índices filtrados corretamente\n",
        "        indices_filtrados_absolutos = [int(indices[int(i)]) for i in indices_filtrados]\n",
        "        client_datasets_filtrados.append(Subset(train_dataset, indices_filtrados_absolutos))\n",
        "    return client_datasets_filtrados\n",
        "\n",
        "def Cifar100NonIIDunbalance_filtrado(train_dataset, num_users, n_class=20, num_samples=250, rate_unbalance=1.2):\n",
        "    num_classes = 100  # Total de classes para CIFAR-100\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict = {i: np.array([]) for i in range(num_users)}\n",
        "    chosen_classes_per_user = {}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        chosen_classes_per_user[user] = chosen_classes\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))  # Ajuste para desbalanceamento\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict[user] = np.concatenate((users_dict[user], idxs), axis=0)\n",
        "    client_datasets_filtrados = []\n",
        "    for user, indices in users_dict.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        # Aplica a filtragem baseada em entropia passando as classes escolhidas\n",
        "        train_X_filtrado, train_y_filtrado, indices_filtrados = filtrar_entropia_classe(train_X, train_y, chosen_classes_per_user[user])\n",
        "        # Corrige para usar os índices filtrados corretamente\n",
        "        indices_filtrados_absolutos = [int(indices[int(i)]) for i in indices_filtrados]\n",
        "        client_datasets_filtrados.append(Subset(train_dataset, indices_filtrados_absolutos))\n",
        "    return client_datasets_filtrados\n"
      ],
      "metadata": {
        "id": "N3MTTg89JVhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [RANDOM FILTER] IDD&NON-IDD FUNCTIONS\n",
        "*   Creates a federated environment with balanced and unbalanced data distributions where entropy is applied before training.\n"
      ],
      "metadata": {
        "id": "Xhtl8iISY4f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def dividir_dados(train_X, train_y, test_size=0.5, random_state=42):\n",
        "    # Divisão dos dados em 50% com estratificação\n",
        "    train_X, _, train_y, _ = train_test_split(train_X, train_y, test_size=test_size, random_state=random_state, stratify=train_y)\n",
        "    return train_X, train_y\n",
        "\n",
        "# Parâmetros:\n",
        "# - num_users: Número de usuários/clientes.\n",
        "# - n_class: Número de classes por usuário.\n",
        "# - num_samples: Número base de amostras por classe para cada usuário.\n",
        "# - rate_unbalance: Fator de desequilíbrio na distribuição das amostras.\n",
        "def MNISTNonIID_filtrorandom(train_dataset, num_users, n_class=2, num_samples=3000, test_size=0.5, rate_unbalance=0):\n",
        "    num_classes = 10  # Total de classes para MNIST\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X, train_y = dividir_dados(train_X, train_y, test_size=test_size)\n",
        "        # Utiliza os índices após a divisão\n",
        "        indices_absolutos = [int(indices[int(i)]) for i in range(len(train_X))]\n",
        "        client_datasets.append(Subset(train_dataset, indices_absolutos))\n",
        "    return client_datasets\n",
        "\n",
        "def FashionNonIID_filtrorandom(train_dataset, num_users, n_class=2, num_samples=3000, test_size=0.5, rate_unbalance=0):\n",
        "    num_classes = 10  # Total de classes para Fashion Mnist\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X, train_y = dividir_dados(train_X, train_y, test_size=test_size)\n",
        "        # Utiliza os índices após a divisão\n",
        "        indices_absolutos = [int(indices[int(i)]) for i in range(len(train_X))]\n",
        "        client_datasets.append(Subset(train_dataset, indices_absolutos))\n",
        "    return client_datasets\n",
        "\n",
        "def Cifar10NonIID_filtrorandom(train_dataset, num_users, n_class=2, num_samples=2500, test_size=0.5, rate_unbalance=0):\n",
        "    num_classes = 10  # Total de classes para Cifar10\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X, train_y = dividir_dados(train_X, train_y, test_size=test_size)\n",
        "        # Utiliza os índices após a divisão\n",
        "        indices_absolutos = [int(indices[int(i)]) for i in range(len(train_X))]\n",
        "        client_datasets.append(Subset(train_dataset, indices_absolutos))\n",
        "    return client_datasets\n",
        "\n",
        "def Cifar100NonIID_filtrorandom(train_dataset, num_users, n_class=20, num_samples=250, test_size=0.5, rate_unbalance=0):\n",
        "    num_classes = 100  # Total de classes para Cifar10\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        for cls in chosen_classes:\n",
        "            idxs = np.random.choice(class_indices[cls], num_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X, train_y = dividir_dados(train_X, train_y, test_size=test_size)\n",
        "        # Utiliza os índices após a divisão\n",
        "        indices_absolutos = [int(indices[int(i)]) for i in range(len(train_X))]\n",
        "        client_datasets.append(Subset(train_dataset, indices_absolutos))\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "\n",
        "# Parâmetros:\n",
        "# - num_users: Número de usuários/clientes.\n",
        "# - n_class: Número de classes por usuário.\n",
        "# - num_samples: Número base de amostras por classe para cada usuário.\n",
        "# - rate_unbalance: Fator de desequilíbrio na distribuição das amostras.\n",
        "# - Alto fator de desbalanceamento de quanitade de classes e dados (DADOS NON-IDD).\n",
        "def MNISTNonIIDunbalance_filtrorandom(train_dataset, num_users, n_class=2, num_samples=3000, rate_unbalance=1.2):\n",
        "    num_classes = 10  # Total de classes para MNIST\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X, train_y = dividir_dados(train_X, train_y, test_size=0.5)\n",
        "        # Utiliza os índices após a divisão\n",
        "        indices_absolutos = [int(indices[int(i)]) for i in range(len(train_X))]\n",
        "        client_datasets.append(Subset(train_dataset, indices_absolutos))\n",
        "    return client_datasets\n",
        "\n",
        "def FashionNonIIDunbalance_filtrorandom(train_dataset, num_users, n_class=2, num_samples=3000, rate_unbalance=1.2):\n",
        "    num_classes = 10  # Total de classes para Fashion MNIST\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X, train_y = dividir_dados(train_X, train_y, test_size=0.5)\n",
        "        # Utiliza os índices após a divisão\n",
        "        indices_absolutos = [int(indices[int(i)]) for i in range(len(train_X))]\n",
        "        client_datasets.append(Subset(train_dataset, indices_absolutos))\n",
        "    return client_datasets\n",
        "\n",
        "def Cifar10NonIIDunbalance_filtrorandom(train_dataset, num_users, n_class=2, num_samples=2500, rate_unbalance=1.2):\n",
        "    num_classes = 10  # Total de classes para Cifar10\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X, train_y = dividir_dados(train_X, train_y, test_size=0.5)\n",
        "        # Utiliza os índices após a divisão\n",
        "        indices_absolutos = [int(indices[int(i)]) for i in range(len(train_X))]\n",
        "        client_datasets.append(Subset(train_dataset, indices_absolutos))\n",
        "    return client_datasets\n",
        "\n",
        "def Cifar100NonIIDunbalance_filtrorandom(train_dataset, num_users, n_class=20, num_samples=250, rate_unbalance=1.2):\n",
        "    num_classes = 100  # Total de classes para Cifar100\n",
        "    labels = np.array(train_dataset.targets)\n",
        "    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    users_dict_train = {i: np.array([]) for i in range(num_users)}\n",
        "    for user in range(num_users):\n",
        "        chosen_classes = np.random.choice(range(num_classes), n_class, replace=False)\n",
        "        user_samples = int(num_samples * (rate_unbalance ** user))\n",
        "        for cls in chosen_classes:\n",
        "            available_samples = min(user_samples, len(class_indices[cls]))\n",
        "            idxs = np.random.choice(class_indices[cls], available_samples, replace=False)\n",
        "            users_dict_train[user] = np.concatenate((users_dict_train[user], idxs), axis=0)\n",
        "    client_datasets = []\n",
        "    for user, indices in users_dict_train.items():\n",
        "        train_X = np.array([train_dataset.data[int(i)] for i in indices])\n",
        "        train_y = np.array([train_dataset.targets[int(i)] for i in indices])\n",
        "        train_X, train_y = dividir_dados(train_X, train_y, test_size=0.5)\n",
        "        # Utiliza os índices após a divisão\n",
        "        indices_absolutos = [int(indices[int(i)]) for i in range(len(train_X))]\n",
        "        client_datasets.append(Subset(train_dataset, indices_absolutos))\n",
        "    return client_datasets"
      ],
      "metadata": {
        "id": "qAxOh0H0Y68-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FED-LEARNING FUNCTIONS\n",
        "\n",
        "Collection of functions aimed at federated learning, covering global model, performance evaluation, training, and validation processes.\n",
        "\n",
        "*   **Data Preparation (prepare_data):** This function automates the preparation of training and test datasets for MNIST, FashionMNIST, CIFAR-10, and CIFAR-100 with appropriate transformations. For CIFAR datasets, it includes data augmentation to improve model generalization.\n",
        "\n",
        "*   **Model Selection (select_model):** Acts as a model factory, selecting and returning a specific convolutional neural network architecture based on the provided dataset name.\n",
        "\n",
        "*   **Client Training (train_client):** Performs training of a model on the client dataset. Computes loss and accuracy, adjusting the model with local data.\n",
        "\n",
        "*   **Model Aggregation (aggregate_models):** Implements Federated Learning aggregation logic, where model parameters from multiple clients are averaged to update the global model.\n",
        "\n",
        "*   **Model Testing (test_model):** Evaluates the global model on the test dataset, computing loss and accuracy, and collecting predictions and true labels for further analysis.\n",
        "\n",
        "*   **Metrics Plotting (plot_metrics, plot_confusion_matrix, plot_client_metrics, plot_all_client_metrics):** Functions dedicated to visualizing performance metrics such as loss and accuracy over epochs, as well as providing confusion matrix visualizations for detailed analysis of model performance."
      ],
      "metadata": {
        "id": "H7tA9qGicI4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JiGbl4bMiHy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import seaborn as sns\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import json\n",
        "from torchvision import models\n",
        "\n",
        "# Neural Networks\n",
        "# CNN/MNIST\n",
        "class CNN_Mnist(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_Mnist, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "# CNN/FashionMNIST\n",
        "class CNN_FashionMnist(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_FashionMnist, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "#CNN/CIFAR-10\n",
        "class CNNCifar10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNCifar10, self).__init__()\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.resnet.bn1 = nn.BatchNorm2d(64)\n",
        "        self.resnet.maxpool = nn.Identity()\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 10),  # CIFAR-10 tem 10 classes\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "#CNN/Cifar100\n",
        "class CNNCifar100(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNCifar100, self).__init__()\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.resnet.bn1 = nn.BatchNorm2d(64)\n",
        "        self.resnet.maxpool = nn.Identity()\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 100),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "def save_experiment(save_name, precision, recall, f1_score, accuracy, loss, client_samples, total_samples, execution_time_per_client, total_execution_time):\n",
        "    experiment_data = {\n",
        "        \"Global Model Metrics\": {\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1-Score\": f1_score,\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"Loss\": loss\n",
        "        },\n",
        "        \"Execution Information\": {\n",
        "            \"Client Samples\": client_samples,\n",
        "            \"Total Sample Count\": total_samples,\n",
        "            \"Execution Time per Client (seconds)\": execution_time_per_client,\n",
        "            \"Total Execution Time (seconds)\": total_execution_time\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Salvando os dados em um arquivo JSON\n",
        "    with open(f\"{save_name}.json\", 'w') as file:\n",
        "        json.dump(experiment_data, file, indent=4)\n",
        "    print(f\"\\nWarning, the main results were saved in a file .json and can be accessed.\")\n",
        "    print(f\"Experiment '{save_name}' saved successfully.\")\n",
        "\n",
        "def prepare_data(dataset_name, num_clients, test_batch_size, disproportion=False, distribution='idd', filter='no'):\n",
        "    if distribution == 'idd' and filter != 'no':\n",
        "       print(\"The following functions will be stopped to prevent any errors. Please define the variables correctly.\")\n",
        "       print(\"Following idd filter are allowed: 'no'.\")\n",
        "       sys.exit()\n",
        "\n",
        "    if distribution=='idd':\n",
        "      # Define transformações comuns, prepração de dados e etc\n",
        "      normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "      if dataset_name == \"CIFAR10\" or dataset_name == \"CIFAR100\":\n",
        "          # Data augmentation para CIFAR-10 e CIFAR-100\n",
        "          transform_train = transforms.Compose([\n",
        "              transforms.RandomHorizontalFlip(),\n",
        "              transforms.RandomRotation(15),\n",
        "              transforms.RandomCrop(32, padding=4),\n",
        "              transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "              transforms.ToTensor(),\n",
        "              normalize,\n",
        "          ])\n",
        "      elif dataset_name == \"MNIST\" or dataset_name == \"FashionMNIST\":\n",
        "          # Transformações padrão para MNIST e FashionMNIST com normalização específica\n",
        "          normalize_mnist = transforms.Normalize((0.5,), (0.5,))\n",
        "          transform_train = transforms.Compose([\n",
        "              transforms.ToTensor(),\n",
        "              normalize_mnist,  # Normalização específica para MNIST e FashionMNIST\n",
        "          ])\n",
        "      else:\n",
        "          # Transformações padrão para outros datasets\n",
        "          transform_train = transforms.Compose([\n",
        "              transforms.ToTensor(),\n",
        "              normalize,  # Normalização geral\n",
        "          ])\n",
        "      # Transformação para o conjunto de teste (sem data augmentation)\n",
        "      transform_test = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          normalize if dataset_name not in [\"MNIST\", \"FashionMNIST\"] else normalize_mnist,\n",
        "      ])\n",
        "      # Seleciona a classe do dataset com base no nome\n",
        "      dataset_class = {\n",
        "          \"MNIST\": datasets.MNIST,\n",
        "          \"FashionMNIST\": datasets.FashionMNIST,\n",
        "          \"CIFAR10\": datasets.CIFAR10,\n",
        "          \"CIFAR100\": datasets.CIFAR100,\n",
        "      }.get(dataset_name, datasets.MNIST)  # MNIST é o dataset padrão\n",
        "      # Carrega os datasets de treino e teste com as transformações aplicadas\n",
        "      train_dataset = dataset_class(root='./data', train=True, download=True, transform=transform_train)\n",
        "      test_dataset = dataset_class(root='./data', train=False, download=True, transform=transform_test)\n",
        "      # Divide o dataset de treino entre os clientes\n",
        "      client_datasets = [Subset(train_dataset, indices.tolist()) for indices in np.array_split(range(len(train_dataset)), num_clients)]\n",
        "      return client_datasets, DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
        "    elif distribution=='non-idd':\n",
        "      normalize_values = {\n",
        "          \"MNIST\": transforms.Normalize((0.1307,), (0.3081,)),\n",
        "          \"FashionMNIST\": transforms.Normalize((0.2860,), (0.3530,)),\n",
        "          \"CIFAR10\": transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "          \"CIFAR100\": transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "      }\n",
        "      normalize = normalize_values.get(dataset_name, transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
        "      transform_train = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          normalize,\n",
        "      ])\n",
        "      transform_test = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          normalize,\n",
        "      ])\n",
        "      dataset_class = {\n",
        "          \"MNIST\": datasets.MNIST,\n",
        "          \"FashionMNIST\": datasets.FashionMNIST,\n",
        "          \"CIFAR10\": datasets.CIFAR10,\n",
        "          \"CIFAR100\": datasets.CIFAR100,\n",
        "      }.get(dataset_name, datasets.MNIST)\n",
        "\n",
        "      if filter == 'no':\n",
        "        if dataset_name == \"MNIST\":\n",
        "            client_datasets = MNISTNonIIDunbalance(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                          num_clients) if disproportion else MNISTNonIID(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                          num_clients)\n",
        "        elif dataset_name == \"FashionMNIST\":\n",
        "            client_datasets = FashionMNISTNonIIDunbalance(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                                num_clients) if disproportion else FashionMNISTNonIID(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                                num_clients)\n",
        "        elif dataset_name == \"CIFAR10\":\n",
        "            client_datasets = Cifar10NonIIDunbalance(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients) if disproportion else Cifar10NonIID(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients)\n",
        "        elif dataset_name == \"CIFAR100\":\n",
        "            client_datasets = Cifar100NonIIDunbalance(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients) if disproportion else Cifar100NonIID(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients)\n",
        "        else:\n",
        "            # Se o nome do dataset não for reconhecido, usa MNIST como padrão\n",
        "            client_datasets = MNISTNonIID(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                          num_clients)\n",
        "      elif filter == 'entropy':\n",
        "        if dataset_name == \"MNIST\":\n",
        "            client_datasets = MNISTNonIIDunbalance_filtrado(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                          num_clients) if disproportion else MNISTNonIID_filtrado(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                          num_clients)\n",
        "        elif dataset_name == \"FashionMNIST\":\n",
        "            client_datasets = FashionMNISTNonIIDunbalance_filtrado(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                                num_clients) if disproportion else FashionMNISTNonIID_filtrado(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                                num_clients)\n",
        "        elif dataset_name == \"CIFAR10\":\n",
        "            client_datasets = Cifar10NonIIDunbalance_filtrado(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients) if disproportion else Cifar10NonIID_filtrado(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients)\n",
        "        elif dataset_name == \"CIFAR100\":\n",
        "            client_datasets = Cifar100NonIIDunbalance_filtrado(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients) if disproportion else Cifar100NonIID_filtrado(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients)\n",
        "        else:\n",
        "            # Se o nome do dataset não for reconhecido, usa MNIST como padrão\n",
        "            client_datasets = MNISTNonIIDunbalance_filtrado(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                          num_clients)\n",
        "      elif filter == 'random':\n",
        "        if dataset_name == \"MNIST\":\n",
        "            client_datasets = MNISTNonIIDunbalance_filtrorandom(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                          num_clients) if disproportion else MNISTNonIID_filtrorandom(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                          num_clients)\n",
        "        elif dataset_name == \"FashionMNIST\":\n",
        "            client_datasets = FashionNonIIDunbalance_filtrorandom(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                                num_clients) if disproportion else FashionNonIID_filtrorandom(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                                num_clients)\n",
        "        elif dataset_name == \"CIFAR10\":\n",
        "            client_datasets = Cifar10NonIIDunbalance_filtrorandom(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients) if disproportion else Cifar10NonIID_filtrorandom(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients)\n",
        "        elif dataset_name == \"CIFAR100\":\n",
        "            client_datasets = Cifar100NonIIDunbalance_filtrorandom(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients) if disproportion else Cifar100NonIID_filtrorandom(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                            num_clients)\n",
        "        else:\n",
        "            # Se o nome do dataset não for reconhecido, usa MNIST como padrão\n",
        "            client_datasets = MNISTNonIIDunbalance_filtrorandom(dataset_class(root='./data', train=True, download=True, transform=transform_train),\n",
        "                                          num_clients)\n",
        "      else:\n",
        "          print(\"Following filters are allowed: 'random', 'entropy', 'no'.\")\n",
        "          sys.exit()\n",
        "      test_dataset = dataset_class(root='./data', train=False, download=True, transform=transform_test)\n",
        "      return client_datasets, DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
        "    else:\n",
        "      print(\"Following distributions are allowed: 'non-idd', 'idd'.\")\n",
        "      sys.exit()\n",
        "\n",
        "def select_model(dataset_name):\n",
        "    if dataset_name == \"FashionMNIST\":\n",
        "        return CNN_FashionMnist()\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        return CNNCifar10()\n",
        "    elif dataset_name == \"CIFAR100\":\n",
        "        return CNNCifar100()\n",
        "    else:  # Padrão para MNIST\n",
        "        return CNN_Mnist()\n",
        "\n",
        "def train_client(model, device, train_loader, optimizer, loss_function, epoch):\n",
        "    # Coloca o modelo em modo de treinamento, habilitando gradientes e regularização\n",
        "    print('\\nStarting training round for the model...')\n",
        "    model.train()\n",
        "    # Inicializa contadores para a perda de treinamento, número de previsões corretas e total de amostras processadas.\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    # Itera sobre o carregador de dados do cliente, processando os dados em lotes.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_function(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total += target.size(0)\n",
        "        if batch_idx % 10 == 0:\n",
        "                total_samples = len(train_loader.dataset)\n",
        "                num_digits = len(str(total_samples))\n",
        "                loader_str = str(train_loader).split(' ')[-1][:-1]  # Obtém o endereço do objeto DataLoader/Client/worker\n",
        "                #Print do treinamento dos clients.\n",
        "                print(f'Client <{loader_str}> Train Epoch: {epoch} [{batch_idx * len(data):0{num_digits}d}/{total_samples} ({100. * batch_idx / len(train_loader):02.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "    accuracy = 100. * correct / total\n",
        "    # Retorna a perda média de treinamento por amostra e a acurácia.\n",
        "    return train_loss / len(train_loader.dataset), accuracy\n",
        "\n",
        "def train_client_fedprox(model_local, model_global, device, train_loader, optimizer, epoch, mu=0.01):\n",
        "    #Treina o modelo local de um cliente usando o algoritmo FedProx.\n",
        "    print('\\nStarting training round for the model...')\n",
        "    model_local.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # Cálculo da perda incluindo o termo de proximidade\n",
        "        output = model_local(data)\n",
        "        classification_loss = F.cross_entropy(output, target)\n",
        "        prox_term = 0.0\n",
        "        for param_local, param_global in zip(model_local.parameters(), model_global.parameters()):\n",
        "            prox_term += (param_local - param_global).pow(2).sum()\n",
        "        prox_term *= mu\n",
        "        loss = classification_loss + prox_term\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # Obtém o índice da classe com a maior probabilidade\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()  # Conta quantas previsões foram corretas\n",
        "        total += target.size(0)\n",
        "        if batch_idx % 10 == 0:\n",
        "          total_samples = len(train_loader.dataset)\n",
        "          num_digits = len(str(total_samples))\n",
        "          loader_str = str(train_loader).split(' ')[-1][:-1]  # Obtém o endereço do objeto DataLoader/Client/worker\n",
        "          #Print do treinamento dos clients.\n",
        "          print(f'Client <{loader_str}> Train Epoch: {epoch} [{batch_idx * len(data):0{num_digits}d}/{total_samples} ({100. * batch_idx / len(train_loader):02.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "    accuracy = 100. * correct / total\n",
        "    return train_loss / total, accuracy\n",
        "\n",
        "def aggregate_models(models):\n",
        "    #Modelo aggregate_models, agrega os modelos dos clientes para atualizar o modelo global.\n",
        "    global_model = models[0]\n",
        "    print('______________________________________________________________________________')\n",
        "    print('Aggregation Step: ____________________________________________________________')\n",
        "    with torch.no_grad():\n",
        "        for key in global_model.state_dict().keys():\n",
        "            if global_model.state_dict()[key].dtype == torch.float32:\n",
        "                # Agrega os tensores, pessos e bias\n",
        "                tensors_to_average = [model.state_dict()[key] for model in models]\n",
        "                # Calcula os valores médio, mínimo e máximo para cada parâmetro em todos os modelos\n",
        "                mean_values = torch.mean(torch.stack(tensors_to_average), dim=0).cpu().numpy().mean()\n",
        "                min_values = torch.min(torch.stack(tensors_to_average), dim=0).values.cpu().numpy().min()\n",
        "                max_values = torch.max(torch.stack(tensors_to_average), dim=0).values.cpu().numpy().max()\n",
        "                # Imprimir estatísticas resumidas antes da agregação\n",
        "                print(f\"\\nParameters for '{key}' aggregation:\")\n",
        "                print(f\"Mean: {mean_values:.4f}, Min: {min_values:.4f}, Max: {max_values:.4f}\")\n",
        "                averaged_tensor = torch.mean(torch.stack(tensors_to_average), dim=0)\n",
        "                global_model.state_dict()[key].copy_(averaged_tensor)\n",
        "    return global_model\n",
        "\n",
        "def test_model(model, device, test_loader, loss_function):\n",
        "    # Coloca o modelo em modo de avaliação, desabilitando layers específicos como dropout e batch normalization.\n",
        "    # Os clientes usam a função test_model para avaliar o desempenho de seus modelos locais em seus próprios dados de teste.\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    targets_list, preds_list = [], []  # Inicializa listas para coletar targets e preds\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += loss_function(output, target).item() * data.size(0)\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Predições do modelo\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            targets_list.extend(target.view_as(pred).cpu().numpy())  # Coleta os rótulos verdadeiros\n",
        "            preds_list.extend(pred.cpu().numpy())  # Coleta as previsões\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)')\n",
        "    print('______________________________________________________________________________')\n",
        "    return test_loss, accuracy, targets_list, preds_list  # Retorna também targets e preds\n",
        "\n",
        "def plot_metrics(train_losses, test_losses, accuracies):\n",
        "    #Plote das métricas de treino e teste ao longo das épocas.\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs, test_losses, 'ro-', label='Test Loss')\n",
        "    plt.title('Training and Test Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracies, 'go-', label='Test Accuracy')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(labels, predictions):\n",
        "    #Plote da matriz de confusão\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=[str(i) for i in range(10)], yticklabels=[str(i) for i in range(10)])\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def plot_client_metrics(client_losses, client_accuracies, client_index):\n",
        "    #Plot para laço dos clients, apresenta plot de loss e acuracy de cada client.\n",
        "    epochs = range(1, len(client_losses) + 1)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, client_losses, 'bo-', label=f'Client {client_index} Training Loss')\n",
        "    plt.title(f'Client {client_index} Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, client_accuracies, 'go-', label=f'Client {client_index} Accuracy')\n",
        "    plt.title(f'Client {client_index} Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_all_client_metrics(client_losses, client_accuracies):\n",
        "    # Determina o número de épocas a partir do comprimento dos dados do primeiro cliente\n",
        "    epochs = range(1, len(client_losses[0]) + 1)\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    # Plot de perdas de treinamento de todos os clientes\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for i, losses in enumerate(client_losses):\n",
        "        plt.plot(epochs, losses, label=f'Client {i+1} Loss')\n",
        "    plt.title('Training Loss by Client')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    # Plot de acurácias de todos os clientes\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i, accuracies in enumerate(client_accuracies):\n",
        "        plt.plot(epochs, accuracies, label=f'Client {i+1} Accuracy')\n",
        "    plt.title('Accuracy by Client')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main(dataset_name='MNIST', num_epochs=10, num_clients=5, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'idd', fedModel='FedAvg', mu = 0.01, filter='no', save_experiment_results = 'experiment_results'):\n",
        "    # Orquestrador do ciclo completo de treinamento e avaliação em um cenário de aprendizado federado.\n",
        "    # A função coordena a preparação dos dados, o treinamento dos modelos locais nos clientes e a\n",
        "    # agregação dos modelos para formar um modelo global e a avaliação do modelo global.\n",
        "\n",
        "    # Declaração de variaveis para armazenamento de informações dados e instanciamento das redes neurais, clients etc.\n",
        "    start_time = time.time()  # Inicia a medição do tempo\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    client_datasets, test_loader = prepare_data(dataset_name, num_clients, test_batch_size, disproportion=disproportion, distribution=distribution, filter=filter)\n",
        "    global_model = select_model(dataset_name).to(device)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    train_losses, test_losses, accuracies = [], [], []\n",
        "    all_labels, all_predictions = [], []  # Inicializa listas para coletar todos os rótulos e previsões\n",
        "    client_sample_counts = [len(client_dataset) for client_dataset in client_datasets]  # Contagem de amostras por cliente\n",
        "    total_samples = sum(client_sample_counts)  # Contagem total de amostras\n",
        "    client_times = []\n",
        "    client_losses = [[] for _ in range(num_clients)]\n",
        "    client_accuracies = [[] for _ in range(num_clients)]\n",
        "\n",
        "    # Processo de treinamento distribuído em um cenário de aprendizado federado, iterando por um número definido de épocas.\n",
        "    # A cada época, todos os clientes treinam localmente uma cópia do modelo global com seus próprios dados.\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        models, epoch_train_loss = [], 0\n",
        "        for client_index, client_dataset in enumerate(client_datasets):\n",
        "            client_start_time = time.time()  # Tempo inicial do cliente\n",
        "            train_loader = DataLoader(client_dataset, batch_size=batch_size, shuffle=True)\n",
        "            model_copy = type(global_model)().to(device)\n",
        "            model_copy.load_state_dict(global_model.state_dict())\n",
        "            optimizer = optim.SGD(model_copy.parameters(), lr=learning_rate, momentum=momentum)\n",
        "            if fedModel == 'FedProx':\n",
        "                mu = 0.01 if mu is None else mu\n",
        "                train_loss, accuracy = train_client_fedprox(model_copy, global_model, device, train_loader, optimizer, epoch, mu)\n",
        "            elif fedModel == 'FedAvg':\n",
        "                train_loss, accuracy = train_client(model_copy, device, train_loader, optimizer, loss_function, epoch)\n",
        "            else:\n",
        "              print(\"Fedmodel allowed: 'FedProx', 'FedAvg'.\")\n",
        "            epoch_train_loss += train_loss\n",
        "            models.append(model_copy)\n",
        "            client_losses[client_index].append(train_loss)\n",
        "            client_accuracies[client_index].append(accuracy)\n",
        "            client_end_time = time.time()  # Tempo final do cliente\n",
        "            client_times.append(client_end_time - client_start_time)  # Calcula o tempo de execução do cliente\n",
        "        global_model = aggregate_models(models)\n",
        "        train_losses.append(epoch_train_loss / len(client_datasets))\n",
        "\n",
        "        # Chama a função test_model modificada para também retornar rótulos e previsões\n",
        "        test_loss, accuracy, labels, predictions = test_model(global_model, device, test_loader, loss_function)\n",
        "        test_losses.append(test_loss)\n",
        "        accuracies.append(accuracy)\n",
        "        # Concatena os rótulos e previsões de todas as épocas para plotar a matriz de confusão ao final\n",
        "        all_labels.extend(labels)\n",
        "        all_predictions.extend(predictions)\n",
        "\n",
        "    # Plota as métricas de treinamento e teste\n",
        "    print(\"Global Model Metrics _________________________________________________________:\")\n",
        "    # Converte listas de rótulos e predições para arrays numpy para cálculo\n",
        "    labels_np = np.array(all_labels)\n",
        "    predictions_np = np.array(all_predictions)\n",
        "    # Calcula precisão, recall, F1-score e suporte para cada classe\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(labels_np, predictions_np, average='weighted')\n",
        "    # Acurácia e Loss já foram calculados e armazenados nas listas `accuracies` e `test_losses`\n",
        "    accuracy_final = accuracies[-1]  # Último valor da lista de acurácias\n",
        "    loss_final = test_losses[-1]  # Último valor da lista de perdas\n",
        "    # Exibe os resultados\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1_score:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy_final:.4f}%\")\n",
        "    print(f\"Loss: {loss_final:.4f} \\n\")\n",
        "    plot_metrics(train_losses, test_losses, accuracies)\n",
        "    # Plota a matriz de confusão usando os rótulos e previsões coletados\n",
        "    print(\"\\nGlobal Model Performance on Test Set Across All Epochs___________________________:\")\n",
        "    plot_confusion_matrix(all_labels, all_predictions)\n",
        "    end_time = time.time()  # Tempo final total\n",
        "    total_time = end_time - start_time  # Calcula o tempo total de execução\n",
        "\n",
        "    #Printa das informações gerais dos clientes\n",
        "    print(\"\\nExecution information ___________________________________________________________:\")\n",
        "    for i, count in enumerate(client_sample_counts):\n",
        "        print(f\"Client {i+1}: {count} samples\")\n",
        "    print(f\"Total sample count: {total_samples}\")\n",
        "    print(f\"Execution time per client (seconds): {client_times}\")\n",
        "    print(f\"Total execution time (seconds): {total_time}\")\n",
        "\n",
        "    print(\"\\nClient Details ___________________________________________________________:\\n\")\n",
        "    plot_all_client_metrics(client_losses, client_accuracies)\n",
        "    print(\"\\nIndividual Client Details ___________________________________:\\n\")\n",
        "    for i in range(num_clients):\n",
        "        plot_client_metrics(client_losses[i], client_accuracies[i], i+1)\n",
        "\n",
        "    client_samples_info = {f\"Client {i+1}\": count for i, count in enumerate(client_sample_counts)}\n",
        "    execution_time_per_client_info = {f\"Client {i+1}\": time for i, time in enumerate(client_times)}\n",
        "    save_experiment(\n",
        "        save_name=save_experiment_results,  # Nome do arquivo de salvamento\n",
        "        precision=precision,  # Valor da precisão\n",
        "        recall=recall,  # Valor do recall\n",
        "        f1_score=f1_score,  # Valor do F1-Score\n",
        "        accuracy=accuracy_final,  # Valor da acurácia\n",
        "        loss=loss_final,  # Valor da perda\n",
        "        client_samples=client_samples_info,  # Lista com a contagem de amostras por cliente\n",
        "        total_samples=total_samples,  # Contagem total de amostras\n",
        "        execution_time_per_client=execution_time_per_client_info,  # Lista com o tempo de execução por cliente\n",
        "        total_execution_time=total_time  # Tempo total de execução\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING\n",
        "\n",
        "This class inherits from nn.Module, which forms the foundation for all neural network modules in PyTorch, specifically adapted to handle images. Thus, the main function performs model training within a federated learning context, optimizing its performance for visual data analysis and prediction.\n",
        "\n",
        "**Class (nn.Module):**\n",
        "*   Contains the architecture of the CNN used in training, which is sent to the dataloaders/workers/clients.\n",
        "\n",
        "**Parameters of main:**\n",
        "*   **dataset_name=:** Specifies that the MNIST dataset will be used for training.\n",
        "*   **num_epochs:** Defines the number of training epochs.\n",
        "*   **num_clients:** Configures the number of clients (or \"training nodes\").\n",
        "*   **batch_size:** Establishes the batch size.\n",
        "*   **learning_rate= and momentum=:** Defines the learning rate and momentum for the SGD optimizer.\n",
        "*   **test_batch_size=:** Defines the batch size for evaluation on the test set.\n",
        "*   **disproportion=:** Defines the imbalance among all participants; datasets may receive classes with different amounts of data.\n",
        "*   **distribution=:** Defines whether the test will be conducted with 'iid' or 'non-iid' configuration.\n",
        "*   if __name__ == \"__main__\": ensures that the training starts, allowing for modularization. This structure also automatically initiates the federated learning environment setup and creation of clients in DataLoaders, organizing the training sequence and managing updates to the global model.\n"
      ],
      "metadata": {
        "id": "7Wbmb6Cpco9j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwlyom0BJJd0"
      },
      "source": [
        "#### Fed-Learning [MNIST]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN para MNIST\n",
        "class CNN_Mnist(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_Mnist, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # If True/False, each client will have highly unbalanced datasets (unbalanced = True).\n",
        "  # num_epochs defines the number of epochs for the models to train.\n",
        "  # num_clients defines the number of clients.\n",
        "  # batch_size defines the batch size for each client for training.\n",
        "  # learning_rate defines the learning rate.\n",
        "  # momentum defines the technique that controls the speed and direction of updates.\n",
        "  # test_batch_size defines the batch size for testing the global model.\n",
        "  # disproportion defines the high level of client data/class imbalance.\n",
        "  # distribution defined as 'idd' or 'non-idd' for testing purposes.\n",
        "  # fedModeledModel='', selects between FedAvg and FedProx for global model aggregation purposes.\n",
        "  # Mu='', defines for FedProx the strength of penalty applied to weight deviations.\n",
        "  # filter='', defines as 'yes' or 'no' for entropy filter application.\n",
        "  # save_experiment_results = # define a name for the results file to be saved in .json format\n",
        "    try:\n",
        "        #---- Test do modelo FedAvg\n",
        "        # Test Completo\n",
        "        #main(dataset_name='MNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='MNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='MNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #---- Test do modelo FedProx\n",
        "        #main(dataset_name='MNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='MNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='MNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #-----\n",
        "        # Test Rapido\n",
        "        #main(dataset_name='MNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='MNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='MNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='MNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='MNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='MNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        pass\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted.\")"
      ],
      "metadata": {
        "id": "tI9K4Xc5QHpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gngljOS1MZS0"
      },
      "source": [
        "#### Fed-Learning [Fashion Mnist]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN para FashionMNIST\n",
        "class CNN_FashionMnist(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_FashionMnist, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # If True/False, each client will have highly unbalanced datasets (unbalanced = True).\n",
        "  # num_epochs defines the number of epochs for the models to train.\n",
        "  # num_clients defines the number of clients.\n",
        "  # batch_size defines the batch size for each client for training.\n",
        "  # learning_rate defines the learning rate.\n",
        "  # momentum defines the technique that controls the speed and direction of updates.\n",
        "  # test_batch_size defines the batch size for testing the global model.\n",
        "  # disproportion defines the high level of client data/class imbalance.\n",
        "  # distribution defined as 'idd' or 'non-idd' for testing purposes.\n",
        "  # fedModeledModel='', selects between FedAvg and FedProx for global model aggregation purposes.\n",
        "  # Mu='', defines for FedProx the strength of penalty applied to weight deviations.\n",
        "  # filter='', defines as 'yes' or 'no' for entropy filter application.\n",
        "  # save_experiment_results = # define a name for the results file to be saved in .json format\n",
        "    try:\n",
        "        #---- Test do modelo FedAvg\n",
        "        # Test Completo\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #----- Test do modelo FedProx\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "\n",
        "        #-----\n",
        "        # Test Rapido\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='FashionMNIST', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        pass\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted.\")"
      ],
      "metadata": {
        "id": "ZVdlpQIUKBNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yi01PEHLAyu"
      },
      "source": [
        "#### Fed-Learning [CIFAR-10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN/CIFAR-10\n",
        "class CNNCifar10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNCifar10, self).__init__()\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.resnet.bn1 = nn.BatchNorm2d(64)\n",
        "        self.resnet.maxpool = nn.Identity()\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 10),  # CIFAR-10 tem 10 classes\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # If True/False, each client will have highly unbalanced datasets (unbalanced = True).\n",
        "    # num_epochs defines the number of epochs for the models to train.\n",
        "    # num_clients defines the number of clients.\n",
        "    # batch_size defines the batch size for each client for training.\n",
        "    # learning_rate defines the learning rate.\n",
        "    # momentum defines the technique that controls the speed and direction of updates.\n",
        "    # test_batch_size defines the batch size for testing the global model.\n",
        "    # disproportion defines the high level of client data/class imbalance.\n",
        "    # distribution defined as 'idd' or 'non-idd' for testing purposes.\n",
        "    # fedModeledModel='', selects between FedAvg and FedProx for global model aggregation purposes.\n",
        "    # Mu='', defines for FedProx the strength of penalty applied to weight deviations.\n",
        "    # filter='', defines as 'yes' or 'no' for entropy filter application.\n",
        "    # save_experiment_results = # define a name for the results file to be saved in .json format\n",
        "    try:\n",
        "        #main(dataset_name='CIFAR10', num_epochs=50, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='CIFAR10_TRUENONIDD_FEDAVG_FILTERNO_10')\n",
        "        # Test Completo\n",
        "        #main(dataset_name='CIFAR10', num_epochs=50, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='CIFAR10', num_epochs=50, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='CIFAR10', num_epochs=50, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #---- Test do modelo FedProx\n",
        "        #main(dataset_name='CIFAR10', num_epochs=50, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='CIFAR10', num_epochs=50, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='CIFAR10', num_epochs=50, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #-----\n",
        "        # Test Rapido\n",
        "        #main(dataset_name='CIFAR10', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='CIFAR10', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='CIFAR10', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='CIFAR10', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='CIFAR10', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        #main(dataset_name='CIFAR10', num_epochs=1, num_clients=3, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "        pass\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted.\")"
      ],
      "metadata": {
        "id": "b2yesyo6Pptp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywDvKdLRM7F8"
      },
      "source": [
        "#### Fed-Learning [CIFAR-100]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da classe CNNCifar100\n",
        "class CNNCifar100(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNCifar100, self).__init__()\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.resnet.bn1 = nn.BatchNorm2d(64)\n",
        "        self.resnet.maxpool = nn.Identity()\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 100),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # If True/False, each client will have highly unbalanced datasets (unbalanced = True).\n",
        "    # num_epochs defines the number of epochs for the models to train.\n",
        "    # num_clients defines the number of clients.\n",
        "    # batch_size defines the batch size for each client for training.\n",
        "    # learning_rate defines the learning rate.\n",
        "    # momentum defines the technique that controls the speed and direction of updates.\n",
        "    # test_batch_size defines the batch size for testing the global model.\n",
        "    # disproportion defines the high level of client data/class imbalance.\n",
        "    # distribution defined as 'idd' or 'non-idd' for testing purposes.\n",
        "    # fedModeledModel='', selects between FedAvg and FedProx for global model aggregation purposes.\n",
        "    # Mu='', defines for FedProx the strength of penalty applied to weight deviations.\n",
        "    # filter='', defines as 'yes' or 'no' for entropy filter application.\n",
        "    # save_experiment_results = # define a name for the results file to be saved in .json format\n",
        "    try:\n",
        "      #----- Test do modelo FedAvg\n",
        "      #main(dataset_name='CIFAR100', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "      #main(dataset_name='CIFAR100', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "      #main(dataset_name='CIFAR100', num_epochs=20, num_clients=5, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "      #----- Test do modelo FedProx\n",
        "      #main(dataset_name='CIFAR100', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "      #main(dataset_name='CIFAR100', num_epochs=20, num_clients=10, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "      #main(dataset_name='CIFAR100', num_epochs=20, num_clients=5, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "\n",
        "      #-----\n",
        "      # Test Rapido\n",
        "      #main(dataset_name='CIFAR100', num_epochs=1, num_clients=2, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedAvg', mu = None, filter='no', save_experiment_results='NameHere')\n",
        "      #main(dataset_name='CIFAR100', num_epochs=1, num_clients=2, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='random', save_experiment_results='NameHere')\n",
        "      #main(dataset_name='CIFAR100', num_epochs=1, num_clients=2, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedAvg', mu = None, filter='random', save_experiment_results='NameHere')\n",
        "      #main(dataset_name='CIFAR100', num_epochs=1, num_clients=2, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=None, distribution = 'idd', fedModel='FedProx', mu = 0.01, filter='no', save_experiment_results='NameHere')\n",
        "      #main(dataset_name='CIFAR100', num_epochs=1, num_clients=2, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=True, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='random', save_experiment_results='NameHere')\n",
        "      #main(dataset_name='CIFAR100', num_epochs=1, num_clients=2, batch_size=64, learning_rate=0.01, momentum=0.5, test_batch_size=1000, disproportion=False, distribution = 'non-idd', fedModel='FedProx', mu = 0.01, filter='random', save_experiment_results='NameHere')\n",
        "      pass\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted.\")"
      ],
      "metadata": {
        "id": "aV78P-h0KdKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "collapsed_sections": [
        "h7lPRfZBcC-m",
        "jIKlxBKMUj07",
        "sRhf1AU6Vb2H",
        "Xhtl8iISY4f3",
        "H7tA9qGicI4q",
        "qwlyom0BJJd0",
        "gngljOS1MZS0"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}