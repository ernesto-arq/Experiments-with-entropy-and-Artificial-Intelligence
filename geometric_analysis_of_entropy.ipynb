{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eR7UhRPCkRmn",
        "OjfQ8jgqoI2b",
        "OEqG6L-miwFr",
        "1xUiw9objtOj",
        "0VMGsKI2k8Nd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Description\n",
        "Experiments conducted through operations on a 2D array to analyze the behavior of entropy.\n",
        "\n",
        "Autor: Ernesto Gurgel Valente Neto"
      ],
      "metadata": {
        "id": "I4n_VMpFDnmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy Analysis [MNist]\n",
        "  1. Entropy Analysis from the perspective of\n",
        "    * Geometric Transformation; and\n",
        "    * Scale Transformation; and\n",
        "    * 2D to 1D Transformation (Flattening); and\n",
        "    * Dimensionality Reduction (PCA); and\n",
        "    * Training Data (1st Entropy => Flattening Scale Change => Training Data)\n",
        "    * Training Data (2nd Flattening Scale Change => Entropy => Training Data)\n",
        "    * Training Data (3rd Flattening Scale Change => PCA => Entropy => Training Data)\n"
      ],
      "metadata": {
        "id": "eR7UhRPCkRmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.image import flip_left_right\n",
        "from tensorflow.image import flip_up_down\n",
        "import numpy as np\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from itertools import islice\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from PIL import Image\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from scipy.stats import entropy\n",
        "\n",
        "def entropia(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    return -np.sum(pk * np.log(pk) / np.log(base))\n",
        "\n",
        "def indicesEntropiaMedian(train_X, train_Y, test_X, test_Y):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], 28, 28, 1))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_Y = to_categorical(train_Y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_Y = to_categorical(test_Y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "\n",
        "  tuplasEntropia = [(index, entropia(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  return indices_filtrados, entropiesLocal_ordenado\n",
        "\n",
        "def flip(img, direcao = 'horizontal'):\n",
        "  train_x3 = img.reshape((img.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  if (direcao == 'horizontal'):\n",
        "    train_x3_flip = [tf.image.flip_left_right(img) for img in train_x3]\n",
        "    return tf.stack(train_x3_flip, axis=0).numpy()\n",
        "  elif (direcao == 'vertical'):\n",
        "    train_x3_flip = [flip_up_down(img) for img in train_x3]\n",
        "    return tf.stack(train_x3_flip, axis=0).numpy()\n",
        "  else:\n",
        "    raise print(\"Direcao deve ser: (horizontal ou vertical).\")\n",
        "\n",
        "def analiseConjuntos(conjuntoA, ConjuntoB):\n",
        "  set1 = set(conjuntoA)\n",
        "  set2 = set(ConjuntoB)\n",
        "  elementosComuns = set1.intersection(set2)\n",
        "  total = len(conjuntoA)\n",
        "  elComum = len(elementosComuns)\n",
        "  porcen = round(((elComum/total)*100),2)\n",
        "  print(f\"Total de elementos em comum: {elComum}, porcentagem de elementos em comun: {porcen}%.\")\n",
        "\n",
        "def plotComparacaoConjuntos(image1, image2):\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  # Plota a primeira imagem\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(image1, cmap='gray')\n",
        "  plt.title('Imagem 1')\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(image2, cmap='gray')\n",
        "  plt.title('Imagem 2')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def plotarDadosTrain(index_inicio, qtd, train_X): # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
        "  base = 330\n",
        "  import matplotlib.pyplot as plt\n",
        "  for i in range(index_inicio, index_inicio + qtd):\n",
        "      plt.subplot(base + 1 + (i - index_inicio))\n",
        "      plt.imshow(train_X[i], cmap=plt.get_cmap('gray'))\n",
        "  plt.show()\n",
        "  print(train_y[index_inicio:index_inicio + qtd])\n",
        "\n",
        "def flip1(img, direcao = 'horizontal'):\n",
        "  train_x3 = img.reshape((img.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  if (direcao == 'horizontal'):\n",
        "    train_x3_flip = [tf.image.flip_left_right(img) for img in train_x3]\n",
        "    return tf.stack(train_x3_flip, axis=0).numpy()\n",
        "  elif (direcao == 'vertical'):\n",
        "    train_x3_flip = [flip_up_down(img) for img in train_x3]\n",
        "    return tf.stack(train_x3_flip, axis=0).numpy()\n",
        "  elif (direcao == '90graus'):\n",
        "    train_x3_flip = [tf.image.rot90(img, k=1) for img in train_x3]\n",
        "    return tf.stack(train_x3_flip, axis=0).numpy()\n",
        "  else:\n",
        "    raise print(\"Direcao deve ser: (horizontal, vertical, 90graus).\")\n",
        "\n",
        "# Transformação geometrica\n",
        "# Direcao deve ser: (horizontal, vertical, 90graus).\n",
        "def analiseGeometrica(train_X, train_Y, test_X, test_Y, n, coordenadas):\n",
        "  entropiaNormal, entropiesLocal_ordenadoNormal = indicesEntropiaMedian(train_X, train_Y, test_X, test_Y) # Calculando antes do conjunto ser invertido\n",
        "  backupOriginalTrainX = train_X\n",
        "  train_X = flip1(train_X, coordenadas)\n",
        "  backupAposFlip = train_X\n",
        "  entropiaFlip, entropiesLocal_ordenadoFlip = indicesEntropiaMedian(train_X, train_Y, test_X, test_Y) # Calculando a entropia apois conjunto ser invertido\n",
        "\n",
        "  #Comparando os dois conjuntos (valor será considerado igual de entropia caso contenha pelo menos 17 de 18 casas decimais iguais, ou seja 95% semelhante onde se considera o ultimo digito 18 arredondamento ou variação do ponto flutuante no calculo)\n",
        "  analiseConjuntos(entropiaFlip, entropiaNormal)\n",
        "  elDifH = 0\n",
        "  elEqH = 0\n",
        "  resultadosDif = []\n",
        "  for i, (x1H, x2H) in enumerate(zip(entropiaFlip, entropiaNormal)):\n",
        "    if np.array_equal(x1H, x2H):\n",
        "      elEqH += 1\n",
        "    else:\n",
        "      resultadosDif.append(i)\n",
        "      elDifH += 1\n",
        "  print(\"\\n\")\n",
        "  print(\"Print da Transformação da imagem\")\n",
        "  plotComparacaoConjuntos(backupOriginalTrainX[0], backupAposFlip[0])\n",
        "  print(f\"Total de elementos iguais: {elEqH}\")\n",
        "  print(\"\\n\")\n",
        "  analiseConjuntos(entropiaNormal, entropiaFlip) #Chamada da Funcao de Analise de se (conjunto A, esta Contido no Conjunto B)\n",
        "  for indice, (valor_normal, valor_flip) in islice(enumerate(zip(entropiaNormal, entropiaFlip)), n):\n",
        "      print(f\"Índice: {indice}, Entropia (Normal): {valor_normal}, Entropia (Flip): {valor_flip}\")\n",
        "  print(\"\\n\")\n",
        "  for indice, (valor_normal_pares, valor_flip_pares) in islice(enumerate(zip(entropiesLocal_ordenadoNormal, entropiesLocal_ordenadoFlip)), n):\n",
        "      print(f\"Índice: {indice}, Entropia (Chave | Valor (Normal)): {valor_normal_pares}, Entropia (Chave | Valor (Flip): {valor_flip_pares}\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(f\"Total de Elementos diferentes: {elDifH}: \")\n",
        "  for idx, valor in enumerate(resultadosDif):\n",
        "    print(f\"Indice {idx}: \", \" Entropia (Chave | Valor (Normal)): \", entropiesLocal_ordenadoNormal[valor], \", Entropia (Chave | Valor (Flip): \", entropiesLocal_ordenadoFlip[valor])\n",
        "    plotComparacaoConjuntos(backupOriginalTrainX[entropiaNormal[valor]], backupAposFlip[entropiaNormal[valor]])"
      ],
      "metadata": {
        "id": "hKTAAMcZ-AYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (test_x, test_y) = mnist.load_data() #Pegar o dataset\n",
        "plotarDadosTrain(1, 9, train_x)"
      ],
      "metadata": {
        "id": "5tP7CcM5kKGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##    * Geometric Transformation;"
      ],
      "metadata": {
        "id": "cKMnE4brAKlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_xHV, train_yHV), (test_xHV, test_yHV) = mnist.load_data() #Pegar o dataset\n",
        "\n",
        "analiseGeometrica(train_xHV, train_yHV, test_xHV, test_yHV, 3, \"horizontal\")"
      ],
      "metadata": {
        "id": "LZtM6E9ys4zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analiseGeometrica(train_xHV, train_yHV, test_xHV, test_yHV, 3, \"vertical\")"
      ],
      "metadata": {
        "id": "W6VQuEGG-xjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analiseGeometrica(train_xHV, train_yHV, test_xHV, test_yHV, 3, \"90graus\")"
      ],
      "metadata": {
        "id": "faBi7DUz-10J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Geometric Analysis"
      ],
      "metadata": {
        "id": "VxV1ckXPF2mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropiaBR(pk, base=10):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "(train_X, train_Y), (test_X, test_Y) = mnist.load_data()\n",
        "\n",
        "#Pre-processamento\n",
        "train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "test_X = test_X.reshape((test_X.shape[0], 28, 28, 1))    # Ajuste da dimensão\n",
        "train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "train_Y = to_categorical(train_Y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "test_Y = to_categorical(test_Y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "#Fim do Processamento\n",
        "\n",
        "#Rotacionando imagens\n",
        "train_x1Vertical = [tf.image.flip_left_right(img) for img in train_X]\n",
        "train_x2Horzontal = [flip_up_down(img) for img in train_X]\n",
        "train_x390graus = [tf.image.rot90(img, k=1) for img in train_X]\n",
        "\n",
        "#Calculando entropia\n",
        "tuplasEntropia0Normal = [(index, entropiaBR(img)) for index, img in enumerate(train_X)]                           #dataset normal\n",
        "tuplasEntropia1Vertical = [(index, entropiaBR(img)) for index, img in enumerate(train_X)]                         #dataset vertical\n",
        "tuplasEntropia2Horizontal = [(index, entropiaBR(img)) for index, img in enumerate(train_X)]                       #dataset horizontal\n",
        "tuplasEntropia390Graus = [(index, entropiaBR(img)) for index, img in enumerate(train_X)]                          #dataset 90 graus"
      ],
      "metadata": {
        "id": "_l3CflUn2Qrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contadorEqual = 0\n",
        "for i, (x1, x2, x3, x4) in enumerate (zip (tuplasEntropia0Normal, tuplasEntropia1Vertical, tuplasEntropia2Horizontal, tuplasEntropia390Graus)):\n",
        "  if (np.array_equal(x1,x2) & np.array_equal(x1,x3) & np.array_equal(x1,x3) & np.array_equal(x1,x4)):\n",
        "    contadorEqual += 1\n",
        "print(f\"Total de imagens iguais: {contadorEqual}. Total de imagens no dataset: {(len(train_x))}\")\n",
        "print(\"Exemplo, imagem nº: \")\n",
        "print(tuplasEntropia0Normal[2],tuplasEntropia1Vertical[2],tuplasEntropia2Horizontal[2],tuplasEntropia390Graus[2])"
      ],
      "metadata": {
        "id": "VF3luOC2CXPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(train_X, train_Y), (test_X, test_Y) = mnist.load_data()\n",
        "valor1 = tuplasEntropia0Normal[2][1]\n",
        "valor2 = tuplasEntropia1Vertical[2][1]\n",
        "valor3 = tuplasEntropia2Horizontal[2][1]\n",
        "valor4 = tuplasEntropia390Graus[2][1]\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "axs[0].imshow(train_X[1], cmap='gray')                    #< normal vindo do dataset\n",
        "axs[0].text(5, 5, str(valor1), color=\"red\", fontsize=12)\n",
        "axs[1].imshow(train_x1Vertical[1], cmap='gray')    #< vertical\n",
        "axs[1].text(5, 5, str(valor2), color=\"red\", fontsize=12)\n",
        "axs[2].imshow(train_x2Horzontal[1], cmap='gray')  #< horizontal\n",
        "axs[2].text(5, 5, str(valor3), color=\"red\", fontsize=12)\n",
        "axs[3].imshow(train_x390graus[1], cmap='gray')     #< girando 90 graus\n",
        "axs[3].text(5, 5, str(valor4), color=\"red\", fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f7tnC9MT2Quf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##     * Scale Transformation (Normalization);"
      ],
      "metadata": {
        "id": "LSPYojrQAQN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "from tensorflow.image import flip_left_right, flip_up_down\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def entropia(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    return -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    #return round(-np.sum(pk * np.log(pk) / np.log(base)), 17)\n",
        "\n",
        "def filtroEntropiaAnaliseEscala(train_X):  # filtro de entropia\n",
        "    base = 2 # base de logaritimo\n",
        "    tuplasEntropia = [(index, entropia(img.flatten(), base = base)) for index, img in enumerate(train_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
        "    entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "    n = len(entropiesLocal_ordenado)\n",
        "    if n % 2 == 1:\n",
        "        median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "    else:\n",
        "        median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "    indices_filtrados = [item[0] for item in entropiesLocal_ordenado if item[1] <= median]  #indices de entropia entropia baixa abaixo da mediana.\n",
        "    return train_X, entropiesLocal_ordenado\n",
        "\n",
        "def carregarPeparar(train_X, train_y, test_X, test_y, d, c): # Carregamento e Tratamento\n",
        "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], d, d, c))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0       # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0         # Normalização\n",
        "  train_y = to_categorical(train_y, 10)         #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_y = to_categorical(test_y, 10)           #10 classes possiveis     -   # to Categorical para as classes\n",
        "  return train_X, train_y, test_X, test_y\n",
        "\n",
        "def analiseConjuntos(conjuntoA, ConjuntoB):\n",
        "  set1 = set(conjuntoA)\n",
        "  set2 = set(ConjuntoB)\n",
        "  elementosComuns = set1.intersection(set2)\n",
        "  total = len(conjuntoA)\n",
        "  elComum = len(elementosComuns)\n",
        "  porcen = round(((elComum/total)*100),2)\n",
        "  print(f\"Total de elementos em comum: {elComum}, porcentagem de elementos em comun: {porcen}%.\")"
      ],
      "metadata": {
        "id": "sKNGWxn5C73O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analise 1 - calcular entropia antes do achatamento da matriz\n",
        "(train_x1, train_y1), (test_x1, test_y1) = mnist.load_data() # Pegar data set\n",
        "train_x1, entropiesLocal_ordenadox1 = filtroEntropiaAnaliseEscala(train_x1) #filtrar direto entropia\n",
        "train_x1, train_y1, test_x1, test_y1 = carregarPeparar(train_x1, train_y1, test_x1, test_y1, 28, 1) #padronização e categorização"
      ],
      "metadata": {
        "id": "y33B-RQkDBLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analise 2 - calcular entropia apos achatamento da matriz\n",
        "(train_x2, train_y2), (test_x2, test_y2) = mnist.load_data() #Pegar o dataset\n",
        "train_x2, train_y2, test_x2, test_y2 = carregarPeparar(train_x2, train_y2, test_x2, test_y2, 28, 1) # filtrar e padrozinar realizando mudança de escala\n",
        "train_x2, entropiesLocal_ordenado2 = filtroEntropiaAnaliseEscala(train_x2) #filtrar apos ajuste de dimensão e normalização"
      ],
      "metadata": {
        "id": "WCxmkl8PDBOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "totalIgualx1x2 = 0\n",
        "resultadosDif = []\n",
        "for i, (x1, x2) in enumerate (zip(entropiesLocal_ordenadox1, entropiesLocal_ordenado2)):\n",
        "  if not np.array_equal(x1,x2):\n",
        "    resultadosDif.append(i)\n",
        "  if np.array_equal(x1,x2):\n",
        "    totalIgualx1x2 += 1\n",
        "\n",
        "print(f\"Tamanho train_x1: \",  len(entropiesLocal_ordenadox1), \"   \", f\"Tamanho do train_x2: \", len(entropiesLocal_ordenado2))\n",
        "print(f\"Ordem e valor igual de (x1,x2) é {totalIgualx1x2}\")\n",
        "print(\"Resultados Diferentes: \", len(resultadosDif))\n",
        "n = 10\n",
        "for index, valor in islice(enumerate(resultadosDif), n):\n",
        "  print(f\"Indice {index}: \", \" Entropia (Antes Normalização)): \", entropiesLocal_ordenadox1[valor], \", Entropia (Depois da Normalização): \", entropiesLocal_ordenado2[valor])"
      ],
      "metadata": {
        "id": "gE1WIxDSDScb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##    * Scale Transformation (intensity [brightness]/linear transformations);"
      ],
      "metadata": {
        "id": "YZGPLHZy2YBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def entropiaBR(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "# Carregando o dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_trainImagemOriginal = x_train\n",
        "\n",
        "# 1. Reduzindo a intensidade das imagens pela metade\n",
        "x_train_metade_escala = x_train * 0.5\n",
        "x_testmetade_escala= x_test * 0.5\n",
        "\n",
        "tuplasEntropia0Normal = [(index, entropiaBR(img)) for index, img in enumerate(x_train)]                                #dataset original\n",
        "tuplasEntropia1Intensidade = [(index, entropiaBR(img)) for index, img in enumerate(x_train_metade_escala)]             #dataset intensidade reduzida\n",
        "\n",
        "# Mostrando a imagem original e as duas versões alteradas\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
        "ax[0].imshow(x_trainImagemOriginal[0], cmap='gray')\n",
        "ax[0].set_title('Original')\n",
        "ax[1].imshow(x_train_metade_escala[0], cmap='gray')\n",
        "ax[1].set_title('Half the Pixel Intensity')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "contadorEqual = 0\n",
        "for i, (x1, x2) in enumerate (zip (tuplasEntropia0Normal, tuplasEntropia1Intensidade)):\n",
        "  if (np.array_equal(x1,x2)):\n",
        "    contadorEqual += 1\n",
        "print(f\"Total de imagens iguais: {contadorEqual}. Total de imagens no dataset: {(len(train_x))}\")\n",
        "print(\"Exemplo, imagem nº: \")\n",
        "print(tuplasEntropia0Normal[2], tuplasEntropia1Intensidade[2])"
      ],
      "metadata": {
        "id": "Pn1KjPgv2T3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def entropiaBR(pk, base=2):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "# Carregando o dataset CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_trainImagemOriginal = x_train\n",
        "\n",
        "# 1. Reduzindo a intensidade das imagens pela metade\n",
        "x_train_metade_escala = (x_train * 0.5).astype(np.uint8)  # Conversão de para exibir a imagem\n",
        "x_train_metade_escala1 = x_train * 0.5\n",
        "x_testmetade_escala1 = x_test * 0.5\n",
        "\n",
        "\n",
        "tuplasEntropia0Normal = [(index, entropiaBR(img)) for index, img in enumerate(x_train)]                                # dataset original\n",
        "tuplasEntropia1Intensidade = [(index, entropiaBR(img)) for index, img in enumerate(x_train_metade_escala1)]            # dataset intensidade reduzida\n",
        "\n",
        "# Mostrando a imagem original e a versão alterada\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
        "ax[0].imshow(x_trainImagemOriginal[0])\n",
        "ax[0].set_title('Original')\n",
        "ax[1].imshow(x_train_metade_escala[0])\n",
        "ax[1].set_title('Metade da Intensidade do Pixel')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "contadorEqual = 0\n",
        "for i, (x1, x2) in enumerate(zip(tuplasEntropia0Normal, tuplasEntropia1Intensidade)):\n",
        "  if (np.array_equal(x1,x2)):\n",
        "    contadorEqual += 1\n",
        "print(f\"Total de imagens iguais: {contadorEqual}. Total de imagens no dataset: {len(x_train)}\")\n",
        "print(\"Exemplo, imagem nº: \")\n",
        "print(tuplasEntropia0Normal[2], tuplasEntropia1Intensidade[2])\n"
      ],
      "metadata": {
        "id": "COVcYjr_2lD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##      * Scale Transformation (intensity [Illumination]/exponential transformations);\n"
      ],
      "metadata": {
        "id": "raP4Cp2TyAaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data() # Pegar data set\n",
        "\n",
        "image = train_x[0]\n",
        "# Mostrar a imagem original\n",
        "plt.figure()\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "plt.title(\"Imagem Original\")\n",
        "plt.colorbar()\n",
        "equalized_image = cv2.equalizeHist(image)\n",
        "# Mostrar a imagem equalizada\n",
        "plt.figure()\n",
        "plt.imshow(equalized_image, cmap=\"gray\")\n",
        "plt.title(\"Imagem Equalizada\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "print(entropia(train_x[0], 10))\n",
        "print(entropia(equalized_image, 10))"
      ],
      "metadata": {
        "id": "4VWnXoIE9WmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Carregar o dataset MNIST\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "# Função para calcular a entropia\n",
        "def entropia(image, bins):\n",
        "    histogram, _ = np.histogram(image.flatten(), bins=bins, range=[0, 256])\n",
        "    # Normalizar o histograma\n",
        "    histogram = histogram / histogram.sum()\n",
        "    # Calcular a entropia\n",
        "    entropy = -np.sum(histogram * np.log2(histogram + 1e-7)) # Adicionando um pequeno valor para evitar log de 0\n",
        "    return entropy\n",
        "\n",
        "# Loop para processar e mostrar as 10 primeiras imagens\n",
        "for i in range(10):\n",
        "    image = train_x[i]\n",
        "\n",
        "    # Mostrar a imagem original\n",
        "    plt.figure()\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    plt.title(f\"Original Image #{i+1}\")\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Equalizar a imagem\n",
        "    equalized_image = cv2.equalizeHist(image)\n",
        "\n",
        "    # Mostrar a imagem equalizada\n",
        "    plt.figure()\n",
        "    plt.imshow(equalized_image, cmap=\"gray\")\n",
        "    plt.title(f\"Equalized Image #{i+1}\")\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Calcular e imprimir a entropia\n",
        "    original_entropy = entropia(image, 10)\n",
        "    equalized_entropy = entropia(equalized_image, 10)\n",
        "    print(f\"Entropia da Imagem Original #{i+1}: {original_entropy}\")\n",
        "    print(f\"Entropia da Imagem Equalizada #{i+1}: {equalized_entropy}\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0IgS3o3a-DUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the absolute difference between entropy values (normal) and entropy (after exponential change)\n"
      ],
      "metadata": {
        "id": "DTJ3Pm40D-NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "#Analise Parte 1 - salvando o array da base original com entropia calculada (Normal)\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data() # Pegar data set\n",
        "train_x1, entropiesLocal_ordenadox1 = filtroEntropiaAnaliseEscala(train_x) #filtrar direto entropia\n",
        "\n",
        "#Analise Parte 2 - salvando o array da base ordenada apos aplicação de filtro de iluminação [intensidade]\n",
        "#train_x2 = cv2.equalizeHist(train_x)\n",
        "train_x2 = [cv2.equalizeHist(img) for img in train_x]\n",
        "train_x2, entropiesLocal_ordenadox2 = filtroEntropiaAnaliseEscala(train_x2) #filtrar direto entropia\n",
        "\n",
        "analiseConjuntos(entropiesLocal_ordenadox1, entropiesLocal_ordenadox2)\n",
        "tupla_ordenada1 = sorted(entropiesLocal_ordenadox1, key=lambda x: x[0])\n",
        "tupla_ordenada2 = sorted(entropiesLocal_ordenadox2, key=lambda x: x[0])\n",
        "print(tupla_ordenada1[:5])\n",
        "print(tupla_ordenada2[:5])\n",
        "indice_a_comparar = 1\n",
        "resultadoComparacao = []\n",
        "for i in range(len(tupla_ordenada1)):\n",
        "    valor_tupla1 = tupla_ordenada1[i][indice_a_comparar]\n",
        "    valor_tupla2 = tupla_ordenada2[i][indice_a_comparar]\n",
        "    diferenca_absoluta = abs(valor_tupla1 - valor_tupla2)\n",
        "    if valor_tupla1 + valor_tupla2 != 0:\n",
        "        porcentagem_similaridade = (1 - (diferenca_absoluta / (valor_tupla1 + valor_tupla2))) * 100\n",
        "        resultadoComparacao.append((i, porcentagem_similaridade))\n",
        "    else:\n",
        "        porcentagem_similaridade = 0.0  # Se a soma for zero, a similaridade é definida como 0\n",
        "\n",
        "resultadoComparacao[:5]\n",
        "x = [item[1] for item in resultadoComparacao]\n",
        "media = round(sum(x) / len(x), 2)\n",
        "print(f\"Similaridade entre a entropia das imagens atraves da diferença absoluta {media}%\")"
      ],
      "metadata": {
        "id": "qS5lSexWymxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "#Analise Parte 1 - salvando o array da base original com entropia calculada (Normal)\n",
        "(train_x, train_y), (test_x, test_y) = cifar10.load_data() # Pegar data set\n",
        "train_x1, entropiesLocal_ordenadox1 = filtroEntropiaAnaliseEscala(train_x) #filtrar direto entropia\n",
        "\n",
        "#Analise Parte 2 - salvando o array da base ordenada apos aplicação de filtro de iluminação [intensidade]\n",
        "#train_x2 = cv2.equalizeHist(train_x)\n",
        "train_x2 = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in train_x]\n",
        "train_x2, entropiesLocal_ordenadox2 = filtroEntropiaAnaliseEscala(train_x2) #filtrar direto entropia\n",
        "\n",
        "analiseConjuntos(entropiesLocal_ordenadox1, entropiesLocal_ordenadox2)\n",
        "tupla_ordenada1 = sorted(entropiesLocal_ordenadox1, key=lambda x: x[0])\n",
        "tupla_ordenada2 = sorted(entropiesLocal_ordenadox2, key=lambda x: x[0])\n",
        "print(tupla_ordenada1[:5])\n",
        "print(tupla_ordenada2[:5])\n",
        "indice_a_comparar = 1\n",
        "resultadoComparacao = []\n",
        "for i in range(len(tupla_ordenada1)):\n",
        "    valor_tupla1 = tupla_ordenada1[i][indice_a_comparar]\n",
        "    valor_tupla2 = tupla_ordenada2[i][indice_a_comparar]\n",
        "    diferenca_absoluta = abs(valor_tupla1 - valor_tupla2)\n",
        "    if valor_tupla1 + valor_tupla2 != 0:\n",
        "        porcentagem_similaridade = (1 - (diferenca_absoluta / (valor_tupla1 + valor_tupla2))) * 100\n",
        "        resultadoComparacao.append((i, porcentagem_similaridade))\n",
        "    else:\n",
        "        porcentagem_similaridade = 0.0  # Se a soma for zero, a similaridade é definida como 0\n",
        "\n",
        "resultadoComparacao[:5]\n",
        "x = [item[1] for item in resultadoComparacao]\n",
        "media = round(sum(x) / len(x), 2)\n",
        "print(f\"Similaridade entre a entropia das imagens atraves da diferença absoluta {media}%\")"
      ],
      "metadata": {
        "id": "6CA6bTzN0OCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking for order difference with and without normalization\n"
      ],
      "metadata": {
        "id": "wcmReilQZI0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.image import flip_left_right\n",
        "from tensorflow.image import flip_up_down\n",
        "import numpy as np\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from itertools import islice\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from PIL import Image\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "def entropiaBR(pk, base=10):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "def entropNormali(train_X):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  # indices de entropia entropia baixa abaixo da mediana.\n",
        "  return indices_filtrados, entropiesLocal_ordenado\n",
        "\n",
        "def entropiNaoNormali(train_X):\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  return indices_filtrados, entropiesLocal_ordenado\n",
        "\n",
        "def analiseConjuntos(conjuntoA, ConjuntoB):\n",
        "  set1 = set(conjuntoA)\n",
        "  set2 = set(ConjuntoB)\n",
        "  elementosComuns = set1.intersection(set2)\n",
        "  total = len(conjuntoA)\n",
        "  elComum = len(elementosComuns)\n",
        "  porcen = round(((elComum/total)*100),2)\n",
        "  print(f\"Total de elementos em comum: {elComum}, porcentagem de elementos em comun: {porcen}%.\")"
      ],
      "metadata": {
        "id": "-AzFEDmwhHvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (test_x, test_y) = mnist.load_data() # Pegar dataset\n",
        "x1, y1 = entropNormali(train_x)\n",
        "x2, y2 = entropiNaoNormali(train_x)\n",
        "\n",
        "print(x1[1], x2[1])\n",
        "analiseConjuntos(x1, x2)"
      ],
      "metadata": {
        "id": "lqh5NmVuhK3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultadosDif = []\n",
        "totalIgualx1x2 = 0\n",
        "for i, (x1, x2) in enumerate(zip(y1, y2)):\n",
        "    if np.array_equal(x1, x2):\n",
        "        totalIgualx1x2 += 1\n",
        "    else:\n",
        "        resultadosDif.append((x1, x2))\n",
        "print(\"Total de resultados com valor igual: \", totalIgualx1x2)\n",
        "print(\"Resulados de entropia diferentes:  \", resultadosDif[:5])"
      ],
      "metadata": {
        "id": "N1nW7dlkiuT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##    * 2D to 1D Transformation (Flattening)."
      ],
      "metadata": {
        "id": "3OCutGCcAU9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analise os resultados dos valores da entropia em relação a formula (Biblioteca vs calculo)\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "from tensorflow.image import flip_left_right, flip_up_down\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numImage = 2\n",
        "(train_x5, train_y5), (test_x5, test_y5) = mnist.load_data() #Pegar o dataset\n",
        "base = 10 # base de logaritimo\n",
        "pk = np.array(train_x5[numImage].flatten())\n",
        "H = entropy(pk, base=base)\n",
        "print(f\"Entropia Biblioteca(scipy.stats) operacao flatten(): ({numImage}, {H})\")\n",
        "pk2 = np.array(train_x5[numImage])\n",
        "H2 = entropy(pk, base=base)\n",
        "print(f\"Entropia Biblioteca(scipy.stats) sem flatten(): ({numImage}, {H2})\")\n",
        "\n",
        "def entropia(pk, base=10, max=1):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    return result\n",
        "\n",
        "# estrutura de dados multidimensional (como uma matriz)\n",
        "(train_x1, train_y1), (test_x1, test_y1) = mnist.load_data() #Pegar o dataset\n",
        "tuplasEntropia1 = [(index, entropia(img.flatten())) for index, img in enumerate(train_x1)]\n",
        "print(\"Entropia Implementação (def Entropia)  operacao flatten(): \", tuplasEntropia1[numImage])\n",
        "\n",
        "# uma estrutura unidimensional (como um vetor).\n",
        "(train_x2, train_y2), (test_x2, test_y2) = mnist.load_data() #Pegar o dataset\n",
        "tuplasEntropia2 = [(index, entropia(img)) for index, img in enumerate(train_x2)]\n",
        "print(\"Entropia Implementação (def Entropia)  sem flatten(): \", tuplasEntropia1[numImage])\n",
        "\n",
        "totalIgualx1x2 = 0\n",
        "resultadosDif = []\n",
        "for i, (x1, x2) in enumerate (zip(tuplasEntropia1, tuplasEntropia2)):\n",
        "  if np.array_equal(x1,x2):\n",
        "     totalIgualx1x2 += 1\n",
        "print(f\"Tamanho train_x1: \",  len(tuplasEntropia1), \"   \", f\"Tamanho do train_x2: \", len(tuplasEntropia2))\n",
        "print(f\"Total de valores iguais de (x1,x2) é {totalIgualx1x2}\")"
      ],
      "metadata": {
        "id": "98dyieEuAhyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality reduction (PCA - Principal Component Analysis);"
      ],
      "metadata": {
        "id": "jarr5uBGSq_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def entropia(pk, base=10):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    return -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    #return round(-np.sum(pk * np.log(pk) / np.log(base)), 17)\n",
        "\n",
        "#PCA Reducao de Dimensionalidade\n",
        "#Dimensão e escala\n",
        "(train_x6, train_y6), (test_x6, test_y6) = mnist.load_data()\n",
        "train_x6 = train_x6.astype('float32') / 255.0\n",
        "test_x6 = test_x6.astype('float32') / 255.0\n",
        "train_x_2d = train_x6.reshape(-1, 28*28)\n",
        "pca = PCA(0.9)\n",
        "train_x_pca = pca.fit_transform(train_x_2d)\n",
        "\n",
        "tuplasEntropia3 = [(index, entropia(img.flatten())) for index, img in enumerate(train_x_pca)]\n",
        "print(tuplasEntropia3[3])\n",
        "\n",
        "#Dimensão\n",
        "(train_x7, train_y7), (test_x7, test_y7) = mnist.load_data()\n",
        "pca = PCA(0.9)\n",
        "train_x_2d4 = train_x6.reshape(-1, 28*28)\n",
        "train_x_pca4 = pca.fit_transform(train_x_2d4)\n",
        "\n",
        "tuplasEntropia4 = [(index, entropia(img)) for index, img in enumerate(train_x_pca4)]\n",
        "print(tuplasEntropia4[3])"
      ],
      "metadata": {
        "id": "-URjmRQ_S0jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the datasets before and after PCA."
      ],
      "metadata": {
        "id": "3xKZMwBXnF-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import time\n",
        "\n",
        "\n",
        "def entropia(pk, base=10):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    return -np.sum(pk * np.log(pk) / np.log(base))\n",
        "\n",
        "def indicesEntropiaMedian(train_X, train_Y, test_X, test_Y):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], 28, 28, 1))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_Y = to_categorical(train_Y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_Y = to_categorical(test_Y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "\n",
        "  tuplasEntropia = [(index, entropia(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  #print(tuplasEntropia[:10])\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  #print(entropiesLocal_ordenado[:10])\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  #print(indices_filtrados[:10])\n",
        "  return indices_filtrados\n",
        "\n",
        "def indicesEntropiaMedianC(train_X, train_Y, test_X, test_Y):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 32, 32, 3))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], 32, 32, 3))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_Y = to_categorical(train_Y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_Y = to_categorical(test_Y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "\n",
        "  tuplasEntropia = [(index, entropia(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  # indices de entropia entropia baixa abaixo da mediana.\n",
        "  return indices_filtrados\n",
        "\n",
        "def indicesEntropiaPCAMedian(train_X, train_Y, test_X, test_Y):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], 28, 28, 1))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_Y = to_categorical(train_Y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_Y = to_categorical(test_Y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "\n",
        "  train_x_2d = train_x.reshape(-1, 28*28)\n",
        "  pca = PCA(0.9)\n",
        "  train_x_pca = pca.fit_transform(train_x_2d) # PCA\n",
        "  train_x_reverted = pca.inverse_transform(train_x_pca)\n",
        "  train_x_cnn = train_x_reverted.reshape(-1, 28, 28, 1)\n",
        "\n",
        "  tuplasEntropia = [(index, entropia(img.flatten())) for index, img in enumerate(train_x_cnn)]\n",
        "  #print(tuplasEntropia[:10])\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  #print(entropiesLocal_ordenado[:10])\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  #print(indices_filtrados[:10])\n",
        "  return indices_filtrados\n",
        "\n",
        "def indicesEntropiaPCAMedianC(train_X, train_Y, test_X, test_Y):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 32, 32, 3))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], 32, 32, 3))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_Y = to_categorical(train_Y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_Y = to_categorical(test_Y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "\n",
        "  train_x_2d = train_x.reshape(-1, 32*32)\n",
        "  pca = PCA(0.9)\n",
        "  train_x_pca = pca.fit_transform(train_x_2d) # PCA\n",
        "  train_x_reverted = pca.inverse_transform(train_x_pca)\n",
        "  train_x_cnn = train_x_reverted.reshape(-3, 32, 32, 3)\n",
        "\n",
        "  tuplasEntropia = [(index, entropia(img.flatten())) for index, img in enumerate(train_x_cnn)]\n",
        "  #print(tuplasEntropia[:10])\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  #print(entropiesLocal_ordenado[:10])\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  # indices de entropia entropia baixa abaixo da mediana.\n",
        "  #print(indices_filtrados[:10])\n",
        "  return indices_filtrados\n",
        "\n",
        "def analiseConjuntos(conjuntoA, ConjuntoB):\n",
        "  set1 = set(conjuntoA)\n",
        "  set2 = set(ConjuntoB)\n",
        "  elementosComuns = set1.intersection(set2)\n",
        "  total = len(conjuntoA)\n",
        "  elComum = len(elementosComuns)\n",
        "  porcen = round(((elComum/total)*100),2)\n",
        "  print(f\"Total de elementos em comum: {elComum}, porcentagem de elementos em comun: {porcen}%.\")"
      ],
      "metadata": {
        "id": "7QsA9mQYnIkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparando se a lista de elementos 1 (Entropia sem PCA) é equivalente a lista de elementos 2 (Com PCA) : Equivalencia em ordem\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "x1Entropia = indicesEntropiaMedian(train_x, train_y, test_x, test_y)\n",
        "x2EntropiaPCA = indicesEntropiaPCAMedian(train_x, train_y, test_x, test_y)\n",
        "\n",
        "analiseConjuntos(x1Entropia, x2EntropiaPCA)"
      ],
      "metadata": {
        "id": "PoIqcOxQnLdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparando se a lista de elementos 1 (Entropia sem PCA) é equivalente a lista de elementos 2 (Com PCA) : Equivalencia em ordem\n",
        "from keras.datasets import cifar10\n",
        "(train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
        "x1Entropia = indicesEntropiaMedianC(train_x, train_y, test_x, test_y)\n",
        "x2EntropiaPCA = indicesEntropiaPCAMedianC(train_x, train_y, test_x, test_y)\n",
        "\n",
        "analiseConjuntos(x1Entropia, x2EntropiaPCA)"
      ],
      "metadata": {
        "id": "ZgWaYJNrqc6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histogram"
      ],
      "metadata": {
        "id": "QMOx0xyZth8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import time\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import kstest\n",
        "from scipy.stats import normaltest\n",
        "from scipy import stats\n",
        "\n",
        "def indicesEntropiaMedianMnist(train_X, train_Y, test_X, test_Y):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], 28, 28, 1))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_Y = to_categorical(train_Y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_Y = to_categorical(test_Y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  # indices de entropia entropia baixa abaixo da mediana.\n",
        "  train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "  train_Y = np.array([train_Y[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "  return train_X, train_Y, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "def indicesEntropiaMedianMnist2(train_X, train_Y, test_X, test_Y):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], 28, 28, 1))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  # indices de entropia entropia baixa abaixo da mediana.\n",
        "  train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
        "  train_Y = np.array([train_Y[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "  return train_X, train_Y, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "\n",
        "def indicesEntropiaMedianCifar10(train_X, train_Y, test_X, test_Y):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 32, 32, 3))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], 32, 32, 3))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_Y = to_categorical(train_Y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_Y = to_categorical(test_Y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  # indices de entropia entropia baixa abaixo da mediana.\n",
        "  train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
        "  train_Y = np.array([train_Y[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "  return train_X, train_Y, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "\n",
        "def indicesEntropiaMedianCifar102(train_X, train_Y, test_X, test_Y):\n",
        "  train_X = train_X.reshape((train_X.shape[0], 32, 32, 3))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], 32, 32, 3))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  # indices de entropia entropia baixa abaixo da mediana.\n",
        "  train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
        "  train_Y = np.array([train_Y[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "  return train_X, train_Y, tuplasEntropia, entropiesLocal_ordenado\n",
        "\n",
        "def entropiaBR(pk, base=10):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result"
      ],
      "metadata": {
        "id": "007wAjVlr0uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x1, train_y1), (test_x1, test_y1) = mnist.load_data()\n",
        "train_x1, train_y1, tuplasEntropia1, entropiesLocal_ordenado1 = indicesEntropiaMedianMnist(train_x1, train_y1, test_x1, test_y1)\n",
        "(train_x2, train_y2), (test_x2, test_y2) =  cifar10.load_data()\n",
        "train_x2, train_y2, tuplasEntropia2, entropiesLocal_ordenado2 = indicesEntropiaMedianCifar10(train_x2, train_y2, test_x2, test_y2)"
      ],
      "metadata": {
        "id": "PrBclJQ6sBjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MNIST [Entropy Histogram]"
      ],
      "metadata": {
        "id": "tiJUtZn34-9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valEntrop = [(ento[1]) for ento in (tuplasEntropia1)]\n",
        "#valEntrop = [(ento[1]) for ento in (entropiesLocal_ordenado1)]\n",
        "soma = sum(valEntrop)\n",
        "alvos = [0.25 * soma, 0.5 * soma, 0.75 * soma, soma]\n",
        "grupos = []\n",
        "grupo_atual = []\n",
        "soma_acumulada = 0\n",
        "index_alvo = 0\n",
        "for num in valEntrop:\n",
        "    grupo_atual.append(num)\n",
        "    soma_acumulada += num\n",
        "    if soma_acumulada >= alvos[index_alvo]:\n",
        "        grupos.append(grupo_atual)\n",
        "        grupo_atual = []\n",
        "        index_alvo += 1\n",
        "        if index_alvo == 4:\n",
        "            break\n",
        "if grupo_atual:\n",
        "    grupos.append(grupo_atual)\n",
        "\n",
        "print(\"Tamanho do Conjunto Analisado: \", len(valEntrop))\n",
        "print(\"Tamanho do grupo 1 - 25% dos valores: \", len(grupos[0]))\n",
        "print(\"Tamanho do grupo 2 - 50% dos valores: \", len(grupos[1]))\n",
        "print(\"Tamanho do grupo 3 - 75% dos valores: \", len(grupos[2]))\n",
        "print(\"Tamanho do grupo 4 - 100% dos valores: \", len(grupos[3]))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "dados = valEntrop\n",
        "plt.hist(dados, bins=10, edgecolor='black', alpha=0.7)\n",
        "plt.title('Histograma dos Dados')\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "xEnt = [(ento[1]) for ento in (tuplasEntropia1)]\n",
        "mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "#Shapiro-Wilk:\n",
        "statSW, pSW = stats.shapiro(xEnt)  # Obtém a estatística de teste e o valor-p\n",
        "validacaoSW = \"Não Rejeitamos H0\" if pSW > 0.05 else \"Rejeitamos H0\"\n",
        "print(\"Cal. Shapiro-Wilk, Statistica: %.13f\" % statSW, \"Valor-p: %.10f\" % pSW, \"Validação de Hipotese H0:\", validacaoSW)\n",
        "#Kolmogorov-Smirnov\n",
        "mu, std_dev = np.mean(xEnt), np.std(xEnt)\n",
        "statKS, pKS = kstest(xEnt, 'norm', args=(mu, std_dev))\n",
        "validacaoKS = \"Não Rejeitamos H0\" if pKS > 0.05 else \"Rejeitamos H0\"\n",
        "print(\"Cal. Kolmogorov-Smirnov, Statistica: %.10f\" % statKS, \"Valor-p: %.10f\" % pKS, \"Validação de Hipotese H0:\", validacaoKS)\n",
        "#D'Agostino e Pearson's\n",
        "statAP, pAP = normaltest(xEnt)\n",
        "validacaoAP = \"Não Rejeitamos H0\" if pAP > 0.05 else \"Rejeitamos H0\"\n",
        "print(\"Cal. D'Agostino e Pearson's, Statistica: %.10f\" % statAP, \"Valor-p: %.10f\" % pAP, \"Validação de Hipotese H0:\", validacaoAP)\n",
        "\n",
        "if (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\") or \\\n",
        "  (validacaoSW == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\") or \\\n",
        "  (validacaoSW == \"Não Rejeitamos H0\" and validacaoKS == \"Não Rejeitamos H0\" and validacaoAP == \"Não Rejeitamos H0\"):\n",
        "  print(\"Com base em uma validação dupla identificamos de que não há evidência suficiente para rejeitar a hipótese nula. A amostra parece seguir uma distribuição normal.\")\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "uBZhphqetGOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distribution of MNIST Label Selection"
      ],
      "metadata": {
        "id": "QX-OwoEfEIAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x1, train_y1), (test_x1, test_y1) = mnist.load_data()\n",
        "train_x1, train_y1, tuplasEntropia1, entropiesLocal_ordenado1 = indicesEntropiaMedianMnist2(train_x1, train_y1, test_x1, test_y1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "valEntrop = train_y1\n",
        "dados = valEntrop\n",
        "plt.hist(dados, bins=10, edgecolor='black', alpha=0.7)\n",
        "plt.title('Histograma dos Dados')\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0jIdmsUW_lVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(train_x1, train_y1), (test_x1, test_y1) = mnist.load_data()\n",
        "valEntrop = train_y1\n",
        "dados = valEntrop\n",
        "plt.hist(dados, bins=10, edgecolor='black', alpha=0.7)\n",
        "plt.title('Histograma dos Dados')\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a2TZ18ykESTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CIFAR-10 [Histogram]"
      ],
      "metadata": {
        "id": "0qE8w59u5CaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valEntrop = [(ento[1]) for ento in (tuplasEntropia2)]\n",
        "#valEntrop = [(ento[1]) for ento in (entropiesLocal_ordenado2)]\n",
        "soma = sum(valEntrop)\n",
        "alvos = [0.25 * soma, 0.5 * soma, 0.75 * soma, soma]\n",
        "grupos = []\n",
        "grupo_atual = []\n",
        "soma_acumulada = 0\n",
        "index_alvo = 0\n",
        "for num in valEntrop:\n",
        "    grupo_atual.append(num)\n",
        "    soma_acumulada += num\n",
        "    if soma_acumulada >= alvos[index_alvo]:\n",
        "        grupos.append(grupo_atual)\n",
        "        grupo_atual = []\n",
        "        index_alvo += 1\n",
        "        if index_alvo == 4:\n",
        "            break\n",
        "if grupo_atual:\n",
        "    grupos.append(grupo_atual)\n",
        "\n",
        "print(\"Tamanho do Conjunto Analisado: \", len(valEntrop))\n",
        "print(\"Tamanho do grupo 1 - 25% dos valores: \", len(grupos[0]))\n",
        "print(\"Tamanho do grupo 2 - 50% dos valores: \", len(grupos[1]))\n",
        "print(\"Tamanho do grupo 3 - 75% dos valores: \", len(grupos[2]))\n",
        "print(\"Tamanho do grupo 4 - 100% dos valores: \", len(grupos[3]))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "dados = valEntrop\n",
        "plt.hist(dados, bins=10, edgecolor='black', alpha=0.7)\n",
        "plt.title('Histograma dos Dados')\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YI1QLjfw5Et4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x2, train_y2), (test_x2, test_y2) =  cifar10.load_data()\n",
        "train_x2, train_y2, tuplasEntropia2, entropiesLocal_ordenado2 = indicesEntropiaMedianCifar102(train_x2, train_y2, test_x2, test_y2)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "valEntrop = train_y2\n",
        "dados = valEntrop\n",
        "plt.hist(dados, bins=10, edgecolor='black', alpha=0.7)\n",
        "plt.title('Histograma dos Dados')\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "69-4rSWE_esY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(train_x2, train_y2), (test_x2, test_y2) =  cifar10.load_data()\n",
        "valEntrop = train_y2\n",
        "dados = valEntrop\n",
        "plt.hist(dados, bins=10, edgecolor='black', alpha=0.7)\n",
        "plt.title('Histograma dos Dados')\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PCmfH5Se_eut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verificar se os selecionados pela entropia possuem uma autocorrelação forte?\n",
        "# verificar se possuem uma autocorrelação similar?"
      ],
      "metadata": {
        "id": "WrkGx-4a_fIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# AutoCorrelação\n",
        "(train_x1, train_y1), (test_x1, test_y1) = mnist.load_data()\n",
        "img = train_x1[0]\n",
        "\n",
        "# Subtraindo a média da imagem para remover qualquer tendência de luminosidade constante\n",
        "img = img - np.mean(img)\n",
        "\n",
        "# Calcular a autocorrelação usando a transformada de Fourier para eficiência\n",
        "f = np.fft.fft2(img)\n",
        "fshift = np.fft.fftshift(f)\n",
        "magnitude_spectrum = np.abs(fshift)\n",
        "autocorr = np.fft.ifft2(magnitude_spectrum * magnitude_spectrum)\n",
        "autocorr = np.abs(autocorr)\n",
        "autocorr = autocorr / autocorr.max()  # Normalizando\n",
        "\n",
        "# Mostrando a imagem e sua autocorrelação\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axs[0].imshow(img, cmap=\"gray\")\n",
        "axs[0].set_title('Original Image')\n",
        "axs[1].imshow(autocorr, cmap=\"hot\")\n",
        "axs[1].set_title('Autocorrelation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DlRAp34Y6Ydy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# AutoCorrelação\n",
        "(train_x1, train_y1), (test_x1, test_y1) = mnist.load_data()\n",
        "train_x1, train_y1, tuplasEntropia1, entropiesLocal_ordenado1 = indicesEntropiaMedianMnist(train_x1, train_y1, test_x1, test_y1)\n",
        "img = train_x1[20000]\n",
        "\n",
        "# Subtraindo a média da imagem para remover qualquer tendência de luminosidade constante\n",
        "img = img - np.mean(img)\n",
        "\n",
        "# Calcular a autocorrelação usando a transformada de Fourier para eficiência\n",
        "f = np.fft.fft2(img)\n",
        "fshift = np.fft.fftshift(f)\n",
        "magnitude_spectrum = np.abs(fshift)\n",
        "autocorr = np.fft.ifft2(magnitude_spectrum * magnitude_spectrum)\n",
        "autocorr = np.abs(autocorr)\n",
        "autocorr = autocorr / autocorr.max()  # Normalizando\n",
        "\n",
        "# Mostrando a imagem e sua autocorrelação\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axs[0].imshow(img, cmap=\"gray\")\n",
        "axs[0].set_title('Original Image')\n",
        "axs[1].imshow(autocorr, cmap=\"hot\")\n",
        "axs[1].set_title('Autocorrelation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zEwch1B8-fb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cifar10 Data Augmentation"
      ],
      "metadata": {
        "id": "igLgMsxUZ9f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "from keras import backend as K\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jqT8Ayo2Z880"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train/=255\n",
        "X_test/=255"
      ],
      "metadata": {
        "id": "KaO4-YTlaD5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropia(pk, base=10):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    return -np.sum(pk * np.log(pk) / np.log(base))\n",
        "\n",
        "def indicesEntropiaMedian(train_X):\n",
        "  tuplasEntropia = [(index, entropia(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  # indices de entropia entropia baixa abaixo da mediana.\n",
        "  return indices_filtrados, entropiesLocal_ordenado, tuplasEntropia"
      ],
      "metadata": {
        "id": "5IOV9nehagiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator()\n",
        "datagen.fit(X_train)\n",
        "\n",
        "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=4, seed=499):\n",
        "  for i in range(0,4):\n",
        "    pyplot.subplot(220 +1 +i)\n",
        "    pyplot.imshow(X_batch[i])\n",
        "  pyplot.show()\n",
        "  break"
      ],
      "metadata": {
        "id": "XOnaiLRAaHTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_filtrados, entropiesLocal_ordenado, tuplasEntropia = indicesEntropiaMedian(X_train)\n",
        "print(tuplasEntropia[200:204])"
      ],
      "metadata": {
        "id": "di0VbQHpasT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(rotation_range=359)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=4, seed=499):\n",
        "  for i in range(0,4):\n",
        "    pyplot.subplot(220 +1 +i)\n",
        "    pyplot.imshow(X_batch[i])\n",
        "  pyplot.show()\n",
        "  break"
      ],
      "metadata": {
        "id": "oJZBTnu5aInM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_filtrados, entropiesLocal_ordenado, tuplasEntropia = indicesEntropiaMedian(X_train)\n",
        "print(tuplasEntropia[200:204])"
      ],
      "metadata": {
        "id": "tgDsuc98bOIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "iL29xVg7Efyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropia(pk, base=10):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    return -np.sum(pk * np.log(pk) / np.log(base))\n",
        "\n",
        "def filtro(train_X, train_y):  # filtro de entropia\n",
        "    base = 2 # base de logaritimo\n",
        "    tuplasEntropia = [(index, entropy(img.flatten(), base = base)) for index, img in enumerate(train_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
        "    entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "    n = len(entropiesLocal_ordenado)\n",
        "    if n % 2 == 1:\n",
        "        median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "    else:\n",
        "        median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "    indices_filtrados = [item[0] for item in entropiesLocal_ordenado if item[1] <= median]  #indices de entropia entropia baixa abaixo da mediana.\n",
        "    train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
        "    train_y = np.array([train_y[i] for i in indices_filtrados]) # passa os indices selecionados\n",
        "    return train_X, train_y, tuplasEntropia\n",
        "\n",
        "def carregarPeparar(train_X, train_y, test_X, test_y, d, c): # Carregamento e Tratamento\n",
        "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], d, d, c))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
        "  train_y = to_categorical(train_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_y = to_categorical(test_y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
        "  return train_X, train_y, test_X, test_y\n",
        "\n",
        "def plotarDadosTrain(index_inicio, qtd, train_X): # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
        "  base = 330\n",
        "  import matplotlib.pyplot as plt\n",
        "  for i in range(index_inicio, index_inicio + qtd):\n",
        "      plt.subplot(base + 1 + (i - index_inicio))\n",
        "      plt.imshow(train_X[i], cmap=plt.get_cmap('gray'))\n",
        "  plt.show()\n",
        "  print(train_y[index_inicio:index_inicio + qtd])"
      ],
      "metadata": {
        "id": "WGIGni_qEmRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import time\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data() #Pegar o dataset\n",
        "plotarDadosTrain(1, 9, train_x)"
      ],
      "metadata": {
        "id": "pgvqWZn2E6bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (test_x, test_y) = mnist.load_data() #Pegar o dataset\n",
        "train_X1, train_y1, tuplas1 =  filtro(train_x, train_y)"
      ],
      "metadata": {
        "id": "NhKAYTi-FXt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (test_x, test_y) = mnist.load_data() #Pegar o dataset\n",
        "train_x, train_y, test_x, test_y = carregarPeparar(train_x, train_y, test_x, test_y, 28, 1)\n",
        "train_X1, train_y11, tuplas2 =  filtro(train_x, train_y)"
      ],
      "metadata": {
        "id": "0G2qHSNZFhe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contador = 0\n",
        "resultado_dif = []\n",
        "for i, (x1,x2) in enumerate (zip(tuplas1, tuplas2)):\n",
        "  if np.array_equal(x1,x2):\n",
        "    contador += 1\n",
        "  else:\n",
        "    resultado_dif.append(i)\n",
        "print(contador)\n",
        "print(resultado_dif[0])"
      ],
      "metadata": {
        "id": "jef02JHGHa-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tuplas1[resultado_dif[0]])\n",
        "print(tuplas2[resultado_dif[0]])"
      ],
      "metadata": {
        "id": "u_40jgZSJC2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of Decimal Places [Entropy]\n",
        "  * Mnist [The pixel values are integers ranging from 0 to 255]\n",
        "  * Fashion Mnist [The pixel values are integers ranging from 0 to 255]\n",
        "  * Cifar10 [The pixel values are integers ranging from 0 to 255]\n",
        "  * Cifar100 [The pixel values are integers ranging from 0 to 255]"
      ],
      "metadata": {
        "id": "OjfQ8jgqoI2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.image import flip_left_right\n",
        "from tensorflow.image import flip_up_down\n",
        "import numpy as np\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from itertools import islice\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from keras.src.utils.np_utils import to_categorical\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from PIL import Image\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from scipy.stats import entropy\n",
        "\n",
        "def entropia(pk, base=10):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    #return -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    return round(-np.sum(pk * np.log(pk) / np.log(base)), 17)\n",
        "\n",
        "def entropiaBR(pk, base=10):\n",
        "    pk = pk / np.sum(pk)\n",
        "    pk = pk[pk > 0]\n",
        "    result = -np.sum(pk * np.log(pk) / np.log(base))\n",
        "    result = round(result, 17)\n",
        "    return result\n",
        "\n",
        "def filtroEntropiaAnaliseEscala(train_X):  # filtro de entropia\n",
        "    base = 2 # base de logaritimo\n",
        "    tuplasEntropia = [(index, entropia(img.flatten(), base = base)) for index, img in enumerate(train_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
        "    entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "    n = len(entropiesLocal_ordenado)\n",
        "    if n % 2 == 1:\n",
        "        median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "    else:\n",
        "        median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "    indices_filtrados = [item[0] for item in entropiesLocal_ordenado if item[1] <= median]  #indices de entropia entropia baixa abaixo da mediana.\n",
        "    return train_X, entropiesLocal_ordenado\n",
        "\n",
        "def carregarPeparar(train_X, train_y, test_X, test_y, d, c): # Carregamento e Tratamento\n",
        "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  test_X = test_X.reshape((test_X.shape[0], d, d, c))    # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0       # Normalização\n",
        "  test_X = test_X.astype(\"float\")/255.0         # Normalização\n",
        "  train_y = to_categorical(train_y, 10)         #10 classes possiveis   -   # to Categorical para as classes\n",
        "  test_y = to_categorical(test_y, 10)           #10 classes possiveis     -   # to Categorical para as classes\n",
        "  return train_X, train_y, test_X, test_y\n",
        "\n",
        "def entropNormali(train_X, d, c):\n",
        "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
        "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  return indices_filtrados, entropiesLocal_ordenado\n",
        "\n",
        "def entropiNaoNormali(train_X):\n",
        "  tuplasEntropia = [(index, entropiaBR(img.flatten())) for index, img in enumerate(train_X)]\n",
        "  entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
        "  n = len(entropiesLocal_ordenado)\n",
        "  if n % 2 == 1:\n",
        "      median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
        "  else:\n",
        "      median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
        "  indices_filtrados = np.array([item[0] for item in entropiesLocal_ordenado if item[1] <= median])  #indices de entropia entropia baixa abaixo da mediana.\n",
        "  return indices_filtrados, entropiesLocal_ordenado\n",
        "\n",
        "def analiseConjuntos(conjuntoA, ConjuntoB):\n",
        "  set1 = set(conjuntoA)\n",
        "  set2 = set(ConjuntoB)\n",
        "  elementosComuns = set1.intersection(set2)\n",
        "  total = len(conjuntoA)\n",
        "  elComum = len(elementosComuns)\n",
        "  porcen = round(((elComum/total)*100),2)\n",
        "  print(f\"Total de elementos em comum: {elComum}, porcentagem de elementos em comun: {porcen}%.\")\n",
        "\n",
        "def casas_decimais_diferentes(a, b):\n",
        "    a_str = str(a)\n",
        "    b_str = str(b)\n",
        "\n",
        "    a_dec = a_str.split('.')[1] if '.' in a_str else ''\n",
        "    b_dec = b_str.split('.')[1] if '.' in b_str else ''\n",
        "\n",
        "    diff = abs(len(a_dec) - len(b_dec))\n",
        "    return diff"
      ],
      "metadata": {
        "id": "nMVKDyKtxAqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mnist\n",
        "  * [Analysis of Filter Before and After Normalization]"
      ],
      "metadata": {
        "id": "o2Bh3uPHio5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (test_x, test_y) = mnist.load_data() # Pegar dataset\n",
        "x1, y1 = entropNormali(train_x, 28, 1)\n",
        "x2, y2 = entropiNaoNormali(train_x)\n",
        "print(\"Conjunto Normalizado imagem nº: \", x1[1], y1[1], \"Conjunto não normalizado: imagem nº\", x2[1], y2[1])\n",
        "analiseConjuntos(y1, y2)\n",
        "\n",
        "resultadosDif = []\n",
        "totalIgualx1x2 = 0\n",
        "acumulador_dif_casas = 0\n",
        "for i, (x1, x2) in enumerate(zip(y1, y2)):\n",
        "    if np.array_equal(x1, x2):\n",
        "        totalIgualx1x2 += 1\n",
        "    else:\n",
        "        resultadosDif.append((x1, x2))\n",
        "        acumulador_dif_casas += casas_decimais_diferentes(x1, x2)\n",
        "if len(resultadosDif) != 0:\n",
        "    media_dif_casas = acumulador_dif_casas / len(resultadosDif)\n",
        "else:\n",
        "    media_dif_casas = 0\n",
        "\n",
        "print(\"Total de resultados com valor igual: \", totalIgualx1x2)\n",
        "print(\"Resulados de entropia diferentes:  \", len(resultadosDif))\n",
        "print(\"Media de casas decimais diferentes: \", media_dif_casas)\n",
        "print(\"Resulados de entropia diferentes:  \", resultadosDif[:5])"
      ],
      "metadata": {
        "id": "TgUns9okpAbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fashion Mnist\n",
        "  * [Analysis of Filter Before and After Normalization]"
      ],
      "metadata": {
        "id": "OEqG6L-miwFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x1, y1 = entropNormali(train_x, 28, 1)\n",
        "x2, y2 = entropiNaoNormali(train_x)\n",
        "print(\"Conjunto Normalizado imagem nº: \", x1[1], y1[1], \"Conjunto não normalizado: imagem nº\", x2[1], y2[1])\n",
        "analiseConjuntos(y1, y2)\n",
        "\n",
        "resultadosDif = []\n",
        "totalIgualx1x2 = 0\n",
        "acumulador_dif_casas = 0\n",
        "for i, (x1, x2) in enumerate(zip(y1, y2)):\n",
        "    if np.array_equal(x1, x2):\n",
        "        totalIgualx1x2 += 1\n",
        "    else:\n",
        "        resultadosDif.append((x1, x2))\n",
        "        acumulador_dif_casas += casas_decimais_diferentes(x1, x2)\n",
        "if len(resultadosDif) != 0:\n",
        "    media_dif_casas = acumulador_dif_casas / len(resultadosDif)\n",
        "else:\n",
        "    media_dif_casas = 0\n",
        "\n",
        "print(\"Total de resultados com valor igual: \", totalIgualx1x2)\n",
        "print(\"Resulados de entropia diferentes:  \", len(resultadosDif))\n",
        "print(\"Media de casas decimais diferentes: \", media_dif_casas)\n",
        "print(\"Resulados de entropia diferentes:  \", resultadosDif[:5])"
      ],
      "metadata": {
        "id": "OePgHiyAioED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cifar 10\n",
        "  * [Analysis of Filter Before and After Normalization]"
      ],
      "metadata": {
        "id": "1xUiw9objtOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "(train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
        "x1, y1 = entropNormali(train_x, 32, 3)\n",
        "x2, y2 = entropiNaoNormali(train_x)\n",
        "print(\"Conjunto Normalizado imagem nº: \", x1[1], y1[1], \"Conjunto não normalizado: imagem nº\", x2[1], y2[1])\n",
        "analiseConjuntos(y1, y2)\n",
        "\n",
        "resultadosDif = []\n",
        "totalIgualx1x2 = 0\n",
        "acumulador_dif_casas = 0\n",
        "for i, (x1, x2) in enumerate(zip(y1, y2)):\n",
        "    if np.array_equal(x1, x2):\n",
        "        totalIgualx1x2 += 1\n",
        "    else:\n",
        "        resultadosDif.append((x1, x2))\n",
        "        acumulador_dif_casas += casas_decimais_diferentes(x1, x2)\n",
        "if len(resultadosDif) != 0:\n",
        "    media_dif_casas = acumulador_dif_casas / len(resultadosDif)\n",
        "else:\n",
        "    media_dif_casas = 0\n",
        "\n",
        "print(\"Total de resultados com valor igual: \", totalIgualx1x2)\n",
        "print(\"Resulados de entropia diferentes:  \", len(resultadosDif))\n",
        "print(\"Media de casas decimais diferentes: \", media_dif_casas)\n",
        "print(\"Resulados de entropia diferentes:  \", resultadosDif[:5])"
      ],
      "metadata": {
        "id": "sCG2AE2djoJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cifar 100\n",
        "  * [Analysis of Filter Before and After Normalization]"
      ],
      "metadata": {
        "id": "0VMGsKI2k8Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar100\n",
        "(train_x, train_y), (test_x, test_y) = cifar100.load_data()\n",
        "x1, y1 = entropNormali(train_x, 32, 3)\n",
        "x2, y2 = entropiNaoNormali(train_x)\n",
        "print(\"Conjunto Normalizado imagem nº: \", x1[1], y1[1], \"Conjunto não normalizado: imagem nº\", x2[1], y2[1])\n",
        "analiseConjuntos(y1, y2)\n",
        "\n",
        "resultadosDif = []\n",
        "totalIgualx1x2 = 0\n",
        "acumulador_dif_casas = 0\n",
        "for i, (x1, x2) in enumerate(zip(y1, y2)):\n",
        "    if np.array_equal(x1, x2):\n",
        "        totalIgualx1x2 += 1\n",
        "    else:\n",
        "        resultadosDif.append((x1, x2))\n",
        "        acumulador_dif_casas += casas_decimais_diferentes(x1, x2)\n",
        "if len(resultadosDif) != 0:\n",
        "    media_dif_casas = acumulador_dif_casas / len(resultadosDif)\n",
        "else:\n",
        "    media_dif_casas = 0\n",
        "\n",
        "print(\"Total de resultados com valor igual: \", totalIgualx1x2)\n",
        "print(\"Resulados de entropia diferentes:  \", len(resultadosDif))\n",
        "print(\"Media de casas decimais diferentes: \", media_dif_casas)\n",
        "print(\"Resulados de entropia diferentes:  \", resultadosDif[:5])"
      ],
      "metadata": {
        "id": "o7ZzbTSjk8Ne"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
